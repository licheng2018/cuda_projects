{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Task Explanation: Vector Addition Kernel + Grid-Stride Loop + Correctness Verification\n",
        "## ðŸŽ¯ 1. Objective of the Task\n",
        "\n",
        "The purpose of this task is to introduce fundamental CUDA programming concepts by implementing a simple parallel operationâ€”vector additionâ€”and then improving it using a grid-stride loop, which is widely used in real production-grade GPU kernels.\n",
        "\n",
        "You will also verify correctness by comparing GPU output against a CPU reference implementation.\n",
        "\n",
        "## ðŸ§© 2. Part 1 â€” Implement a Basic CUDA Vector Addition Kernel\n",
        "### Task\n",
        "\n",
        "Write a CUDA kernel where each thread computes one output element:\n",
        "\n",
        "C[i] = A[i] + B[i]\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "* Computing global thread index:\n",
        "\n",
        "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\n",
        "* Avoiding out-of-bounds access by checking idx < N.\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "This simple kernel helps you learn:\n",
        "\n",
        "* How GPUs launch thousands of threads\n",
        "\n",
        "* How computation is mapped to threads\n",
        "\n",
        "* CUDA kernel structure (__global__)\n",
        "\n",
        "* Host-device execution flow\n",
        "\n",
        "This is the â€œHello Worldâ€ of CUDA and the foundation of all ML acceleration work.\n",
        "\n",
        "## ðŸ” 3. Part 2 â€” Rewrite Using a Grid-Stride Loop\n",
        "### Task\n",
        "\n",
        "Modify the kernel so that each thread processes multiple elements using a loop like:\n",
        "\n",
        "for (int i = idx; i < N; i += blockDim.x * gridDim.x) {\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "### Why Grid-Stride Loops Are Important\n",
        "\n",
        "This pattern is used in PyTorch, TensorFlow, cuBLAS, FlashAttention, and most CUDA ML kernels because it allows:\n",
        "\n",
        "* Handling vectors larger than the number of threads\n",
        "\n",
        "* Configuring threads independent of data size\n",
        "\n",
        "* Better GPU utilization and load balancing\n",
        "\n",
        "* Contiguous memory access for performance\n",
        "\n",
        "This step marks the transition from â€œtoy kernelâ€ â†’ â€œreal GPU engineering skill.â€\n",
        "\n",
        "## ðŸ§ª 4. Part 3 â€” Verify Correctness\n",
        "### Task\n",
        "\n",
        "Create host-side code to:\n",
        "\n",
        "1. Allocate memory for A, B, and C\n",
        "\n",
        "2. Initialize A and B with known values\n",
        "\n",
        "3. Run the GPU kernel\n",
        "\n",
        "4. Compute a CPU reference result\n",
        "\n",
        "5. Compare GPU and CPU values\n",
        "\n",
        "6. Print PASS/FAIL\n",
        "\n",
        "### What to Check\n",
        "\n",
        "* All GPU outputs match CPU outputs\n",
        "\n",
        "* Handle float comparisons with tolerance:\n",
        "\n",
        "fabs(gpu - cpu) < 1e-6\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Correctness checking is mandatory in professional GPU development:\n",
        "\n",
        "* Ensures kernels behave as expected\n",
        "\n",
        "* Avoids silent numerical bugs\n",
        "\n",
        "* Builds confidence before performance optimization\n",
        "\n",
        "Every CUDA kernel in ML frameworks undergoes similar validation.\n",
        "\n",
        "## ðŸ§  5. What You Will Learn\n",
        "\n",
        "By completing this task, you will understand:\n",
        "\n",
        "Core CUDA Programming Concepts\n",
        "\n",
        "* Kernel launches\n",
        "\n",
        "* Thread indexing\n",
        "\n",
        "* Memory management\n",
        "\n",
        "* Parallel execution model\n",
        "\n",
        "High-Performance GPU Design Patterns\n",
        "\n",
        "* Grid-stride loops\n",
        "\n",
        "* Coalesced memory access\n",
        "\n",
        "* Load balancing\n",
        "\n",
        "Engineering Best Practices\n",
        "\n",
        "* CPU reference comparison\n",
        "\n",
        "* GPU debugging workflow\n",
        "\n",
        "* Repeatable benchmarking\n",
        "\n",
        "This is foundational knowledge for ML systems engineering, GPU kernel optimization, LLM inference acceleration, and CUDA-based scientific computation.\n",
        "\n",
        "## ðŸ“¦ 6. Final Deliverables\n",
        "\n",
        "You should produce the following files:\n",
        "\n",
        "File\tDescription\n",
        "vector_add_basic.cu\tBasic per-thread vector addition kernel\n",
        "vector_add_gridstride.cu\tScalable kernel with grid-stride loop\n",
        "test_vector_add.cpp/cu\tCPU vs GPU correctness checking\n",
        "benchmark.txt\t(Optional) Timing comparison: CPU vs GPU\n",
        "\n",
        "These can be uploaded to GitHub as part of your Week-1 CUDA project."
      ],
      "metadata": {
        "id": "0ZImFiAPBb3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdc7JqW9_yR6",
        "outputId": "45030f5a-d167-4723-c80d-f90ba20dddcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Fri Dec 12 00:59:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ],
      "metadata": {
        "id": "ttXPpHOgFmzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 â€” Implement a Basic CUDA Vector Addition Kernel"
      ],
      "metadata": {
        "id": "cNdhMOkTJ5Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#define N 1024\n",
        "\n",
        "__global__ void vector_add(float* a, float* b, float* c, int n){\n",
        "    int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (id < n){\n",
        "        c[id] = a[id] + b[id];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    float *a, *b, *c;\n",
        "    size_t bytes = N * sizeof(float);\n",
        "    cudaMallocManaged(&a, bytes);\n",
        "    cudaMallocManaged(&b, bytes);\n",
        "    cudaMallocManaged(&c, bytes);\n",
        "\n",
        "\n",
        "    for(int i = 0; i < N; i++){\n",
        "        a[i] = i * 1.0f;\n",
        "        b[i] = i * 2.0f;\n",
        "    }\n",
        "\n",
        "\n",
        "    int threads = 256;\n",
        "    int blocks  = (N + threads - 1) / threads;\n",
        "    dim3 grid_dim(blocks);\n",
        "    dim3 block_dim(threads);\n",
        "\n",
        "    vector_add<<<grid_dim, block_dim>>>(a, b, c, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    std::cout << \"GPU output\" <<\" \";\n",
        "    for (int j = 0; j < 10; ++j)\n",
        "        std::cout << c[j] << \" \";\n",
        "     std::cout << std::endl;\n",
        "\n",
        "    std::cout << \"CPU output\" <<\" \";\n",
        "    for (int j = 0; j < 10; ++j)\n",
        "        std::cout << a[j] + b[j] << \" \";\n",
        "     std::cout << std::endl;\n",
        "\n",
        "\n",
        "    cudaFree(a);\n",
        "    cudaFree(b);\n",
        "    cudaFree(c);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJynWp7eAox3",
        "outputId": "d5fd809c-4361-4586-ca1a-90991f31ae35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 vector_add.cu -o vector_add\n",
        "!./vector_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCjLwWbtHaQM",
        "outputId": "91b9c0ad-8cee-4a21-fe86-317a0f1dc6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU output 0 3 6 9 12 15 18 21 24 27 \n",
            "CPU output 0 3 6 9 12 15 18 21 24 27 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 â€” Rewrite Using a Grid-Stride Loop"
      ],
      "metadata": {
        "id": "jXtXFQ_5J7Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add_gridstride_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --------------------\n",
        "// Simple CUDA error check macro\n",
        "// --------------------\n",
        "#define CUDA_CHECK(call)                                                     \\\n",
        "    do {                                                                     \\\n",
        "        cudaError_t err = (call);                                            \\\n",
        "        if (err != cudaSuccess) {                                            \\\n",
        "            fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                         \\\n",
        "                    __FILE__, __LINE__, cudaGetErrorString(err));            \\\n",
        "            std::exit(EXIT_FAILURE);                                         \\\n",
        "        }                                                                    \\\n",
        "    } while (0)\n",
        "\n",
        "// --------------------\n",
        "// TODO: Implement grid-stride vector add kernel\n",
        "// Requirements:\n",
        "//  - Use global thread index: tid = blockIdx.x * blockDim.x + threadIdx.x\n",
        "//  - Use stride = blockDim.x * gridDim.x\n",
        "//  - Loop: for (i = tid; i < N; i += stride) C[i] = A[i] + B[i]\n",
        "//  - A, B are read-only; C is output\n",
        "// --------------------\n",
        "__global__ void vectorAddGridStride(const float* A, const float* B, float* C, int N) {\n",
        "    // TODO: compute tid\n",
        "    // TODO: compute stride\n",
        "    // TODO: grid-stride loop\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    if(tid < N){\n",
        "        for (int i = tid; i < N; i += stride){\n",
        "            C[i] = A[i] + B[i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// --------------------\n",
        "// CPU reference for correctness check\n",
        "// --------------------\n",
        "static void vectorAddCPU(const float* A, const float* B, float* C, int N) {\n",
        "    for (int i = 0; i < N; i++) C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "// --------------------\n",
        "// TODO: Fill helper to compare results\n",
        "// Requirements:\n",
        "//  - Return true if all elements match within tolerance (e.g., 1e-6)\n",
        "//  - Print the first mismatch (index + values) then return false\n",
        "// --------------------\n",
        "static bool checkClose(const float* gpu, const float* cpu, int N, float tol) {\n",
        "    // TODO\n",
        "    for (int i = 0; i < N; i++){\n",
        "        float diff = fabsf(gpu[i] - cpu[i]);\n",
        "        if (diff > tol){\n",
        "          printf(\"Mismatch at %d: gpu=%f cpu=%f diff=%f\\n\", i, gpu[i], cpu[i], diff);\n",
        "          return false;\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // --------------------\n",
        "    // Config\n",
        "    // --------------------\n",
        "    const int N = 1 << 24;          // ~16 million floats\n",
        "    const size_t bytes = size_t(N) * sizeof(float);\n",
        "    const float tol = 1e-6f;\n",
        "\n",
        "    // --------------------\n",
        "    // Host alloc\n",
        "    // --------------------\n",
        "    float* hA = (float*)std::malloc(bytes);\n",
        "    float* hB = (float*)std::malloc(bytes);\n",
        "    float* hC_gpu = (float*)std::malloc(bytes);\n",
        "    float* hC_cpu = (float*)std::malloc(bytes);\n",
        "\n",
        "    if (!hA || !hB || !hC_gpu || !hC_cpu) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // --------------------\n",
        "    // Init host data\n",
        "    // --------------------\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        hA[i] = 0.001f * i;\n",
        "        hB[i] = 0.002f * i;\n",
        "    }\n",
        "\n",
        "    // --------------------\n",
        "    // Device alloc\n",
        "    // --------------------\n",
        "    float *dA = nullptr, *dB = nullptr, *dC = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, bytes));\n",
        "\n",
        "    // --------------------\n",
        "    // H2D copy\n",
        "    // --------------------\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytes, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // --------------------\n",
        "    // TODO: Choose launch config\n",
        "    // Requirements:\n",
        "    //  - Set blockSize (commonly 128/256/512)\n",
        "    //  - Set gridSize (e.g., (N + blockSize - 1)/blockSize) OR a fixed large grid\n",
        "    // --------------------\n",
        "    int blockSize = 256; // TODO\n",
        "    int gridSize  = (N + blockSize - 1)/blockSize; // TODO\n",
        "\n",
        "    // --------------------\n",
        "    // Launch kernel\n",
        "    // --------------------\n",
        "    vectorAddGridStride<<<gridSize, blockSize>>>(dA, dB, dC, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // --------------------\n",
        "    // D2H copy\n",
        "    // --------------------\n",
        "    CUDA_CHECK(cudaMemcpy(hC_gpu, dC, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // --------------------\n",
        "    // CPU reference\n",
        "    // --------------------\n",
        "    vectorAddCPU(hA, hB, hC_cpu, N);\n",
        "\n",
        "    // --------------------\n",
        "    // TODO: Correctness check\n",
        "    // --------------------\n",
        "    bool ok = checkClose(hC_gpu, hC_cpu, N, tol); // TODO: make checkClose work\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // --------------------\n",
        "    // Cleanup\n",
        "    // --------------------\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hC_gpu);\n",
        "    std::free(hC_cpu);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlQBnmImJy0H",
        "outputId": "9c6304bd-6135-482c-cc8c-4a7fa2d282db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add_gridstride_skeleton.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 vector_add_gridstride_skeleton.cu -o vector_add_gridstride_skeleton\n",
        "!./vector_add_gridstride_skeleton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sjDIBKVLGF7",
        "outputId": "e8a039a1-8de9-47c7-d98e-9e315442a7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctness: PASS\n"
          ]
        }
      ]
    }
  ]
}