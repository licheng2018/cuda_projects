{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# üìò Task Explanation: Nsight Compute Profiling ‚Äî Warp Stalls and Memory Stalls\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to use **Nsight Compute** to perform **kernel-level performance profiling** and identify the primary causes of performance loss, with a specific focus on **warp stalls** and **memory stalls**.\n",
        "\n",
        "By completing this task, you will learn how to move beyond runtime measurements and understand **why a CUDA kernel is slow at the micro-architectural level**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is Nsight Compute?\n",
        "**Nsight Compute** is NVIDIA‚Äôs **kernel-focused performance profiler**.  \n",
        "Unlike Nsight Systems, which provides a timeline view of the entire application, Nsight Compute analyzes:\n",
        "- Individual CUDA kernels\n",
        "- Instruction throughput\n",
        "- Memory behavior\n",
        "- Warp execution efficiency\n",
        "\n",
        "Nsight Compute answers the question:\n",
        "> *‚ÄúWhat is limiting the performance of this specific kernel?‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Nsight Compute Profiling\n",
        "\n",
        "### Task\n",
        "Profile a CUDA kernel using Nsight Compute and collect detailed performance metrics.\n",
        "\n",
        "You should:\n",
        "- Run Nsight Compute on one or more CUDA kernels\n",
        "- Collect metrics related to:\n",
        "  - Warp execution\n",
        "  - Memory access\n",
        "  - Instruction scheduling\n",
        "\n",
        "### Goal\n",
        "Obtain a detailed breakdown of how warps are scheduled and where execution time is being lost.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Identify Warp Stalls\n",
        "\n",
        "### What Is a Warp Stall?\n",
        "A **warp stall** occurs when a warp is ready to execute but cannot proceed due to a hardware or dependency limitation.\n",
        "\n",
        "Common causes include:\n",
        "- Instruction dependencies (e.g., waiting for a previous instruction to complete)\n",
        "- Insufficient instruction-level parallelism (ILP)\n",
        "- Execution pipeline contention\n",
        "\n",
        "### Task\n",
        "Using Nsight Compute, identify:\n",
        "- The dominant warp stall reasons\n",
        "- The percentage of cycles spent stalled\n",
        "- Whether stalls are due to compute or scheduling issues\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Identify Memory Stalls\n",
        "\n",
        "### What Is a Memory Stall?\n",
        "A **memory stall** happens when a warp is waiting for data to be loaded from memory.\n",
        "\n",
        "Typical sources:\n",
        "- Global memory latency\n",
        "- Cache misses (L1 / L2)\n",
        "- Uncoalesced memory accesses\n",
        "- Register spills to local memory\n",
        "\n",
        "### Task\n",
        "Analyze memory-related stall metrics and determine:\n",
        "- Whether the kernel is memory-bound\n",
        "- Which memory level (global, L2, shared, local) is the bottleneck\n",
        "- Whether access patterns are inefficient\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Look For in Nsight Compute\n",
        "\n",
        "Key metrics and sections to examine:\n",
        "- Warp stall breakdown (e.g., stalled on memory, stalled on dependencies)\n",
        "- Memory throughput vs. theoretical peak\n",
        "- Cache hit / miss rates\n",
        "- Instruction issue efficiency\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- Are warps mostly stalled or actively executing?\n",
        "- Is the kernel limited by memory latency or compute throughput?\n",
        "- Are stalls caused by memory access patterns or instruction dependencies?\n",
        "- Which optimizations (e.g., memory coalescing, unrolling, prefetching) could reduce stalls?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. Nsight Compute profiling reports for selected kernels\n",
        "2. A summary of dominant warp and memory stall reasons\n",
        "3. A short analysis explaining:\n",
        "   - Why these stalls occur\n",
        "   - How they impact performance\n",
        "   - What optimizations could mitigate them\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to interpret Nsight Compute metrics\n",
        "- The difference between warp stalls and memory stalls\n",
        "- How low-level hardware behavior affects kernel performance\n",
        "- How to connect profiling results to concrete optimization strategies\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Identifying warp and memory stalls is critical for optimizing:\n",
        "- Matrix multiplication kernels\n",
        "- Reduction and normalization kernels\n",
        "- Attention and FlashAttention implementations\n",
        "- Compiler-generated kernels (e.g., Triton)\n",
        "\n",
        "This task trains you to reason about GPU performance at the **same level used by professional ML systems and GPU kernel engineers**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "0a3a4446-a542-4eb9-b3c0-be1d17ce837c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvcc: command not found\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn__N7dDp_8S"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJj_KPxso5zD",
        "outputId": "9988e2a2-b059-4bd0-8d15-37717e37d5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting matmul_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ncu_stall_profile_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Task: Nsight Compute profiling + identify warp stalls & memory stalls\n",
        "// NO SOLUTION: fill TODOs to create a kernel that you can profile.\n",
        "//\n",
        "// Options (pick one by implementing the kernel accordingly):\n",
        "//  A) Memory-stall oriented kernel: strided global loads, cache-miss friendly\n",
        "//  B) Warp-stall oriented kernel: dependency chain / low ILP\n",
        "//  C) Compare two kernels with a flag to see stall breakdown differences\n",
        "// ------------------------------------------------------------\n",
        "\n",
        "__global__ void kernelToProfile(const float* __restrict__ in,\n",
        "                                float* __restrict__ out,\n",
        "                                int N,\n",
        "                                int stride,\n",
        "                                int iters) {\n",
        "    // TODO:\n",
        "    // - compute global thread index tid\n",
        "    // - create your access pattern and/or dependency chain\n",
        "    // - do repeated work controlled by iters\n",
        "    // - write out[tid] to prevent compiler eliminating the loop\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Optional: second variant to compare stalls (leave as TODO or unused)\n",
        "// ------------------------------------------------------------\n",
        "__global__ void kernelToProfileAlt(const float* __restrict__ in,\n",
        "                                   float* __restrict__ out,\n",
        "                                   int N,\n",
        "                                   int stride,\n",
        "                                   int iters) {\n",
        "    // TODO\n",
        "}\n",
        "\n",
        "static void initHost(float* a, int N) {\n",
        "    for (int i = 0; i < N; ++i) a[i] = 0.001f * (i % 1000);\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    // Simple args: ./app <stride> <iters> <mode>\n",
        "    // mode: 0 -> kernelToProfile, 1 -> kernelToProfileAlt\n",
        "    int stride = (argc > 1) ? std::atoi(argv[1]) : 1;\n",
        "    int iters  = (argc > 2) ? std::atoi(argv[2]) : 256;\n",
        "    int mode   = (argc > 3) ? std::atoi(argv[3]) : 0;\n",
        "\n",
        "    const int N = 1 << 24;\n",
        "    const size_t bytes = size_t(N) * sizeof(float);\n",
        "\n",
        "    float* hIn  = (float*)std::malloc(bytes);\n",
        "    float* hOut = (float*)std::malloc(bytes);\n",
        "    if (!hIn || !hOut) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    initHost(hIn, N);\n",
        "\n",
        "    float *dIn=nullptr, *dOut=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dIn, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(dIn, hIn, bytes, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemset(dOut, 0, bytes));\n",
        "\n",
        "    // TODO: choose launch config\n",
        "    int blockSize = 0; // TODO (e.g., 256)\n",
        "    int gridSize  = 0; // TODO (e.g., (N + blockSize - 1) / blockSize)\n",
        "\n",
        "    // Warmup (optional)\n",
        "    for (int i = 0; i < 3; ++i) {\n",
        "        if (mode == 0) kernelToProfile<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "        else           kernelToProfileAlt<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Profile target launch (Nsight Compute will capture this kernel)\n",
        "    if (mode == 0) kernelToProfile<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "    else           kernelToProfileAlt<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(hOut, dOut, bytes, cudaMemcpyDeviceToHost));\n",
        "    printf(\"Done. stride=%d iters=%d mode=%d out0=%f\\n\", stride, iters, mode, hOut[0]);\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dIn));\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    std::free(hIn);\n",
        "    std::free(hOut);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqbVBUnpdIt",
        "outputId": "9e9e8275-1e34-47c6-92d7-333528955f23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Naive MatMul Kernel (Baseline)]                          Kernel=1 | correctness = PASS | time=9.1753 ms | GFLOPS=234.05\n",
            "[Block Tiling Kernel (structure only, still global loads)]Kernel=2 | correctness = PASS | time=4.0592 ms | GFLOPS=529.04\n",
            "[Shared-Memory Tiled Kernel(Tile16)]                    Kernel=3 | correctness = PASS | time=2.4577 ms | GFLOPS=873.76\n",
            "[Shared-Memory Tiled Kernel(Tile32)]                   Kernel=4 | correctness = PASS | time=10.4073 ms | GFLOPS=206.34\n",
            "[Shared-Memory Tiled Kernel(Tile32padding)]            Kernel=5 | correctness = PASS | time=2.0924 ms | GFLOPS=1026.32\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 ncu_stall_profile_skeleton.cu -o ncu_stall_profile_skeleton\n",
        "!./ncu_stall_profile_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nsight Compute (collect stall-related sections)\n",
        "!ncu --set full --kernel-name kernelToProfile -o ncu_report_stride1 ./ncu_stalls 1 256 0\n",
        "!ncu --set full --kernel-name kernelToProfile -o ncu_report_stride4 ./ncu_stalls 4 256 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
