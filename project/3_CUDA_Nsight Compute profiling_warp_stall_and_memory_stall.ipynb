{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# üìò Task Explanation: Nsight Compute Profiling ‚Äî Warp Stalls and Memory Stalls\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to use **Nsight Compute** to perform **kernel-level performance profiling** and identify the primary causes of performance loss, with a specific focus on **warp stalls** and **memory stalls**.\n",
        "\n",
        "By completing this task, you will learn how to move beyond runtime measurements and understand **why a CUDA kernel is slow at the micro-architectural level**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is Nsight Compute?\n",
        "**Nsight Compute** is NVIDIA‚Äôs **kernel-focused performance profiler**.  \n",
        "Unlike Nsight Systems, which provides a timeline view of the entire application, Nsight Compute analyzes:\n",
        "- Individual CUDA kernels\n",
        "- Instruction throughput\n",
        "- Memory behavior\n",
        "- Warp execution efficiency\n",
        "\n",
        "Nsight Compute answers the question:\n",
        "> *‚ÄúWhat is limiting the performance of this specific kernel?‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Nsight Compute Profiling\n",
        "\n",
        "### Task\n",
        "Profile a CUDA kernel using Nsight Compute and collect detailed performance metrics.\n",
        "\n",
        "You should:\n",
        "- Run Nsight Compute on one or more CUDA kernels\n",
        "- Collect metrics related to:\n",
        "  - Warp execution\n",
        "  - Memory access\n",
        "  - Instruction scheduling\n",
        "\n",
        "### Goal\n",
        "Obtain a detailed breakdown of how warps are scheduled and where execution time is being lost.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Identify Warp Stalls\n",
        "\n",
        "### What Is a Warp Stall?\n",
        "A **warp stall** occurs when a warp is ready to execute but cannot proceed due to a hardware or dependency limitation.\n",
        "\n",
        "Common causes include:\n",
        "- Instruction dependencies (e.g., waiting for a previous instruction to complete)\n",
        "- Insufficient instruction-level parallelism (ILP)\n",
        "- Execution pipeline contention\n",
        "\n",
        "### Task\n",
        "Using Nsight Compute, identify:\n",
        "- The dominant warp stall reasons\n",
        "- The percentage of cycles spent stalled\n",
        "- Whether stalls are due to compute or scheduling issues\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Identify Memory Stalls\n",
        "\n",
        "### What Is a Memory Stall?\n",
        "A **memory stall** happens when a warp is waiting for data to be loaded from memory.\n",
        "\n",
        "Typical sources:\n",
        "- Global memory latency\n",
        "- Cache misses (L1 / L2)\n",
        "- Uncoalesced memory accesses\n",
        "- Register spills to local memory\n",
        "\n",
        "### Task\n",
        "Analyze memory-related stall metrics and determine:\n",
        "- Whether the kernel is memory-bound\n",
        "- Which memory level (global, L2, shared, local) is the bottleneck\n",
        "- Whether access patterns are inefficient\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Look For in Nsight Compute\n",
        "\n",
        "Key metrics and sections to examine:\n",
        "- Warp stall breakdown (e.g., stalled on memory, stalled on dependencies)\n",
        "- Memory throughput vs. theoretical peak\n",
        "- Cache hit / miss rates\n",
        "- Instruction issue efficiency\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- Are warps mostly stalled or actively executing?\n",
        "- Is the kernel limited by memory latency or compute throughput?\n",
        "- Are stalls caused by memory access patterns or instruction dependencies?\n",
        "- Which optimizations (e.g., memory coalescing, unrolling, prefetching) could reduce stalls?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. Nsight Compute profiling reports for selected kernels\n",
        "2. A summary of dominant warp and memory stall reasons\n",
        "3. A short analysis explaining:\n",
        "   - Why these stalls occur\n",
        "   - How they impact performance\n",
        "   - What optimizations could mitigate them\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to interpret Nsight Compute metrics\n",
        "- The difference between warp stalls and memory stalls\n",
        "- How low-level hardware behavior affects kernel performance\n",
        "- How to connect profiling results to concrete optimization strategies\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Identifying warp and memory stalls is critical for optimizing:\n",
        "- Matrix multiplication kernels\n",
        "- Reduction and normalization kernels\n",
        "- Attention and FlashAttention implementations\n",
        "- Compiler-generated kernels (e.g., Triton)\n",
        "\n",
        "This task trains you to reason about GPU performance at the **same level used by professional ML systems and GPU kernel engineers**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "e10b272a-0af1-439d-a206-a36319bf4766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Mon Jan 19 15:53:40 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   32C    P0             58W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJj_KPxso5zD",
        "outputId": "c707c574-f1bd-4e55-d69d-fcc0e04ba1f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ncu_stall_profile_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ncu_stall_profile_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Task: Nsight Compute profiling + identify warp stalls & memory stalls\n",
        "// NO SOLUTION: fill TODOs to create a kernel that you can profile.\n",
        "//\n",
        "// Options (pick one by implementing the kernel accordingly):\n",
        "//  A) Memory-stall oriented kernel: strided global loads, cache-miss friendly\n",
        "//  B) Warp-stall oriented kernel: dependency chain / low ILP\n",
        "//  C) Compare two kernels with a flag to see stall breakdown differences\n",
        "// ------------------------------------------------------------\n",
        "\n",
        "__device__ __forceinline__ float prevent_opt(float x) {\n",
        "  asm volatile(\"\" : \"+f\"(x));\n",
        "  return x;\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void kernelToProfile(const float* __restrict__ in,\n",
        "                                float* __restrict__ out,\n",
        "                                int N,\n",
        "                                int stride,\n",
        "                                int iters) {\n",
        "    // TODO:\n",
        "    // - compute global thread index tid\n",
        "    // - create your access pattern and/or dependency chain\n",
        "    // - do repeated work controlled by iters\n",
        "    // - write out[tid] to prevent compiler eliminating the loop\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(tid >= N) return;\n",
        "\n",
        "    int idx = tid;\n",
        "    float acc = 0.0f;\n",
        "    for (int it = 0; it < iters; ++it) {\n",
        "        idx = idx + stride;\n",
        "        if (idx >= N) idx -= N;\n",
        "\n",
        "        float v = in[idx];\n",
        "        acc = prevent_opt(acc + v);\n",
        "        }\n",
        "    out[tid] = acc;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Optional: second variant to compare stalls (leave as TODO or unused)\n",
        "// ------------------------------------------------------------\n",
        "__global__ void kernelToProfileAlt(const float* __restrict__ in,\n",
        "                                   float* __restrict__ out,\n",
        "                                   int N,\n",
        "                                   int stride,\n",
        "                                   int iters) {\n",
        "    // TODO\n",
        "    int tid = (int)(blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // One load to seed (keeps kernel \"real\" but avoids becoming memory-bound)\n",
        "    float x = in[tid];\n",
        "    float acc = x;\n",
        "\n",
        "    #pragma unroll 1\n",
        "    for (int it = 0; it < iters; ++it) {\n",
        "        // Tight dependency chain: each op depends on previous acc.\n",
        "        // Use simple arithmetic to avoid special-function units dominating.\n",
        "        acc = acc * 1.000001f + 0.000001f;\n",
        "        acc = prevent_opt(acc);\n",
        "    }\n",
        "\n",
        "    out[tid] = acc;\n",
        "}\n",
        "\n",
        "static void initHost(float* a, int N) {\n",
        "    for (int i = 0; i < N; ++i) a[i] = 0.001f * (i % 1000);\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    // Simple args: ./app <stride> <iters> <mode>\n",
        "    // mode: 0 -> kernelToProfile, 1 -> kernelToProfileAlt\n",
        "    int stride = (argc > 1) ? std::atoi(argv[1]) : 1;\n",
        "    int iters  = (argc > 2) ? std::atoi(argv[2]) : 256;\n",
        "    int mode   = (argc > 3) ? std::atoi(argv[3]) : 0;\n",
        "\n",
        "    const int N = 1 << 24;\n",
        "    const size_t bytes = size_t(N) * sizeof(float);\n",
        "\n",
        "    float* hIn  = (float*)std::malloc(bytes);\n",
        "    float* hOut = (float*)std::malloc(bytes);\n",
        "    if (!hIn || !hOut) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    initHost(hIn, N);\n",
        "\n",
        "    float *dIn=nullptr, *dOut=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dIn, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(dIn, hIn, bytes, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemset(dOut, 0, bytes));\n",
        "\n",
        "    // TODO: choose launch config\n",
        "    int blockSize = 256; // TODO (e.g., 256)\n",
        "    int gridSize  = (N + blockSize - 1) / blockSize; // TODO (e.g., (N + blockSize - 1) / blockSize)\n",
        "\n",
        "    // Warmup (optional)\n",
        "    for (int i = 0; i < 3; ++i) {\n",
        "        if (mode == 0) kernelToProfile<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "        else           kernelToProfileAlt<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Profile target launch (Nsight Compute will capture this kernel)\n",
        "    if (mode == 0) kernelToProfile<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "    else           kernelToProfileAlt<<<gridSize, blockSize>>>(dIn, dOut, N, stride, iters);\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(hOut, dOut, bytes, cudaMemcpyDeviceToHost));\n",
        "    printf(\"Done. stride=%d iters=%d mode=%d out0=%f\\n\", stride, iters, mode, hOut[0]);\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dIn));\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    std::free(hIn);\n",
        "    std::free(hOut);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqbVBUnpdIt",
        "outputId": "b20fd245-bfc7-45b0-d689-d845976fefbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. stride=1 iters=256 mode=0 out0=32.896004\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_80 ncu_stall_profile_skeleton.cu -o ncu_stall_profile_skeleton\n",
        "!./ncu_stall_profile_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UptEolh9KF4",
        "outputId": "54ed7aef-c5d4-499c-a670-060255e30958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 1369 (/content/ncu_stall_profile_skeleton)\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "Done. stride=1 iters=256 mode=0 out0=32.896004\n",
            "==PROF== Disconnected from process 1369\n",
            "==PROF== Report: /content/ncu_report_stride1.ncu-rep\n",
            "==PROF== Connected to process 1638 (/content/ncu_stall_profile_skeleton)\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "==PROF== Profiling \"kernelToProfile\": 0%....50%....100% - 47 passes\n",
            "Done. stride=4 iters=256 mode=0 out0=124.584007\n",
            "==PROF== Disconnected from process 1638\n",
            "==PROF== Report: /content/ncu_report_stride4.ncu-rep\n"
          ]
        }
      ],
      "source": [
        "# Nsight Compute (collect stall-related sections)\n",
        "!ncu --set full --kernel-name kernelToProfile -o ncu_report_stride1 ./ncu_stall_profile_skeleton 1 256 0\n",
        "!ncu --set full --kernel-name kernelToProfile -o ncu_report_stride4 ./ncu_stall_profile_skeleton 4 256 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}