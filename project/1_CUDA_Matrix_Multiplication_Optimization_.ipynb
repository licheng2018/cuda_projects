{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Task Explanation: CUDA Matrix Multiplication Optimization  \n",
        "*(Naive ‚Üí Block Tiling ‚Üí Shared Memory)*\n",
        "\n",
        "This task guides you through a **three-day progressive implementation and optimization** of CUDA matrix multiplication (GEMM).  \n",
        "You will start from a **naive implementation**, then introduce **block tiling**, and finally optimize using **shared memory with bank-conflict awareness**.  \n",
        "The goal is to build a complete and systematic understanding of GPU optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## üóìÔ∏è Day 1 ‚Äî Implement Naive MatMul Kernel & Test Correctness\n",
        "\n",
        "### üéØ Objective\n",
        "Implement the most straightforward CUDA matrix multiplication kernel to serve as a **performance baseline** for later optimizations, and ensure the result is correct.\n",
        "\n",
        "### üß© Tasks\n",
        "- Write a naive CUDA kernel:\n",
        "  - Each thread computes one element of matrix `C`\n",
        "  - All operands are read directly from **global memory**\n",
        "- Implement a CPU reference version\n",
        "- Compare GPU results with CPU results to verify correctness\n",
        "\n",
        "### üß† Key Concepts\n",
        "- Mapping 2D thread indices to matrix row/column indices\n",
        "- Understanding why the naive implementation repeatedly loads elements from `A` and `B`\n",
        "- Recognizing why this version is typically **memory-bound**\n",
        "\n",
        "### üì¶ Deliverables\n",
        "- Naive CUDA MatMul kernel\n",
        "- Correctness test (PASS / FAIL)\n",
        "- Baseline performance measurement\n",
        "\n",
        "---\n",
        "\n",
        "## üóìÔ∏è Day 2 ‚Äî Implement Block Tiling & Configure Tile Size\n",
        "\n",
        "### üéØ Objective\n",
        "Introduce **block-level tiling** so that threads within a block cooperate on computing a submatrix, laying the structural foundation for shared-memory optimization.\n",
        "\n",
        "### üß© Tasks\n",
        "- Decompose matrices into fixed-size tiles (e.g., 16√ó16, 32√ó32)\n",
        "- Assign each thread block to compute one tile of matrix `C`\n",
        "- Use 2D block and thread indexing to map elements within a tile\n",
        "- Experiment with different tile sizes (16√ó16 vs 32√ó32)\n",
        "\n",
        "### üß† Key Concepts\n",
        "- How tiling improves data locality\n",
        "- How tile size affects:\n",
        "  - Parallelism\n",
        "  - Occupancy\n",
        "  - Resource usage\n",
        "- Why block tiling alone does **not yet reduce global memory traffic**\n",
        "\n",
        "### üì¶ Deliverables\n",
        "- Block-tiled MatMul kernel\n",
        "- Performance comparison across different tile sizes\n",
        "\n",
        "---\n",
        "\n",
        "## üóìÔ∏è Day 3 ‚Äî Cache Tiles in Shared Memory & Handle Bank Conflicts\n",
        "\n",
        "### üéØ Objective\n",
        "Use **shared memory** to cache matrix tiles, reduce global memory accesses, and analyze and mitigate **shared-memory bank conflicts**.\n",
        "\n",
        "### üß© Tasks\n",
        "- Load tiles of matrices `A` and `B` into shared memory\n",
        "- Synchronize threads within a block using `__syncthreads()`\n",
        "- Perform multiply‚Äìaccumulate operations using shared memory\n",
        "- Analyze shared-memory access patterns\n",
        "- Identify and mitigate potential bank conflicts (e.g., via padding)\n",
        "\n",
        "### üß† Key Concepts\n",
        "- How shared memory enables data reuse and reduces global memory traffic\n",
        "- How bank conflicts serialize shared-memory accesses and hurt performance\n",
        "- Why certain tile layouts naturally avoid bank conflicts\n",
        "- The role of padding in shared-memory layouts\n",
        "\n",
        "### üì¶ Deliverables\n",
        "- Shared-memory tiled MatMul kernel\n",
        "- Bank-conflict analysis and optimization notes\n",
        "- Performance comparison against naive and block-tiling versions\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Should Learn from This Task\n",
        "By completing this task, you should understand:\n",
        "- The full optimization pipeline for CUDA GEMM\n",
        "- Why performance improves when moving from **global memory ‚Üí shared memory**\n",
        "- Trade-offs between tile size and hardware resources\n",
        "- The real impact of bank conflicts on performance\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Matrix multiplication is the computational core of:\n",
        "- cuBLAS GEMM\n",
        "- Linear and Attention layers in Transformers\n",
        "- Triton matmul kernels\n",
        "- Tensor Core‚Äìbased acceleration\n",
        "\n",
        "Completing this task means you have built **GPU kernel engineer‚Äìlevel fundamentals** for ML systems and high-performance computing."
      ],
      "metadata": {
        "id": "7lXGn4pqo8-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "B5pBKltrp9GP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d78741a-7571-4153-d35a-b24f7f05c4de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Jan 11 12:14:59 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ],
      "metadata": {
        "id": "Sn__N7dDp_8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CJj_KPxso5zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7f76e0-f797-4dd4-b53a-5af6890e2c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// -----------------------------\n",
        "// TODO: set TILE size (try 16 and 32)\n",
        "// -----------------------------\n",
        "#ifndef TILE\n",
        "#define TILE 16  // TODO\n",
        "#endif\n",
        "\n",
        "// ============================================================\n",
        "// Day 1: Naive MatMul Kernel (Baseline)\n",
        "// C = A * B\n",
        "// A: MxK, B: KxN, C: MxN (row-major)\n",
        "// ============================================================\n",
        "__global__ void matmulNaive(const float* __restrict__ A,\n",
        "                            const float* __restrict__ B,\n",
        "                            float* __restrict__ C,\n",
        "                            int M, int N, int K) {\n",
        "    // TODO:\n",
        "    // - compute (row, col) from blockIdx/threadIdx\n",
        "    // - guard: if row < M && col < N\n",
        "    // - compute dot product over k in [0, K)\n",
        "    // - write C[row*N + col]\n",
        "\n",
        "    int row = blockIdx.y * TILE + threadIdx.y;\n",
        "    int col = blockIdx.x * TILE + threadIdx.x;\n",
        "\n",
        "    float acc = 0.0f;\n",
        "    if (row < M && col < N){\n",
        "        for (int k = 0; k < K; ++k){\n",
        "            acc += A[row * K + k] * B[k*N + col];\n",
        "        }\n",
        "        C[row * N + col] = acc;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// ============================================================\n",
        "// Day 2: Block Tiling Kernel (structure only, still global loads)\n",
        "// Each block computes one TILE x TILE output tile.\n",
        "// ============================================================\n",
        "__global__ void matmulBlockTiled(const float* __restrict__ A,\n",
        "                                 const float* __restrict__ B,\n",
        "                                 float* __restrict__ C,\n",
        "                                 int M, int N, int K) {\n",
        "    // TODO:\n",
        "    // - compute tile origin (blockRow, blockCol)\n",
        "    // - compute local (ty, tx) = threadIdx.y/x\n",
        "    // - compute global (row, col)\n",
        "    // - loop over k in [0, K) (still global reads)\n",
        "    // - write output\n",
        "\n",
        "    // Thread coordinates within the tile\n",
        "    int blockRow = blockIdx.y * TILE;   // starting row of C tile\n",
        "    int blockCol = blockIdx.x * TILE;   // starting col of C tile\n",
        "\n",
        "    //Global coordinates (row, col) for the output element computed by this thread\n",
        "    int row = blockRow + threadIdx.y;\n",
        "    int col = blockCol + threadIdx.x;\n",
        "\n",
        "    if (row >= M || col >= N)return;\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    // K dimension tiling (still global loads)\n",
        "    #pragma unroll\n",
        "    for(int k_tile = 0; k_tile < K;  k_tile += TILE){\n",
        "        for(int k_in_tile = 0; k_in_tile < TILE; k_in_tile++){\n",
        "            int k = k_tile + k_in_tile;\n",
        "            if(k < K){\n",
        "                acc += A[row * K + k] * B[k * N + col];\n",
        "            }\n",
        "        }\n",
        "    C[row * N + col] = acc;\n",
        "    }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Day 3: Shared-Memory Tiled Kernel\n",
        "// Load A/B tiles into shared memory, synchronize, compute partial sums.\n",
        "// Also add a place to handle bank conflicts (e.g., padding) as TODO.\n",
        "// ============================================================\n",
        "__global__ void matmulSharedTiled(const float* __restrict__ A,\n",
        "                                  const float* __restrict__ B,\n",
        "                                  float* __restrict__ C,\n",
        "                                  int M, int N, int K) {\n",
        "    // TODO:\n",
        "    // - declare shared memory tiles for A and B\n",
        "    //   (optionally with padding for bank-conflict mitigation)\n",
        "    // - compute (row, col)\n",
        "    // - for each tile along K:\n",
        "    //     * load A tile element into shared (guarded)\n",
        "    //     * load B tile element into shared (guarded)\n",
        "    //     * __syncthreads()\n",
        "    //     * accumulate over tile dimension\n",
        "    //     * __syncthreads()\n",
        "    // - write C (guarded)\n",
        "\n",
        "    // Thread coordinates within the tile\n",
        "    int blockRow = blockIdx.y * TILE;   // starting row of C tile\n",
        "    int blockCol = blockIdx.x * TILE;   // starting col of C tile\n",
        "\n",
        "    //Global coordinates (row, col) for the output element computed by this thread\n",
        "    int row = blockRow + threadIdx.y;\n",
        "    int col = blockCol + threadIdx.x;\n",
        "\n",
        "    if (row >= M || col >= N)return;\n",
        "\n",
        "    extern __shared__ float As[TILE][TILE];\n",
        "    extern __shared__ float Bs[TILE][TILE];\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    // ---- K-tiling loop ----\n",
        "    // t enumerates which K-tile we are processing\n",
        "    for(int k_tile = 0; k_tile < K;  k_tile += TILE){\n",
        "        //load A tile elements(row, k_tile + tx)\n",
        "        int aRow = row;\n",
        "        int aCol = k_tile + threadIdx.x;\n",
        "        As[threadIdx.y][threadIdx.x] = (aRow < M && aCol < K)? A[aRow * K + aCol]:0.0f;\n",
        "\n",
        "        //load B tile elements(k_tile + ty, col)\n",
        "\n",
        "        int bRow = k_tile + threadIdx.y;\n",
        "        int bCol = col;\n",
        "        Bs[threadIdx.y][threadIdx.x] = (bRow < K && bCol < N)? A[bRow * N + bCol]:0.0f;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "\n",
        "        // Multiply-accumulate this K-tile\n",
        "        #pragma unroll\n",
        "        for(int kk = 0; kk < TILE; kk++){\n",
        "            acc += As[threadIdx.y][kk] * Bs[kk][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "    }\n",
        "    if (row < M && col < N) {\n",
        "        C[row * N + col] = acc;\n",
        "        }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference GEMM (correctness baseline)\n",
        "// ============================================================\n",
        "static void matmulCPU(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            double acc = 0.0;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                acc += (double)A[i * K + k] * (double)B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = (float)acc;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: correctness checker\n",
        "// Requirements:\n",
        "//  - compare gpu vs ref within tol\n",
        "//  - print first mismatch (i, gpu, ref)\n",
        "// ============================================================\n",
        "static bool checkClose(const float* gpu, const float* ref, int count, float tol) {\n",
        "    // TODO\n",
        "    for (int i = 0; i < count; ++i){\n",
        "        if (fabsf(gpu[i] - ref[i]) >  tol){\n",
        "            printf(\"mismatch in %d digit, cpu is %.2f, gpu is %.2f\\n\", i, ref[i], gpu[i]);\n",
        "\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Timing helper (CUDA events) - implemented\n",
        "// ============================================================\n",
        "template <typename KernelFunc>\n",
        "static float timeKernelMs(KernelFunc kernel,\n",
        "                          dim3 grid, dim3 block,\n",
        "                          const float* dA, const float* dB, float* dC,\n",
        "                          int M, int N, int K,\n",
        "                          int warmup, int iters) {\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        kernel<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < iters; ++i) {\n",
        "        kernel<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    return ms / iters;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO (optional): GFLOPS estimator\n",
        "// FLOPs ‚âà 2*M*N*K\n",
        "// ============================================================\n",
        "static double estimateGFLOPS(int M, int N, int K, float ms) {\n",
        "    // TODO\n",
        "    double flops = 2.0 * (double)M * (double)N * (double)K;   // mul+add\n",
        "    double gflops = flops / (ms * 1e-3) / 1e9;\n",
        "    return gflops;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Sizes (try non-multiples of TILE to test boundary guards)\n",
        "    const int M = 1024;\n",
        "    const int N = 1024;\n",
        "    const int K = 1024;\n",
        "\n",
        "    // Tolerance (float accumulation)\n",
        "    const float tol = 1e-3f; // TODO (e.g., 1e-3 or 1e-2)\n",
        "\n",
        "    const size_t bytesA = size_t(M) * K * sizeof(float);\n",
        "    const size_t bytesB = size_t(K) * N * sizeof(float);\n",
        "    const size_t bytesC = size_t(M) * N * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytesA);\n",
        "    float* hB = (float*)std::malloc(bytesB);\n",
        "    float* hC_ref = (float*)std::malloc(bytesC);\n",
        "    float* hC_gpu = (float*)std::malloc(bytesC);\n",
        "\n",
        "    if (!hA || !hB || !hC_ref || !hC_gpu) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init\n",
        "    for (int i = 0; i < M * K; ++i) hA[i] = 0.001f * (i % 1000);\n",
        "    for (int i = 0; i < K * N; ++i) hB[i] = 0.002f * (i % 1000);\n",
        "\n",
        "    // CPU reference\n",
        "    matmulCPU(hA, hB, hC_ref, M, N, K);\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA=nullptr, *dB=nullptr, *dC=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytesA));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytesB));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, bytesC));\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytesA, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB, bytesB, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: choose block/grid for 2D kernels\n",
        "    // Typical:\n",
        "    //   block = (TILE, TILE)\n",
        "    //   grid  = (ceil(N/TILE), ceil(M/TILE))\n",
        "    // ------------------------------------------------------------\n",
        "\n",
        "    dim3 block(TILE, TILE, 1); // TODO\n",
        "    dim3 grid((N + TILE - 1)/TILE, (M + TILE - 1)/TILE, 1);  // TODO\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Choose which kernel to run:\n",
        "    //  - Day1: matmulNaive\n",
        "    //  - Day2: matmulBlockTiled\n",
        "    //  - Day3: matmulSharedTiled\n",
        "    // ------------------------------------------------------------\n",
        "    int which = 2; // TODO: 1/2/3\n",
        "\n",
        "    // Launch for correctness + timing\n",
        "    if (which == 1) {\n",
        "        matmulNaive<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    } else if (which == 2) {\n",
        "        matmulBlockTiled<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    } else {\n",
        "        matmulSharedTiled<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(hC_gpu, dC, bytesC, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Correctness\n",
        "    bool ok = checkClose(hC_gpu, hC_ref, M * N, tol);\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // Timing\n",
        "    const int warmup = 5;\n",
        "    const int iters = 20;\n",
        "    float ms = 0.0f;\n",
        "\n",
        "    /*\n",
        "    if (which == 1) {\n",
        "        ms = timeKernelMs(matmulNaive, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    } else if (which == 2) {\n",
        "        ms = timeKernelMs(matmulBlockTiled, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    } else {\n",
        "        ms = timeKernelMs(matmulSharedTiled, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    }*/\n",
        "\n",
        "\n",
        "\n",
        "    ms = timeKernelMs(matmulNaive, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    double gflops = estimateGFLOPS(M, N, K, ms);\n",
        "    printf(\"[Naive MatMul Kernel (Baseline)]                          Kernel=%d | time=%.4f ms | GFLOPS=%.2f\\n\", 1, ms, gflops);\n",
        "\n",
        "\n",
        "    ms = timeKernelMs(matmulBlockTiled, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    gflops = estimateGFLOPS(M, N, K, ms);\n",
        "    printf(\"[Block Tiling Kernel (structure only, still global loads)]Kernel=%d | time=%.4f ms | GFLOPS=%.2f\\n\", 2, ms, gflops);\n",
        "\n",
        "    ms = timeKernelMs(matmulSharedTiled, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    gflops = estimateGFLOPS(M, N, K, ms);\n",
        "    printf(\"[Shared-Memory Tiled Kernel(fix size)]                    Kernel=%d | time=%.4f ms | GFLOPS=%.2f\\n\", 3, ms, gflops);\n",
        "\n",
        "\n",
        "    ms = timeKernelMs(matmulSharedTiled, grid, block, dA, dB, dC, M, N, K, warmup, iters);\n",
        "    gflops = estimateGFLOPS(M, N, K, ms);\n",
        "    printf(\"[Shared-Memory Tiled Kernel(dynamic size)]                Kernel=%d | time=%.4f ms | GFLOPS=%.2f\\n\", 4, ms, gflops);\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hC_ref);\n",
        "    std::free(hC_gpu);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matmul_skeleton.cu -o matmul_skeleton\n",
        "!./matmul_skeleton"
      ],
      "metadata": {
        "id": "oNqbVBUnpdIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1526549f-4aa2-41ee-c5ef-82b334b3d0ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mmatmul_skeleton.cu(122)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity As is treated as a static definition\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mmatmul_skeleton.cu(123)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity Bs is treated as a static definition\n",
            "\n",
            "Correctness: PASS\n",
            "[Naive MatMul Kernel (Baseline)]                          Kernel=2 | time=9.1808 ms | GFLOPS=233.91\n",
            "[Block Tiling Kernel (structure only, still global loads)]Kernel=2 | time=4.3786 ms | GFLOPS=490.45\n",
            "[Shared-Memory Tiled Kernel(fix size)]                    Kernel=2 | time=2.6231 ms | GFLOPS=818.69\n",
            "[Shared-Memory Tiled Kernel(dynamic size)]                Kernel=2 | time=2.4891 ms | GFLOPS=862.75\n"
          ]
        }
      ]
    }
  ]
}