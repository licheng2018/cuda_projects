{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# üìò Task Explanation: PyTorch C++ Extensions ‚Äî TensorAccessor & ATen API\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to understand the **foundations of PyTorch C++ extensions** and learn how to write **custom C++/CUDA operators** that integrate seamlessly with PyTorch.\n",
        "\n",
        "This task focuses on:\n",
        "- How PyTorch exposes tensors to C++/CUDA\n",
        "- How to safely and efficiently access tensor data\n",
        "- How custom operators interact with PyTorch‚Äôs autograd system\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: Why PyTorch C++ Extensions?\n",
        "While Python is ideal for model definition and experimentation, **performance-critical operators** (e.g., LayerNorm, Softmax, fused kernels) are often implemented in **C++/CUDA**.\n",
        "\n",
        "PyTorch C++ extensions allow you to:\n",
        "- Write custom high-performance kernels\n",
        "- Call them directly from Python\n",
        "- Register forward and backward functions\n",
        "- Participate fully in PyTorch‚Äôs autograd system\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî PyTorch C++ Extension Basics\n",
        "\n",
        "### Task\n",
        "Learn the basic structure of a PyTorch C++ extension and how it is built and loaded.\n",
        "\n",
        "You should understand:\n",
        "- How to create a C++ extension using `torch.utils.cpp_extension`\n",
        "- How Python code loads compiled shared libraries\n",
        "- The role of `PYBIND11_MODULE` in binding C++ functions to Python\n",
        "- The difference between:\n",
        "  - Pure C++ extensions\n",
        "  - C++ + CUDA extensions\n",
        "\n",
        "### Key Concepts\n",
        "- `setup.py` or `load()` workflow\n",
        "- CMake / NVCC integration\n",
        "- ABI compatibility with PyTorch\n",
        "- CPU vs CUDA dispatch\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî TensorAccessor\n",
        "\n",
        "### What Is TensorAccessor?\n",
        "`TensorAccessor` is a lightweight wrapper that provides **type-safe and bounds-aware access** to tensor data inside CUDA kernels.\n",
        "\n",
        "It allows you to:\n",
        "- Index tensors using `tensor[i][j]` syntax\n",
        "- Avoid manual pointer arithmetic\n",
        "- Improve code readability and safety\n",
        "\n",
        "### Task\n",
        "Learn how to:\n",
        "- Convert a PyTorch tensor to a `TensorAccessor`\n",
        "- Use `TensorAccessor` inside CUDA kernels\n",
        "- Understand layout assumptions (contiguous, strides)\n",
        "\n",
        "### Key Considerations\n",
        "- Tensor must be contiguous (or you must handle strides explicitly)\n",
        "- Access patterns affect memory coalescing\n",
        "- TensorAccessor does not perform automatic bounds checking on device\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî ATen API\n",
        "\n",
        "### What Is ATen?\n",
        "**ATen** is PyTorch‚Äôs core C++ tensor library.  \n",
        "It provides:\n",
        "- Tensor creation and manipulation\n",
        "- Device and dtype abstraction\n",
        "- Dispatch to CPU or CUDA implementations\n",
        "\n",
        "### Task\n",
        "Learn how to:\n",
        "- Use `at::Tensor` in C++ code\n",
        "- Access tensor metadata (shape, dtype, device)\n",
        "- Launch CUDA kernels using ATen utilities\n",
        "- Write device-agnostic code where possible\n",
        "\n",
        "### Common ATen Operations\n",
        "- `tensor.data_ptr<T>()`\n",
        "- `tensor.size(dim)`\n",
        "- `tensor.stride(dim)`\n",
        "- `at::zeros_like`, `at::empty`\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- How does PyTorch pass tensors from Python to C++?\n",
        "- What are the differences between `data_ptr` and `TensorAccessor`?\n",
        "- When should you prefer ATen APIs over raw CUDA code?\n",
        "- How does PyTorch ensure correct device and dtype dispatch?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A minimal PyTorch C++ extension that can be imported in Python\n",
        "2. A C++ function that:\n",
        "   - Accepts `at::Tensor` inputs\n",
        "   - Accesses tensor data using `TensorAccessor`\n",
        "3. A basic CUDA kernel launched via ATen\n",
        "4. A short write-up explaining:\n",
        "   - Data flow from Python ‚Üí C++ ‚Üí CUDA\n",
        "   - Why this approach is used in real ML systems\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How PyTorch integrates Python, C++, and CUDA\n",
        "- How tensors are represented internally\n",
        "- How high-performance ML operators are built\n",
        "- How to extend PyTorch beyond Python\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "PyTorch C++ extensions are used in:\n",
        "- Custom fused operators\n",
        "- FlashAttention and fused LN kernels\n",
        "- High-performance training and inference backends\n",
        "\n",
        "Mastering TensorAccessor and ATen is a **key step toward ML Systems and CUDA kernel engineering roles**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaway\n",
        "> **PyTorch C++ extensions bridge the gap between Python productivity and C++/CUDA performance, enabling production-grade ML operators.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "84d54ec2-4d78-4c76-919f-70cf8a1753ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sat Jan 24 12:44:22 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyuqf4e1iZBl"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß© Task: PyTorch C++/CUDA Extension on Colab \n",
        "\n",
        "## üéØ Goal\n",
        "In this task, you will build a **minimal PyTorch C++/CUDA extension** directly in **Google Colab** that:\n",
        "\n",
        "- Compiles C++ + CUDA code using PyTorch utilities\n",
        "- Uses **ATen API** in C++\n",
        "- Uses **TensorAccessor** inside a CUDA kernel\n",
        "- Is callable from Python\n",
        "\n",
        "‚ö†Ô∏è This is a **skeleton only**. You must fill in all TODO sections.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Environment Assumptions\n",
        "- Google Colab with **GPU enabled**\n",
        "- CUDA already available via PyTorch\n",
        "- No local files required (everything written via `%%writefile`)\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Step 1 ‚Äî Create C++ Interface (ext.h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnGkyLiaCIfU",
        "outputId": "462f7e93-ed38-464b-bbae-3e682724c874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing softmax_profile_compare.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ext.h\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// C++ forward declaration\n",
        "torch::Tensor my_op_forward(torch::Tensor input);\n",
        "\n",
        "// CUDA launcher declaration\n",
        "void my_op_cuda_launcher(torch::Tensor input, torch::Tensor output);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß± Step 2 ‚Äî C++ Wrapper with ATen (ext.cpp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ext.cpp\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: ATen wrapper\n",
        "// Requirements:\n",
        "//  - Validate input device (CUDA), dtype (float32), contiguous, 2D\n",
        "//  - Allocate output tensor with same shape/device/dtype\n",
        "//  - Call CUDA launcher\n",
        "//  - Return output\n",
        "// ------------------------------------------------------------\n",
        "torch::Tensor my_op_forward(torch::Tensor input) {\n",
        "    // TODO: checks (CUDA, contiguous, dtype, dim)\n",
        "    // TODO: allocate output using ATen\n",
        "    // TODO: call my_op_cuda_launcher(input, output)\n",
        "    // TODO: return output\n",
        "    return torch::Tensor();\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// PyBind\n",
        "// ------------------------------------------------------------\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &my_op_forward, \"MyOp forward (CUDA)\");\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß± Step 3 ‚Äî Write CUDA Kernel + TensorAccessor (ext_cuda.cu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ext_cuda.cu\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_FLOAT(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "#define CHECK_2D(x) TORCH_CHECK((x).dim() == 2, #x \" must be 2D\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x); CHECK_FLOAT(x); CHECK_2D(x)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: CUDA kernel using TensorAccessor\n",
        "// Input/Output shape: [B, D]\n",
        "// Task: elementwise transform y[b,d] = f(x[b,d]) (you choose f)\n",
        "// Requirements:\n",
        "//  - Use TensorAccessor (PackedTensorAccessor32)\n",
        "//  - Correct indexing (b,d)\n",
        "//  - Bounds checks\n",
        "// ------------------------------------------------------------\n",
        "__global__ void my_kernel(\n",
        "    torch::PackedTensorAccessor32<float, 2, torch::RestrictPtrTraits> x,\n",
        "    torch::PackedTensorAccessor32<float, 2, torch::RestrictPtrTraits> y,\n",
        "    int B, int D\n",
        ") {\n",
        "    // TODO:\n",
        "    // - compute b, d from blockIdx/threadIdx\n",
        "    // - if (b < B && d < D) { y[b][d] = ...; }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: CUDA launcher\n",
        "// Requirements:\n",
        "//  - Validate tensors\n",
        "//  - Choose block/grid\n",
        "//  - Create accessors from tensors\n",
        "//  - Launch kernel\n",
        "// ------------------------------------------------------------\n",
        "void my_op_cuda_launcher(torch::Tensor input, torch::Tensor output) {\n",
        "    CHECK_INPUT(input);\n",
        "    CHECK_INPUT(output);\n",
        "\n",
        "    // TODO: get B, D from input sizes\n",
        "    // int B = ...\n",
        "    // int D = ...\n",
        "\n",
        "    // TODO: choose dim3 block, grid\n",
        "    // dim3 block(...);\n",
        "    // dim3 grid(...);\n",
        "\n",
        "    // TODO: launch kernel with accessors\n",
        "    // my_kernel<<<grid, block>>>(..., ..., B, D);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 4 ‚Äî Colab Cell 4 ‚Äî Build Extension (JIT Compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TIP: set verbose=True if compilation issues\n",
        "ext = load(\n",
        "    name=\"tensor_accessor_ext\",\n",
        "    sources=[\"ext.cpp\", \"ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Extension loaded:\", ext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 5 ‚Äî CPU Reference + Test Harness (Correctness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: CPU reference (must match your CUDA kernel's f(x))\n",
        "# Requirements:\n",
        "#  - input: x [B, D] on CPU\n",
        "#  - output: y [B, D] on CPU\n",
        "# ------------------------------------------------------------\n",
        "def my_op_cpu_reference(x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: implement same math as CUDA kernel\n",
        "    return torch.empty_like(x)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: correctness check\n",
        "# Requirements:\n",
        "#  - create test tensor on CUDA\n",
        "#  - run ext.forward\n",
        "#  - compare with CPU reference (move tensors appropriately)\n",
        "#  - print max error\n",
        "# ------------------------------------------------------------\n",
        "def test_correctness(B=256, D=1024, atol=1e-5, rtol=1e-4):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "test_correctness()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 6 ‚Äî Benchmark (CUDA Events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: benchmark\n",
        "# Requirements:\n",
        "#  - time ext.forward(x) with CUDA events\n",
        "#  - include warmup\n",
        "#  - print average ms\n",
        "# ------------------------------------------------------------\n",
        "def benchmark(B=4096, D=1024, iters=200, warmup=20):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "benchmark()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 7 ‚Äî Nsight Compute Profiling Script (Colab-Friendly Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile run_ncu_ext.sh\n",
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# Build is handled by Python JIT in this notebook.\n",
        "# This script is a placeholder to show how you would profile if `ncu` is available.\n",
        "\n",
        "# TODO:\n",
        "# 1) Ensure your workload triggers the kernel\n",
        "# 2) Use --kernel-name to match your kernel name\n",
        "# Example commands:\n",
        "\n",
        "# ncu --set full --kernel-name \"my_kernel\" -o ncu_report python your_driver.py\n",
        "echo \"If Nsight Compute is available, run something like:\"\n",
        "echo \"ncu --set full --kernel-name my_kernel -o ncu_report python driver.py\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!chmod +x run_ncu_ext.sh\n",
        "!./run_ncu_ext.sh\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
