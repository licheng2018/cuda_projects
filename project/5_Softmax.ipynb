{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation: Softmax CUDA Kernel Implementation and Profiling\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "The objective of this task is to implement a **Softmax CUDA kernel** and then use **GPU profiling tools** to analyze its performance characteristics.  \n",
        "Softmax is a **reduction-heavy and numerically sensitive** operator, making it an ideal case study for understanding GPU parallelism, memory access patterns, and performance bottlenecks.\n",
        "\n",
        "This task emphasizes both **correctness** and **performance analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Background: What Is Softmax?\n",
        "For an input vector \\( x \\in \\mathbb{R}^D \\), Softmax is defined as:\n",
        "\\[\n",
        "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{D} e^{x_j}}\n",
        "\\]\n",
        "\n",
        "In practice, a **numerically stable form** is used:\n",
        "\\[\n",
        "\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{D} e^{x_j - \\max(x)}}\n",
        "\\]\n",
        "\n",
        "Softmax involves:\n",
        "- A **max reduction**\n",
        "- A **sum reduction**\n",
        "- Elementwise exponentiation and normalization\n",
        "\n",
        "These operations make Softmax both **compute-intensive** and **memory-sensitive**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part A â€” Softmax CUDA Kernel\n",
        "\n",
        "### Task\n",
        "Design and implement a CUDA kernel for Softmax where:\n",
        "- Each row (or vector) is processed independently\n",
        "- Reductions (max and sum) are parallelized\n",
        "- Numerical stability is ensured via the **subtract-max trick**\n",
        "\n",
        "### Key Design Considerations\n",
        "- How to map rows to thread blocks or warps\n",
        "- How to implement max and sum reductions efficiently\n",
        "- Whether to use shared memory or warp-level primitives\n",
        "- Minimizing redundant global memory accesses\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Correctness Requirements\n",
        "- Use a numerically stable Softmax formulation\n",
        "- Match a CPU reference implementation within tolerance\n",
        "- Handle edge cases (large/small values, varying vector length)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part B â€” Profiling the Softmax Kernel\n",
        "\n",
        "### Task\n",
        "Profile the Softmax CUDA kernel using GPU profiling tools such as:\n",
        "- **Nsight Compute** (kernel-level analysis)\n",
        "- (Optional) **Nsight Systems** (application-level timeline)\n",
        "\n",
        "### What to Analyze\n",
        "- Kernel execution time\n",
        "- Warp execution efficiency\n",
        "- Memory throughput and cache behavior\n",
        "- Warp stalls vs memory stalls\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Key Metrics to Inspect\n",
        "- Occupancy and active warps\n",
        "- Stall reasons (e.g., memory dependency, execution dependency)\n",
        "- Global memory load efficiency\n",
        "- Shared memory usage and bank conflicts (if used)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Key Questions to Answer\n",
        "- Is the kernel **memory-bound or compute-bound**?\n",
        "- Which stage dominates runtime: max reduction, sum reduction, or normalization?\n",
        "- Are reductions efficiently parallelized?\n",
        "- Could kernel fusion or reduced memory traffic improve performance?\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Deliverables\n",
        "You should produce:\n",
        "1. A Softmax CUDA kernel implementation\n",
        "2. A CPU reference for correctness verification\n",
        "3. Profiling reports (Nsight Compute)\n",
        "4. A short analysis explaining:\n",
        "   - Performance bottlenecks\n",
        "   - Dominant stall reasons\n",
        "   - Possible optimization directions\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to implement reduction-heavy kernels on GPU\n",
        "- Why numerical stability matters in GPU kernels\n",
        "- How to interpret profiling metrics for real ML operators\n",
        "- How Softmax kernels are optimized in practice\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Relevance to ML Systems\n",
        "Softmax is a core component in:\n",
        "- Attention mechanisms\n",
        "- Classification layers\n",
        "- Transformer models\n",
        "\n",
        "Efficient Softmax implementations are critical for:\n",
        "- LLM training and inference\n",
        "- Kernel fusion (e.g., FlashAttention)\n",
        "- High-performance ML systems\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Key Takeaway\n",
        "> **Softmax combines numerical stability challenges with reduction-heavy computation, making it a perfect kernel for learning both CUDA optimization and GPU profiling.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "4eb776bf-2e9c-499a-e5ef-6ad8791ff21b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Wed Jan 21 12:31:33 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyuqf4e1iZBl"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile softmax_profile_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cstring>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "static inline float frand(unsigned int &s) {\n",
        "  s = 1664525u * s + 1013904223u;\n",
        "  return (s & 0x00FFFFFF) / float(0x01000000); // [0,1)\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference: numerically stable softmax per row\n",
        "// y[b, i] = exp(x[b, i] - max) / sum_j exp(x[b, j] - max)\n",
        "// ============================================================\n",
        "static void softmax_cpu(const float* x, float* y, int B, int D) {\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    float* yb = y + b * D;\n",
        "\n",
        "    float m = -INFINITY;\n",
        "    for (int i = 0; i < D; ++i) m = fmaxf(m, xb[i]);\n",
        "\n",
        "    double sum = 0.0;\n",
        "    for (int i = 0; i < D; ++i) sum += std::exp((double)(xb[i] - m));\n",
        "\n",
        "    double inv = 1.0 / sum;\n",
        "    for (int i = 0; i < D; ++i) yb[i] = (float)(std::exp((double)(xb[i] - m)) * inv);\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Warp reduction helpers using __shfl_down_sync\n",
        "// ============================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "  // TODO\n",
        "  return v;\n",
        "}\n",
        "__device__ __forceinline__ float warpReduceMax(float v) {\n",
        "  // TODO\n",
        "  return v;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Softmax CUDA kernel\n",
        "// Inputs:\n",
        "//   x: [B, D] row-major\n",
        "// Outputs:\n",
        "//   y: [B, D]\n",
        "// Requirements:\n",
        "//   - numerically stable subtract-max\n",
        "//   - parallelize max + sum reductions\n",
        "//   - avoid OOB and handle any D\n",
        "// Notes:\n",
        "//   - you can do 1 warp per row, 1 block per row, or multi-warp per row\n",
        "// ============================================================\n",
        "__global__ void softmax_kernel(const float* __restrict__ x,\n",
        "                               float* __restrict__ y,\n",
        "                               int B, int D) {\n",
        "  // TODO\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Accuracy checker\n",
        "// ============================================================\n",
        "static bool check_allclose(const float* a, const float* b, int n, float rtol, float atol) {\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    float av = a[i], bv = b[i];\n",
        "    float diff = std::fabs(av - bv);\n",
        "    float tol = atol + rtol * std::fabs(bv);\n",
        "    if (diff > tol || std::isnan(av) || std::isnan(bv)) {\n",
        "      printf(\"Mismatch at %d: got=%g ref=%g diff=%g tol=%g\\n\", i, av, bv, diff, tol);\n",
        "      return false;\n",
        "    }\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Benchmark helper (CUDA events)\n",
        "// ============================================================\n",
        "template <typename Kernel, typename... Args>\n",
        "static float bench_kernel_ms(Kernel k, dim3 grid, dim3 block,\n",
        "                             int warmup, int iters, Args... args) {\n",
        "  for (int i = 0; i < warmup; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < iters; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "  return ms / iters;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // Args: ./softmax <B> <D> <iters>\n",
        "  int B = (argc > 1) ? std::atoi(argv[1]) : 4096;\n",
        "  int D = (argc > 2) ? std::atoi(argv[2]) : 1024;\n",
        "  int iters = (argc > 3) ? std::atoi(argv[3]) : 200;\n",
        "\n",
        "  printf(\"Softmax skeleton: B=%d D=%d iters=%d\\n\", B, D, iters);\n",
        "\n",
        "  const int n = B * D;\n",
        "  const size_t bytes = size_t(n) * sizeof(float);\n",
        "\n",
        "  float* hx = (float*)std::malloc(bytes);\n",
        "  float* hy_ref = (float*)std::malloc(bytes);\n",
        "  float* hy_gpu = (float*)std::malloc(bytes);\n",
        "\n",
        "  if (!hx || !hy_ref || !hy_gpu) {\n",
        "    fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  // Init inputs (include some large values to stress stability)\n",
        "  unsigned int st = 123u;\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    float r = (frand(st) - 0.5f) * 2.0f;\n",
        "    float scale = (i % 97 == 0) ? 20.0f : 3.0f; // occasional large magnitude\n",
        "    hx[i] = r * scale;\n",
        "  }\n",
        "\n",
        "  // CPU reference\n",
        "  softmax_cpu(hx, hy_ref, B, D);\n",
        "\n",
        "  // Device alloc\n",
        "  float *dx=nullptr, *dy=nullptr;\n",
        "  CUDA_CHECK(cudaMalloc(&dx, bytes));\n",
        "  CUDA_CHECK(cudaMalloc(&dy, bytes));\n",
        "  CUDA_CHECK(cudaMemcpy(dx, hx, bytes, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemset(dy, 0, bytes));\n",
        "\n",
        "  // ------------------------------------------------------------\n",
        "  // TODO: choose launch config\n",
        "  // Common choices:\n",
        "  //  - 1 block per row: grid=(B), block=(some threads)\n",
        "  //  - or 1 warp per row: grid=(ceil(B / warpsPerBlock)), block=(warpsPerBlock*32)\n",
        "  // ------------------------------------------------------------\n",
        "  dim3 block(0, 0, 1); // TODO\n",
        "  dim3 grid(0, 0, 1);  // TODO\n",
        "\n",
        "  // Correctness run\n",
        "  softmax_kernel<<<grid, block>>>(dx, dy, B, D);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hy_gpu, dy, bytes, cudaMemcpyDeviceToHost));\n",
        "  bool ok = check_allclose(hy_gpu, hy_ref, n, 1e-4f, 1e-5f);\n",
        "  printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "  // Benchmark\n",
        "  int warmup = 20;\n",
        "  float ms = bench_kernel_ms(softmax_kernel, grid, block, warmup, iters, dx, dy, B, D);\n",
        "  printf(\"Benchmark: %.4f ms\\n\", ms);\n",
        "\n",
        "  // Cleanup\n",
        "  CUDA_CHECK(cudaFree(dx));\n",
        "  CUDA_CHECK(cudaFree(dy));\n",
        "  std::free(hx);\n",
        "  std::free(hy_ref);\n",
        "  std::free(hy_gpu);\n",
        "  return ok ? 0 : 2;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile run_ncu_softmax.sh\n",
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "B=\"${1:-4096}\"\n",
        "D=\"${2:-1024}\"\n",
        "ITERS=\"${3:-200}\"\n",
        "\n",
        "nvcc -O3 -lineinfo -o softmax softmax_profile_skeleton.cu\n",
        "./softmax \"${B}\" \"${D}\" \"${ITERS}\"\n",
        "\n",
        "# Nsight Compute profiling\n",
        "ncu --set full \\\n",
        "  --kernel-name \"softmax_kernel\" \\\n",
        "  --launch-skip 0 --launch-count 1 \\\n",
        "  -o \"ncu_softmax_B${B}_D${D}\" \\\n",
        "  ./softmax \"${B}\" \"${D}\" \"${ITERS}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!chmod +x run_ncu_softmax.sh\n",
        "!./run_ncu_softmax.sh 4096 1024 200"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
