{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# üìò Task Explanation: Softmax CUDA Kernel Implementation and Profiling\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to implement a **Softmax CUDA kernel** and then use **GPU profiling tools** to analyze its performance characteristics.  \n",
        "Softmax is a **reduction-heavy and numerically sensitive** operator, making it an ideal case study for understanding GPU parallelism, memory access patterns, and performance bottlenecks.\n",
        "\n",
        "This task emphasizes both **correctness** and **performance analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is Softmax?\n",
        "For an input vector \\( x \\in \\mathbb{R}^D \\), Softmax is defined as:\n",
        "\\[\n",
        "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{D} e^{x_j}}\n",
        "\\]\n",
        "\n",
        "In practice, a **numerically stable form** is used:\n",
        "\\[\n",
        "\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{D} e^{x_j - \\max(x)}}\n",
        "\\]\n",
        "\n",
        "Softmax involves:\n",
        "- A **max reduction**\n",
        "- A **sum reduction**\n",
        "- Elementwise exponentiation and normalization\n",
        "\n",
        "These operations make Softmax both **compute-intensive** and **memory-sensitive**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Softmax CUDA Kernel\n",
        "\n",
        "### Task\n",
        "Design and implement a CUDA kernel for Softmax where:\n",
        "- Each row (or vector) is processed independently\n",
        "- Reductions (max and sum) are parallelized\n",
        "- Numerical stability is ensured via the **subtract-max trick**\n",
        "\n",
        "### Key Design Considerations\n",
        "- How to map rows to thread blocks or warps\n",
        "- How to implement max and sum reductions efficiently\n",
        "- Whether to use shared memory or warp-level primitives\n",
        "- Minimizing redundant global memory accesses\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Correctness Requirements\n",
        "- Use a numerically stable Softmax formulation\n",
        "- Match a CPU reference implementation within tolerance\n",
        "- Handle edge cases (large/small values, varying vector length)\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Profiling the Softmax Kernel\n",
        "\n",
        "### Task\n",
        "Profile the Softmax CUDA kernel using GPU profiling tools such as:\n",
        "- **Nsight Compute** (kernel-level analysis)\n",
        "- (Optional) **Nsight Systems** (application-level timeline)\n",
        "\n",
        "### What to Analyze\n",
        "- Kernel execution time\n",
        "- Warp execution efficiency\n",
        "- Memory throughput and cache behavior\n",
        "- Warp stalls vs memory stalls\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Key Metrics to Inspect\n",
        "- Occupancy and active warps\n",
        "- Stall reasons (e.g., memory dependency, execution dependency)\n",
        "- Global memory load efficiency\n",
        "- Shared memory usage and bank conflicts (if used)\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- Is the kernel **memory-bound or compute-bound**?\n",
        "- Which stage dominates runtime: max reduction, sum reduction, or normalization?\n",
        "- Are reductions efficiently parallelized?\n",
        "- Could kernel fusion or reduced memory traffic improve performance?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A Softmax CUDA kernel implementation\n",
        "2. A CPU reference for correctness verification\n",
        "3. Profiling reports (Nsight Compute)\n",
        "4. A short analysis explaining:\n",
        "   - Performance bottlenecks\n",
        "   - Dominant stall reasons\n",
        "   - Possible optimization directions\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to implement reduction-heavy kernels on GPU\n",
        "- Why numerical stability matters in GPU kernels\n",
        "- How to interpret profiling metrics for real ML operators\n",
        "- How Softmax kernels are optimized in practice\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Softmax is a core component in:\n",
        "- Attention mechanisms\n",
        "- Classification layers\n",
        "- Transformer models\n",
        "\n",
        "Efficient Softmax implementations are critical for:\n",
        "- LLM training and inference\n",
        "- Kernel fusion (e.g., FlashAttention)\n",
        "- High-performance ML systems\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaway\n",
        "> **Softmax combines numerical stability challenges with reduction-heavy computation, making it a perfect kernel for learning both CUDA optimization and GPU profiling.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "84d54ec2-4d78-4c76-919f-70cf8a1753ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sat Jan 24 12:44:22 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyuqf4e1iZBl"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnGkyLiaCIfU",
        "outputId": "462f7e93-ed38-464b-bbae-3e682724c874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing softmax_profile_compare.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile softmax_profile_compare.cu\n",
        "// Compare 3 CUDA softmax strategies with profiling:\n",
        "//  (1) warp-per-row\n",
        "//  (2) block-per-row\n",
        "//  (3) multi-warp-per-row (tunable warpsPerRow)\n",
        "//\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <algorithm>\n",
        "#include <random>\n",
        "#include <iostream>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                      \\\n",
        "  cudaError_t err = (call);                                        \\\n",
        "  if (err != cudaSuccess) {                                        \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                      \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));          \\\n",
        "    std::exit(EXIT_FAILURE);                                       \\\n",
        "  }                                                                \\\n",
        "} while(0)\n",
        "\n",
        "static inline int div_up(int a, int b) { return (a + b - 1) / b; }\n",
        "\n",
        "// ======================= Warp-level reduce =======================\n",
        "\n",
        "__device__ __forceinline__ float warpReduceMax(float v, unsigned mask = 0xffffffffu) {\n",
        "  v = fmaxf(v, __shfl_down_sync(mask, v, 16));\n",
        "  v = fmaxf(v, __shfl_down_sync(mask, v, 8));\n",
        "  v = fmaxf(v, __shfl_down_sync(mask, v, 4));\n",
        "  v = fmaxf(v, __shfl_down_sync(mask, v, 2));\n",
        "  v = fmaxf(v, __shfl_down_sync(mask, v, 1));\n",
        "  return v;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ float warpReduceSum(float v, unsigned mask = 0xffffffffu) {\n",
        "  v += __shfl_down_sync(mask, v, 16);\n",
        "  v += __shfl_down_sync(mask, v, 8);\n",
        "  v += __shfl_down_sync(mask, v, 4);\n",
        "  v += __shfl_down_sync(mask, v, 2);\n",
        "  v += __shfl_down_sync(mask, v, 1);\n",
        "  return v;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ float warpAllReduceMax(float v, unsigned mask = 0xffffffffu) {\n",
        "  float m = warpReduceMax(v, mask);\n",
        "  return __shfl_sync(mask, m, 0);\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ float warpAllReduceSum(float v, unsigned mask = 0xffffffffu) {\n",
        "  float s = warpReduceSum(v, mask);\n",
        "  return __shfl_sync(mask, s, 0);\n",
        "}\n",
        "\n",
        "// ======================= Block-level reduce =======================\n",
        "// Assumes blockDim.x is multiple of 32 and <= 1024\n",
        "__device__ __forceinline__ float blockReduceMax(float v) {\n",
        "  __shared__ float shm[32]; // up to 32 warps\n",
        "  int lane = threadIdx.x & 31;\n",
        "  int warp = threadIdx.x >> 5;\n",
        "  int numWarps = (blockDim.x + 31) >> 5;\n",
        "\n",
        "  v = warpReduceMax(v);\n",
        "  if (lane == 0) shm[warp] = v;\n",
        "  __syncthreads();\n",
        "\n",
        "  float out = -INFINITY;\n",
        "  if (warp == 0) {\n",
        "    out = (lane < numWarps) ? shm[lane] : -INFINITY;\n",
        "    out = warpReduceMax(out);\n",
        "  }\n",
        "  out = __shfl_sync(0xffffffffu, out, 0);\n",
        "  return out;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ float blockReduceSum(float v) {\n",
        "  __shared__ float shm[32];\n",
        "  int lane = threadIdx.x & 31;\n",
        "  int warp = threadIdx.x >> 5;\n",
        "  int numWarps = (blockDim.x + 31) >> 5;\n",
        "\n",
        "  v = warpReduceSum(v);\n",
        "  if (lane == 0) shm[warp] = v;\n",
        "  __syncthreads();\n",
        "\n",
        "  float out = 0.0f;\n",
        "  if (warp == 0) {\n",
        "    out = (lane < numWarps) ? shm[lane] : 0.0f;\n",
        "    out = warpReduceSum(out);\n",
        "  }\n",
        "  out = __shfl_sync(0xffffffffu, out, 0);\n",
        "  return out;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// (1) Warp-per-row kernel\n",
        "// Mapping:\n",
        "//   - 1 warp handles 1 row\n",
        "//   - 1 block contains multiple warps => handles multiple rows\n",
        "// ============================================================\n",
        "__global__ void softmax_kernel_warp_per_row(const float* __restrict__ x,\n",
        "                                            float* __restrict__ y,\n",
        "                                            int B, int D) {\n",
        "  int tid  = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5;\n",
        "  int warpsPerBlock = blockDim.x >> 5;\n",
        "\n",
        "  int row = blockIdx.x * warpsPerBlock + warp;\n",
        "  if (row >= B) return;\n",
        "\n",
        "  const float* row_x = x + row * D;\n",
        "  float* row_y = y + row * D;\n",
        "\n",
        "  float local_max = -INFINITY;\n",
        "  for (int j = lane; j < D; j += 32) {\n",
        "    local_max = fmaxf(local_max, row_x[j]);\n",
        "  }\n",
        "  float m = warpAllReduceMax(local_max);\n",
        "\n",
        "  float local_sum = 0.0f;\n",
        "  for (int j = lane; j < D; j += 32) {\n",
        "    local_sum += expf(row_x[j] - m);\n",
        "  }\n",
        "  float s = warpAllReduceSum(local_sum);\n",
        "\n",
        "  for (int j = lane; j < D; j += 32) {\n",
        "    row_y[j] = expf(row_x[j] - m) / s;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// (2) Block-per-row kernel\n",
        "// Mapping:\n",
        "//   - 1 block handles 1 row\n",
        "//   - threads stride across D\n",
        "// ============================================================\n",
        "__global__ void softmax_kernel_block_per_row(const float* __restrict__ x,\n",
        "                                             float* __restrict__ y,\n",
        "                                             int B, int D) {\n",
        "  int row = blockIdx.x;\n",
        "  if (row >= B) return;\n",
        "\n",
        "  const float* row_x = x + row * D;\n",
        "  float* row_y = y + row * D;\n",
        "\n",
        "  float local_max = -INFINITY;\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    local_max = fmaxf(local_max, row_x[j]);\n",
        "  }\n",
        "  float m = blockReduceMax(local_max);\n",
        "\n",
        "  float local_sum = 0.0f;\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    local_sum += expf(row_x[j] - m);\n",
        "  }\n",
        "  float s = blockReduceSum(local_sum);\n",
        "\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    row_y[j] = expf(row_x[j] - m) / s;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// (3) Multi-warp-per-row kernel (tunable)\n",
        "// Mapping:\n",
        "//   - 1 block handles 1 row\n",
        "//   - blockDim.x = warpsPerRow * 32  (2/4/8/16 ...)\n",
        "// Purpose:\n",
        "//   - explore tradeoff between \"too few threads\" and \"too much overhead\"\n",
        "//   - closer to attention-style blockwise softmax patterns\n",
        "// ============================================================\n",
        "__global__ void softmax_kernel_multiwarp_per_row(const float* __restrict__ x,\n",
        "                                                 float* __restrict__ y,\n",
        "                                                 int B, int D) {\n",
        "  int row = blockIdx.x;\n",
        "  if (row >= B) return;\n",
        "\n",
        "  const float* row_x = x + row * D;\n",
        "  float* row_y = y + row * D;\n",
        "\n",
        "  float local_max = -INFINITY;\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    local_max = fmaxf(local_max, row_x[j]);\n",
        "  }\n",
        "  float m = blockReduceMax(local_max);\n",
        "\n",
        "  float local_sum = 0.0f;\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    local_sum += expf(row_x[j] - m);\n",
        "  }\n",
        "  float s = blockReduceSum(local_sum);\n",
        "\n",
        "  for (int j = threadIdx.x; j < D; j += blockDim.x) {\n",
        "    row_y[j] = expf(row_x[j] - m) / s;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ======================= CPU reference (optional) =======================\n",
        "\n",
        "static void softmax_cpu_ref(const std::vector<float>& x,\n",
        "                            std::vector<float>& y,\n",
        "                            int B, int D) {\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* row_x = x.data() + b * D;\n",
        "    float* row_y = y.data() + b * D;\n",
        "\n",
        "    float m = -INFINITY;\n",
        "    for (int j = 0; j < D; ++j) m = std::max(m, row_x[j]);\n",
        "\n",
        "    double s = 0.0;\n",
        "    for (int j = 0; j < D; ++j) s += std::exp(double(row_x[j] - m));\n",
        "\n",
        "    for (int j = 0; j < D; ++j) row_y[j] = float(std::exp(double(row_x[j] - m)) / s);\n",
        "  }\n",
        "}\n",
        "\n",
        "static bool check_close(const std::vector<float>& a,\n",
        "                        const std::vector<float>& b,\n",
        "                        float atol, float rtol,\n",
        "                        int B, int D,\n",
        "                        int max_report = 10) {\n",
        "  int bad = 0;\n",
        "  for (int i = 0; i < B * D; ++i) {\n",
        "    float av = a[i], bv = b[i];\n",
        "    float diff = std::fabs(av - bv);\n",
        "    float tol = atol + rtol * std::fabs(bv);\n",
        "    if (!(diff <= tol) || std::isnan(av) || std::isnan(bv)) {\n",
        "      if (bad < max_report) {\n",
        "        int row = i / D;\n",
        "        int col = i % D;\n",
        "        std::fprintf(stderr, \"Mismatch at (row=%d,col=%d): gpu=%g ref=%g diff=%g tol=%g\\n\",\n",
        "                     row, col, av, bv, diff, tol);\n",
        "      }\n",
        "      bad++;\n",
        "      if (bad >= max_report) break;\n",
        "    }\n",
        "  }\n",
        "  return bad == 0;\n",
        "}\n",
        "\n",
        "// ======================= Timing helper =======================\n",
        "\n",
        "template <typename LaunchFn>\n",
        "static float time_kernel_ms(LaunchFn launch, int warmup, int reps) {\n",
        "  // Warmup\n",
        "  for (int i = 0; i < warmup; ++i) launch();\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < reps; ++i) launch();\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "  return ms / reps;\n",
        "}\n",
        "\n",
        "// ======================= CLI =======================\n",
        "\n",
        "static int get_arg_int(int argc, char** argv, const char* name, int def) {\n",
        "  for (int i = 1; i < argc - 1; ++i) {\n",
        "    if (std::string(argv[i]) == name) return std::atoi(argv[i + 1]);\n",
        "  }\n",
        "  return def;\n",
        "}\n",
        "static int has_flag(int argc, char** argv, const char* name) {\n",
        "  for (int i = 1; i < argc; ++i) if (std::string(argv[i]) == name) return 1;\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  int B = get_arg_int(argc, argv, \"--B\", 4096);\n",
        "  int D = get_arg_int(argc, argv, \"--D\", 1024);\n",
        "  int reps = get_arg_int(argc, argv, \"--reps\", 200);\n",
        "  int warmup = get_arg_int(argc, argv, \"--warmup\", 50);\n",
        "  int check = get_arg_int(argc, argv, \"--check\", 0);\n",
        "\n",
        "  int warpBlockWarps = get_arg_int(argc, argv, \"--warp_block_warps\", 8); // for warp-per-row\n",
        "  int blockThreads   = get_arg_int(argc, argv, \"--block_threads\", 256);  // for block-per-row\n",
        "  int mwWarps        = get_arg_int(argc, argv, \"--mw_warps\", 8);          // for multiwarp-per-row\n",
        "\n",
        "  // sanitize\n",
        "  warpBlockWarps = std::max(1, std::min(32, warpBlockWarps));\n",
        "  blockThreads = std::max(32, std::min(1024, blockThreads));\n",
        "  blockThreads = (blockThreads / 32) * 32; // force multiple of 32 for our blockReduce\n",
        "  mwWarps = std::max(1, std::min(32, mwWarps));\n",
        "\n",
        "  std::printf(\"B=%d D=%d reps=%d warmup=%d check=%d\\n\", B, D, reps, warmup, check);\n",
        "  std::printf(\"Configs:\\n\");\n",
        "  std::printf(\"  warp-per-row: warpsPerBlock=%d (block.x=%d)\\n\", warpBlockWarps, warpBlockWarps * 32);\n",
        "  std::printf(\"  block-per-row: blockThreads=%d\\n\", blockThreads);\n",
        "  std::printf(\"  multiwarp-per-row: warpsPerRow=%d (block.x=%d)\\n\", mwWarps, mwWarps * 32);\n",
        "\n",
        "  size_t bytes = size_t(B) * size_t(D) * sizeof(float);\n",
        "  std::vector<float> hx(B * D), hy(B * D), href;\n",
        "  href.resize(B * D);\n",
        "\n",
        "  // Random input with some variance (avoid all small values)\n",
        "  std::mt19937 rng(123);\n",
        "  std::normal_distribution<float> dist(0.0f, 5.0f);\n",
        "  for (auto& v : hx) v = dist(rng);\n",
        "\n",
        "  float *dx = nullptr, *dy = nullptr;\n",
        "  CUDA_CHECK(cudaMalloc(&dx, bytes));\n",
        "  CUDA_CHECK(cudaMalloc(&dy, bytes));\n",
        "  CUDA_CHECK(cudaMemcpy(dx, hx.data(), bytes, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemset(dy, 0, bytes));\n",
        "\n",
        "  // ---------------- Time kernels ----------------\n",
        "  // (1) warp-per-row\n",
        "  dim3 block1(warpBlockWarps * 32, 1, 1);\n",
        "  dim3 grid1(div_up(B, warpBlockWarps), 1, 1);\n",
        "\n",
        "  auto launch1 = [&]() {\n",
        "    softmax_kernel_warp_per_row<<<grid1, block1>>>(dx, dy, B, D);\n",
        "  };\n",
        "  float ms1 = time_kernel_ms(launch1, warmup, reps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "  // (2) block-per-row\n",
        "  dim3 block2(blockThreads, 1, 1);\n",
        "  dim3 grid2(B, 1, 1);\n",
        "  auto launch2 = [&]() {\n",
        "    softmax_kernel_block_per_row<<<grid2, block2>>>(dx, dy, B, D);\n",
        "  };\n",
        "  float ms2 = time_kernel_ms(launch2, warmup, reps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "  // (3) multiwarp-per-row (tunable)\n",
        "  dim3 block3(mwWarps * 32, 1, 1);\n",
        "  dim3 grid3(B, 1, 1);\n",
        "  auto launch3 = [&]() {\n",
        "    softmax_kernel_multiwarp_per_row<<<grid3, block3>>>(dx, dy, B, D);\n",
        "  };\n",
        "  float ms3 = time_kernel_ms(launch3, warmup, reps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "  // ---------------- Correctness (optional) ----------------\n",
        "  if (check) {\n",
        "    // run CPU reference\n",
        "    softmax_cpu_ref(hx, href, B, D);\n",
        "\n",
        "    // check each kernel once\n",
        "    auto run_and_copy = [&](auto launch, const char* name) {\n",
        "      CUDA_CHECK(cudaMemset(dy, 0, bytes));\n",
        "      launch();\n",
        "      CUDA_CHECK(cudaDeviceSynchronize());\n",
        "      CUDA_CHECK(cudaMemcpy(hy.data(), dy, bytes, cudaMemcpyDeviceToHost));\n",
        "      bool ok = check_close(hy, href, /*atol*/1e-5f, /*rtol*/1e-4f, B, D);\n",
        "      std::printf(\"Check %-22s : %s\\n\", name, ok ? \"PASS\" : \"FAIL\");\n",
        "    };\n",
        "    run_and_copy(launch1, \"warp-per-row\");\n",
        "    run_and_copy(launch2, \"block-per-row\");\n",
        "    run_and_copy(launch3, \"multiwarp-per-row\");\n",
        "  }\n",
        "\n",
        "  // ---------------- Report ----------------\n",
        "  auto report = [&](const char* name, float ms) {\n",
        "    // Rough traffic estimate:\n",
        "    // Read x (B*D floats) + write y (B*D floats) => 2 * bytes.\n",
        "    // (softmax also re-reads x in 2nd/3rd loops; here we don't count that extra to keep \"effective\" bandwidth simple)\n",
        "    double gb = (2.0 * double(bytes)) / 1e9;\n",
        "    double gbps = gb / (double(ms) / 1e3);\n",
        "    double ns_per_elem = (double(ms) * 1e6) / double(B) / double(D);\n",
        "    std::printf(\"%-22s : %8.4f ms  |  ~%7.2f GB/s  |  %7.3f ns/elem\\n\", name, ms, gbps, ns_per_elem);\n",
        "  };\n",
        "\n",
        "  std::printf(\"\\n=== Average time per launch ===\\n\");\n",
        "  report(\"warp-per-row\", ms1);\n",
        "  report(\"block-per-row\", ms2);\n",
        "  report(\"multiwarp-per-row\", ms3);\n",
        "\n",
        "  // Suggest next steps for deep profiling\n",
        "  std::printf(\"\\n=== Nsight Compute tips ===\\n\");\n",
        "  std::printf(\"1) Run full profile for all kernels:\\n\");\n",
        "  std::printf(\"   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B %d --D %d --reps 50 --warmup 10\\n\", B, D);\n",
        "  std::printf(\"2) Focus on memory & math throughput:\\n\");\n",
        "  std::printf(\"   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B %d --D %d --reps 50 --warmup 10\\n\", B, D);\n",
        "  std::printf(\"3) Look at stalls (scheduler / memory dependency):\\n\");\n",
        "  std::printf(\"   ncu --section \\\"Warp State Statistics\\\" --section \\\"Scheduler Statistics\\\" --section \\\"Memory Workload Analysis\\\" ./softmax_prof --B %d --D %d --reps 50 --warmup 10\\n\", B, D);\n",
        "\n",
        "  CUDA_CHECK(cudaFree(dx));\n",
        "  CUDA_CHECK(cudaFree(dy));\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537grFp0CIfV",
        "outputId": "5636f499-d576-4159-a388-723892a9a050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01msoftmax_profile_compare.cu(279)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"has_flag\"\u001b[0m was declared but never referenced\n",
            "  static int has_flag(int argc, char** argv, const char* name) {\n",
            "             ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 softmax_profile_compare.cu -o softmax_prof"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D Â∞è,B Â§ß(warp-per-row ÈÄöÂ∏∏Ëµ¢Ôºâ\n",
        "!./softmax_prof --B 65536 --D 128  --reps 300 --warmup 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsHQnJIJCk8O",
        "outputId": "1ded0f45-ecc8-4801-ada1-063720961eaf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B=65536 D=128 reps=300 warmup=50 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=256\n",
            "  multiwarp-per-row: warpsPerRow=8 (block.x=256)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   0.2871 ms  |  ~ 233.75 GB/s  |    0.034 ns/elem\n",
            "block-per-row          :   0.9043 ms  |  ~  74.21 GB/s  |    0.108 ns/elem\n",
            "multiwarp-per-row      :   0.7211 ms  |  ~  93.06 GB/s  |    0.086 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 65536 --D 128 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 65536 --D 128 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 65536 --D 128 --reps 50 --warmup 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D ‰∏≠Á≠âÔºàmulti-warp-per-row ÂæÄÂæÄÊõ¥ÂÆπÊòìÊàê‰∏∫ÊúÄ‰ºòÔºâ\n",
        "!./softmax_prof --B 16384 --D 512  --reps 200 --warmup 50 --mw_warps 4\n",
        "!./softmax_prof --B 16384 --D 512  --reps 200 --warmup 50 --mw_warps 8\n",
        "!./softmax_prof --B 16384 --D 512  --reps 200 --warmup 50 --mw_warps 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3K4f5bhCpNM",
        "outputId": "4017a45a-d88d-41ec-f56b-1eaa96dfa0a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B=16384 D=512 reps=200 warmup=50 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=256\n",
            "  multiwarp-per-row: warpsPerRow=4 (block.x=128)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   0.3332 ms  |  ~ 201.43 GB/s  |    0.040 ns/elem\n",
            "block-per-row          :   0.5817 ms  |  ~ 115.37 GB/s  |    0.069 ns/elem\n",
            "multiwarp-per-row      :   0.4076 ms  |  ~ 164.65 GB/s  |    0.049 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "B=16384 D=512 reps=200 warmup=50 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=256\n",
            "  multiwarp-per-row: warpsPerRow=8 (block.x=256)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   0.3288 ms  |  ~ 204.12 GB/s  |    0.039 ns/elem\n",
            "block-per-row          :   0.3533 ms  |  ~ 189.95 GB/s  |    0.042 ns/elem\n",
            "multiwarp-per-row      :   0.3503 ms  |  ~ 191.59 GB/s  |    0.042 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "B=16384 D=512 reps=200 warmup=50 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=256\n",
            "  multiwarp-per-row: warpsPerRow=16 (block.x=512)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   0.3279 ms  |  ~ 204.65 GB/s  |    0.039 ns/elem\n",
            "block-per-row          :   0.3503 ms  |  ~ 191.58 GB/s  |    0.042 ns/elem\n",
            "multiwarp-per-row      :   0.4480 ms  |  ~ 149.81 GB/s  |    0.053 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 16384 --D 512 --reps 50 --warmup 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D Â§ßÔºàblock-per-row Êõ¥Á®≥Ôºâ\n",
        "!./softmax_prof --B 4096 --D 4096 --reps 120 --warmup 20 --block_threads 256\n",
        "!./softmax_prof --B 4096 --D 4096 --reps 120 --warmup 20 --block_threads 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hETi-dHCuWj",
        "outputId": "0bc5f21d-1dce-4c86-f438-9c395bda4c47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B=4096 D=4096 reps=120 warmup=20 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=256\n",
            "  multiwarp-per-row: warpsPerRow=8 (block.x=256)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   1.2928 ms  |  ~ 103.82 GB/s  |    0.077 ns/elem\n",
            "block-per-row          :   1.0331 ms  |  ~ 129.91 GB/s  |    0.062 ns/elem\n",
            "multiwarp-per-row      :   0.7535 ms  |  ~ 178.12 GB/s  |    0.045 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n",
            "B=4096 D=4096 reps=120 warmup=20 check=0\n",
            "Configs:\n",
            "  warp-per-row: warpsPerBlock=8 (block.x=256)\n",
            "  block-per-row: blockThreads=512\n",
            "  multiwarp-per-row: warpsPerRow=8 (block.x=256)\n",
            "\n",
            "=== Average time per launch ===\n",
            "warp-per-row           :   1.2903 ms  |  ~ 104.02 GB/s  |    0.077 ns/elem\n",
            "block-per-row          :   0.9278 ms  |  ~ 144.66 GB/s  |    0.055 ns/elem\n",
            "multiwarp-per-row      :   0.7547 ms  |  ~ 177.84 GB/s  |    0.045 ns/elem\n",
            "\n",
            "=== Nsight Compute tips ===\n",
            "1) Run full profile for all kernels:\n",
            "   ncu --set full --kernel-name regex:softmax_kernel_.* ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n",
            "2) Focus on memory & math throughput:\n",
            "   ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,dram__throughput.avg.pct_of_peak_sustained_elapsed,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n",
            "3) Look at stalls (scheduler / memory dependency):\n",
            "   ncu --section \"Warp State Statistics\" --section \"Scheduler Statistics\" --section \"Memory Workload Analysis\" ./softmax_prof --B 4096 --D 4096 --reps 50 --warmup 10\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}