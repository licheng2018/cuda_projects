{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation (Day by Day): PyTorch LayerNorm C++/CUDA Extension\n",
        "\n",
        "This week focuses on building a **production-style PyTorch LayerNorm (LN) extension**, starting from a C++ forward wrapper and ending with a **complete forward + backward CUDA implementation**, benchmarked against PyTorchâ€™s official kernel.\n",
        "\n",
        "The goal is to understand **how real PyTorch operators are written, registered, validated, and optimized**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 2 â€” Register LN Forward (C++ Wrapper) & Python Test\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Expose a **custom LN forward implementation** to Python via a PyTorch C++ extension and validate the **Python â†’ C++ â†’ CUDA** execution path.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Write a C++ forward wrapper using ATen:\n",
        "  - Accept `at::Tensor` inputs\n",
        "  - Validate device, dtype, and layout\n",
        "  - Allocate output tensors\n",
        "  - Dispatch to a CUDA kernel\n",
        "- Register the forward function using `PYBIND11_MODULE`\n",
        "- Call the operator from Python and verify:\n",
        "  - Correct execution\n",
        "  - Correct output shape and dtype\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- PyTorch C++ extension registration\n",
        "- ATen tensor checks and allocation\n",
        "- Python â†” C++ ABI boundary\n",
        "- Kernel launch from C++\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Callable `ln_forward()` from Python\n",
        "- Successful Python test script\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 3 â€” Register LN Backward & Verify Gradient Correctness\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Extend the LN operator to support **backward propagation** and ensure it integrates correctly with PyTorchâ€™s autograd system.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement and register LN backward:\n",
        "  - Compute gradients for `dx`, `dgamma`, and `dbeta`\n",
        "  - Use CUDA kernels for gradient computation\n",
        "- Bind backward logic via:\n",
        "  - Custom `torch::autograd::Function` **or**\n",
        "  - Manual backward registration (educational setup)\n",
        "- Verify gradient correctness:\n",
        "  - Compare against PyTorch autograd results\n",
        "  - Use numerical tolerances\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Autograd mechanics\n",
        "- Forward/backward dependency management\n",
        "- Gradient reduction patterns\n",
        "- Numerical stability in backward pass\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working backward kernel\n",
        "- Gradient correctness test (PASS)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 4 â€” Compile, Debug, and Fix Edge Cases\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Harden the extension so it behaves correctly across **realistic and corner-case inputs**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Fix compilation issues:\n",
        "  - Template errors\n",
        "  - Device/dtype mismatches\n",
        "- Debug runtime errors:\n",
        "  - Illegal memory access\n",
        "  - Incorrect indexing\n",
        "- Handle edge cases:\n",
        "  - Non-multiple-of-warp feature sizes\n",
        "  - Small batch sizes\n",
        "  - Large/small variance values\n",
        "- Add assertions and sanity checks\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- CUDA debugging strategies\n",
        "- Shape- and stride-related pitfalls\n",
        "- Numerical edge cases in normalization\n",
        "- Defensive programming in C++ extensions\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Stable, crash-free extension\n",
        "- Clean compilation with `-O3`\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 5 â€” Benchmark: Custom LN vs PyTorch Official Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Quantitatively compare your LN implementation against **PyTorchâ€™s official LayerNorm**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Benchmark forward and backward:\n",
        "  - Your custom LN extension\n",
        "  - `torch.nn.LayerNorm`\n",
        "- Measure:\n",
        "  - Kernel execution time\n",
        "  - End-to-end forward/backward time\n",
        "- Use consistent input sizes and warm-up\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Fair benchmarking methodology\n",
        "- Kernel launch overhead\n",
        "- Memory-bound vs compute-bound behavior\n",
        "- Why official kernels are highly optimized\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Benchmark table or logs\n",
        "- Short performance analysis\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 6 â€” Implement Fused GELU + Bias CUDA Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Apply the same extension workflow to a **fused operator**, reinforcing kernel fusion concepts common in ML systems.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement a CUDA kernel that fuses:\n",
        "  - Bias addition\n",
        "  - GELU activation\n",
        "- Register the fused kernel as a PyTorch extension\n",
        "- Test correctness against PyTorch reference\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Kernel fusion benefits\n",
        "- Reducing memory traffic\n",
        "- Elementwise kernel optimization\n",
        "- Operator fusion in Transformers\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working fused GELU + Bias kernel\n",
        "- Python correctness test\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 7 â€” Weekly Project: Full PyTorch LN Extension\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Deliver a **complete, reusable PyTorch LN extension** suitable for learning portfolios or ML systems interviews.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Integrate:\n",
        "  - LN forward\n",
        "  - LN backward\n",
        "- Clean up codebase:\n",
        "  - Clear APIs\n",
        "  - Consistent naming\n",
        "- Add:\n",
        "  - Python test scripts\n",
        "  - Benchmark script\n",
        "  - README-style documentation\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- End-to-end operator development\n",
        "- Code organization for extensions\n",
        "- Production-style validation and benchmarking\n",
        "\n",
        "### ðŸ“¦ Final Deliverable\n",
        "- A full **PyTorch LayerNorm C++/CUDA extension**\n",
        "- Runnable from Python with forward + backward\n",
        "- Benchmarked and validated\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Weekly Takeaway\n",
        "> **This week trains you to think like an ML systems engineer: designing, registering, debugging, validating, and benchmarking a real PyTorch operatorâ€”not just writing a CUDA kernel.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "3d3b09e3-100f-4842-9e32-14b2dae86ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Jan 25 12:24:44 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyuqf4e1iZBl",
        "outputId": "deec54af-a115-4337-c887-32d79d205764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [1 InRelease 129 kB/129 kB 100%] [Connected to cloud.r-project.org (65.9.86.\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 38.8 kB/128 kB 30%] [Waiting for headers] [Connecting to r2u.st\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [3 InRelease 47.5 kB/128 kB 37%] [4 InRelease 3,632 B/3,632 B 100%] [Connect\r0% [3 InRelease 47.5 kB/128 kB 37%] [Connecting to r2u.stat.illinois.edu] [Wait\r                                                                               \rGet:5 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,640 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [61.5 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,293 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:17 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,309 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [86.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,604 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,486 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,971 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:25 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:26 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [45.0 kB]\n",
            "Get:27 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:28 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,882 kB]\n",
            "Get:29 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,650 kB]\n",
            "Fetched 39.0 MB in 4s (10.4 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4-config-common cuda-tools-12-4\n",
            "  cuda-visual-tools-12-4 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gds-tools-12-4 gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libcublas-12-4 libcublas-dev-12-4 libcufft-12-4\n",
            "  libcufft-dev-12-4 libcufile-12-4 libcufile-dev-12-4 libcurand-12-4\n",
            "  libcurand-dev-12-4 libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4\n",
            "  libcusparse-dev-12-4 libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4\n",
            "  libnvfatbin-dev-12-4 libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4\n",
            "  libnvjpeg-dev-12-4 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxcomposite1 libxkbcommon-x11-0 libxtst6 libxxf86dga1\n",
            "  nsight-compute-2024.1.1 nsight-systems-2023.4.4 openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4 cuda-toolkit-12-4-config-common\n",
            "  cuda-tools-12-4 cuda-visual-tools-12-4 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gds-tools-12-4\n",
            "  gsettings-desktop-schemas libatk-bridge2.0-0 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libatk1.0-0 libatk1.0-data libatspi2.0-0\n",
            "  libcublas-12-4 libcublas-dev-12-4 libcufft-12-4 libcufft-dev-12-4\n",
            "  libcufile-12-4 libcufile-dev-12-4 libcurand-12-4 libcurand-dev-12-4\n",
            "  libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4 libcusparse-dev-12-4\n",
            "  libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4 libnvfatbin-dev-12-4\n",
            "  libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4 libnvjpeg-dev-12-4\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 libxxf86dga1 nsight-compute-2024.1.1\n",
            "  nsight-systems-2023.4.4 openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "0 upgraded, 88 newly installed, 0 to remove and 103 not upgraded.\n",
            "Need to get 3,068 MB of archives.\n",
            "After this operation, 6,782 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.29+7-1ubuntu1~22.04 [42.6 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.29+7-1ubuntu1~22.04 [214 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:30 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-12-4 12.4.127-1 [1,200 kB]\n",
            "Get:31 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-12-4 12.4.127-1 [16.8 MB]\n",
            "Get:32 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-12-4 12.4.127-1 [3,830 kB]\n",
            "Get:33 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-12-4 12.4.127-1 [49.9 MB]\n",
            "Get:34 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-12-4 12.4.127-1 [225 kB]\n",
            "Get:35 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-12-4 12.4.127-1 [4,920 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-12-4 12.4.127-1 [2,431 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-12-4 12.4.127-1 [51.5 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-12-4 12.4.127-1 [9,148 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-12-4 12.4.1-1 [2,538 B]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-12-4 12.4.127-1 [191 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4-config-common 12.4.127-1 [16.4 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-12-4 12.4.127-1 [165 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-12-4 12.4.127-1 [28.6 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-12-4 12.4.127-1 [1,000 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvm-12-4 12.4.131-1 [19.5 MB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-crt-12-4 12.4.131-1 [78.0 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-12-4 12.4.131-1 [32.0 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-12-4 12.4.127-1 [58.8 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-12-4 12.4.1-1 [2,510 B]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-12-4 12.4.127-1 [50.0 kB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-12-4 12.4.127-1 [17.6 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-12-4 12.4.127-1 [24.0 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-12-4 12.4.5.8-1 [231 MB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-12-4 11.2.1.3-1 [171 MB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-12-4 1.9.1.3-1 [850 kB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-12-4 10.3.5.147-1 [41.4 MB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-12-4 11.6.1.9-1 [78.9 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-12-4 12.3.1.170-1 [115 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-12-4 12.2.5.30-1 [95.5 MB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-12-4 12.4.127-1 [15.5 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-12-4 12.4.127-1 [721 kB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-12-4 12.3.1.117-1 [2,327 kB]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-12-4 12.4.1-1 [2,606 B]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-12-4 12.4.127-1 [18.7 kB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-12-4 12.4.127-1 [16.9 MB]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-dev-12-4 12.4.127-1 [87.0 kB]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-12-4 12.4.5.8-1 [249 MB]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-12-4 11.2.1.3-1 [342 MB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-12-4 1.9.1.3-1 [2,435 kB]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-12-4 10.3.5.147-1 [41.6 MB]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-12-4 11.6.1.9-1 [51.4 MB]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-12-4 12.3.1.170-1 [116 MB]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-12-4 12.2.5.30-1 [92.0 MB]\n",
            "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-dev-12-4 12.4.127-1 [13.9 MB]\n",
            "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-dev-12-4 12.4.127-1 [591 kB]\n",
            "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-12-4 12.3.1.117-1 [2,025 kB]\n",
            "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-12-4 12.4.1-1 [2,646 B]\n",
            "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-12-4 12.4.127-1 [119 MB]\n",
            "Get:79 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2024.1.1 2024.1.1.4-1 [594 MB]\n",
            "Get:80 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-12-4 12.4.1-1 [4,060 B]\n",
            "Get:81 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.4.4 2023.4.4.54-234433681190v0 [316 MB]\n",
            "Get:82 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-12-4 12.4.1-1 [3,348 B]\n",
            "Get:83 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-12-4 12.4.127-1 [145 kB]\n",
            "Get:84 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-12-4 12.4.127-1 [115 MB]\n",
            "Get:85 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-12-4 12.4.1-1 [2,942 B]\n",
            "Get:86 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-12-4 1.9.1.3-1 [39.0 MB]\n",
            "Get:87 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-12-4 12.4.1-1 [2,462 B]\n",
            "Get:88 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4 12.4.1-1 [3,336 B]\n",
            "Fetched 3,068 MB in 1min 29s (34.5 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package cuda-cccl-12-4.\n",
            "Preparing to unpack .../05-cuda-cccl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-12-4.\n",
            "Preparing to unpack .../06-cuda-cupti-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-12-4.\n",
            "Preparing to unpack .../07-cuda-cupti-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-12-4.\n",
            "Preparing to unpack .../08-cuda-nvdisasm-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-12-4.\n",
            "Preparing to unpack .../09-cuda-cuobjdump-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-gdb-12-4.\n",
            "Preparing to unpack .../10-cuda-gdb-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-12-4.\n",
            "Preparing to unpack .../11-cuda-nvprof-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-12-4.\n",
            "Preparing to unpack .../12-cuda-nvtx-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-12-4.\n",
            "Preparing to unpack .../13-cuda-sanitizer-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-12-4.\n",
            "Preparing to unpack .../14-cuda-command-line-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-12-4.\n",
            "Preparing to unpack .../15-cuda-cuxxfilt-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4-config-common.\n",
            "Preparing to unpack .../16-cuda-toolkit-12-4-config-common_12.4.127-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-12-4.\n",
            "Preparing to unpack .../17-cuda-cudart-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-12-4.\n",
            "Preparing to unpack .../18-cuda-driver-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-12-4.\n",
            "Preparing to unpack .../19-cuda-cudart-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvm-12-4.\n",
            "Preparing to unpack .../20-cuda-nvvm-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-crt-12-4.\n",
            "Preparing to unpack .../21-cuda-crt-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-crt-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-12-4.\n",
            "Preparing to unpack .../22-cuda-nvcc-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-12-4.\n",
            "Preparing to unpack .../23-cuda-nvprune-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-compiler-12-4.\n",
            "Preparing to unpack .../24-cuda-compiler-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-documentation-12-4.\n",
            "Preparing to unpack .../25-cuda-documentation-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-12-4.\n",
            "Preparing to unpack .../26-cuda-nvrtc-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-12-4.\n",
            "Preparing to unpack .../27-cuda-opencl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-12-4.\n",
            "Preparing to unpack .../28-libcublas-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-12-4.\n",
            "Preparing to unpack .../29-libcufft-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-12-4.\n",
            "Preparing to unpack .../30-libcufile-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-12-4.\n",
            "Preparing to unpack .../31-libcurand-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-12-4.\n",
            "Preparing to unpack .../32-libcusolver-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-12-4.\n",
            "Preparing to unpack .../33-libcusparse-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-12-4.\n",
            "Preparing to unpack .../34-libnpp-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-12-4.\n",
            "Preparing to unpack .../35-libnvjitlink-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-12-4.\n",
            "Preparing to unpack .../36-libnvfatbin-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-12-4.\n",
            "Preparing to unpack .../37-libnvjpeg-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-12-4.\n",
            "Preparing to unpack .../38-cuda-libraries-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-12-4.\n",
            "Preparing to unpack .../39-cuda-profiler-api-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-12-4.\n",
            "Preparing to unpack .../40-cuda-nvrtc-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-dev-12-4.\n",
            "Preparing to unpack .../41-cuda-opencl-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-dev-12-4.\n",
            "Preparing to unpack .../42-libcublas-dev-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-dev-12-4.\n",
            "Preparing to unpack .../43-libcufft-dev-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-dev-12-4.\n",
            "Preparing to unpack .../44-libcufile-dev-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-dev-12-4.\n",
            "Preparing to unpack .../45-libcurand-dev-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-12-4.\n",
            "Preparing to unpack .../46-libcusolver-dev-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-12-4.\n",
            "Preparing to unpack .../47-libcusparse-dev-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-dev-12-4.\n",
            "Preparing to unpack .../48-libnpp-dev-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-dev-12-4.\n",
            "Preparing to unpack .../49-libnvjitlink-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-dev-12-4.\n",
            "Preparing to unpack .../50-libnvfatbin-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-12-4.\n",
            "Preparing to unpack .../51-libnvjpeg-dev-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-12-4.\n",
            "Preparing to unpack .../52-cuda-libraries-dev-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../53-openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../54-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../55-openjdk-11-jre_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../56-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-12-4.\n",
            "Preparing to unpack .../57-cuda-nsight-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package nsight-compute-2024.1.1.\n",
            "Preparing to unpack .../58-nsight-compute-2024.1.1_2024.1.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-12-4.\n",
            "Preparing to unpack .../59-cuda-nsight-compute-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../60-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../61-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../62-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../63-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../64-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../65-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../66-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../67-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../68-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../69-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../70-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package nsight-systems-2023.4.4.\n",
            "Preparing to unpack .../71-nsight-systems-2023.4.4_2023.4.4.54-234433681190v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-12-4.\n",
            "Preparing to unpack .../72-cuda-nsight-systems-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-12-4.\n",
            "Preparing to unpack .../73-cuda-nvml-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-12-4.\n",
            "Preparing to unpack .../74-cuda-nvvp-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-12-4.\n",
            "Preparing to unpack .../75-cuda-visual-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package gds-tools-12-4.\n",
            "Preparing to unpack .../76-gds-tools-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package cuda-tools-12-4.\n",
            "Preparing to unpack .../77-cuda-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4.\n",
            "Preparing to unpack .../78-cuda-toolkit-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../79-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../80-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../81-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../82-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../83-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../84-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../85-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../86-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../87-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Setting up gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Setting up cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Setting alternatives\n",
            "Setting up libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-12-4 (10.3.5.147-1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libnpp-12-4 (12.2.5.30-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcufft-12-4 (11.2.1.3-1) ...\n",
            "Setting up libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-12-4 (12.4.5.8-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Setting up libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Setting up cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Setting up libcufile-12-4 (1.9.1.3-1) ...\n",
            "Setting alternatives\n",
            "Setting up nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Setting up cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Setting up libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Setting up cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Setting up cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Setting up libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Setting up libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Setting up cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-crt-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Setting up libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Setting up cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmMYxeMcEayh",
        "outputId": "30fd8ab4-eb4a-4752-bd96-e0a7086439e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTirfEh46lkF"
      },
      "source": [
        "## ðŸ§± Step 1 â€” Register LN Forward (C++ wrapper) + Python call test (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhZ_FIDt6lkF",
        "outputId": "8664c8b8-9a17-484b-f6ab-5543e4e609f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Day2 extension loaded: <module 'ln_ext_forward_v2' from '/root/.cache/torch_extensions/py312_cu126/ln_ext_forward/ln_ext_forward_v2.so'>\n",
            "y: torch.Size([16, 128]) torch.float32 cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Day 1 (ONE CELL): forward-only LN extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Install ninja if not already present\n",
        "!pip install ninja\n",
        "\n",
        "# -----------------------------\n",
        "# Write files\n",
        "# -----------------------------\n",
        "os.makedirs(\"ln_ext\", exist_ok=True)\n",
        "\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "torch::Tensor ln_forward(torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "#define CHECK_2D(x) TORCH_CHECK((x).dim() == 2, #x \" must be 2D [B, D]\")\n",
        "#define CHECK_1D(x) TORCH_CHECK((x).dim() == 1, #x \" must be 1D [D]\")\n",
        "\n",
        "torch::Tensor ln_forward(\n",
        "torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps) {\n",
        "    // TODO:\n",
        "    // - validate: x CUDA/contiguous/float32/2D\n",
        "    // - validate: gamma,beta CUDA/contiguous/float32/1D and gamma.size(0)==D\n",
        "    // - allocate y [B,D], mean [B], inv_std [B]\n",
        "    // - call ln_forward_cuda_launcher(...)\n",
        "    // - return y (or return a tuple if you prefer, but keep API consistent)\n",
        "\n",
        "    // Placeholder (compilable but not correct):\n",
        "    // This section doesn't consider gamma and beta size and all data is on GPU\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x); CHECK_2D(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma); CHECK_1D(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);  CHECK_1D(beta);\n",
        "\n",
        "\n",
        "\n",
        "    TORCH_CHECK(x.device() == gamma.device(), \"x and gamma must be on same device\");\n",
        "    TORCH_CHECK(x.device() == beta.device(),  \"x and beta must be on same device\");\n",
        "    TORCH_CHECK(x.size(0) > 0 && x.size(1) > 0, \"x must have non-zero B and D\");\n",
        "    TORCH_CHECK(eps > 0.0, \"eps must be > 0\");\n",
        "\n",
        "\n",
        "\n",
        "    auto B = x.size(0);\n",
        "    auto D = x.size(1);\n",
        "    TORCH_CHECK(gamma.size(0) == D, \"gamma must have shape [D]\");\n",
        "    TORCH_CHECK(beta.size(0)  == D, \"beta must have shape [D]\");\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: replace with real launcher call\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "// TODO: warp reduce helper\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: implement with __shfl_down_sync\n",
        "    for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);}\n",
        "    return v;\n",
        "}\n",
        "\n",
        "// TODO: forward kernel (warp reduce)\n",
        "// x: [B,D], gamma/beta: [D], y:[B,D], mean/inv_std:[B]\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO:\n",
        "    // - map row(s) to warps/blocks\n",
        "    // - compute mean via warp reduction\n",
        "    // - compute variance via warp reduction\n",
        "    // - write mean/inv_std\n",
        "    // - normalize + affine and write y\n",
        "    int row = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    int lane = tid&32;\n",
        "    int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "    int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "    const float* xrow = x + (size_t)row * D;\n",
        "    float* yrow = y + (size_t)row * D;\n",
        "\n",
        "    // step 1: each thread acculates partial sum and sumq\n",
        "    float sumx = 0.0f;\n",
        "    float sumq = 0.0f;\n",
        "    for (int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        sumx += xi;\n",
        "        sumq += xi * xi;\n",
        "    }\n",
        "\n",
        "    //step2: reduce within each warp\n",
        "    sumx = warpReduceSum(sumx);\n",
        "    sumq = warpReduceSum(sumq);\n",
        "\n",
        "    //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "    __shared__ float warp_sums[32];\n",
        "    __shared__ float warp_sumsq[32];\n",
        "\n",
        "    if (lane == 0){\n",
        "        warp_sums[warp] = sumx;\n",
        "        warp_sumsq[warp] = sumq;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    //get block sum\n",
        "    float block_sum = 0.f, block_sumsq = 0.f;\n",
        "    if(warp == 0){\n",
        "      block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "      block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "      block_sum   = warpReduceSum(block_sum);\n",
        "      block_sumsq = warpReduceSum(block_sumsq);\n",
        "    }\n",
        "\n",
        "    //step4: broadcast mean and inv_std to all threads\n",
        "    __shared__ float sh_mu, sh_inv;\n",
        "    if (tid == 0){\n",
        "        float mu = block_sum / float(D);\n",
        "        float var = block_sumsq / (float)D - mu * mu;\n",
        "        float inv = rsqrtf(var + eps);\n",
        "        sh_mu = mu;\n",
        "        sh_inv = inv;\n",
        "        mean[row] = mu;\n",
        "        inv_std[row] = inv;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    //step5: get result\n",
        "    float mu = sh_mu, inv = sh_inv;\n",
        "    for(int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        float xhat = (xi - mu) * inv;\n",
        "        yrow[i] = xhat * gamma[i] + beta[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    // TODO: choose launch config\n",
        "    // dim3 block(0,0,1); // TODO\n",
        "    // dim3 grid(0,0,1);  // TODO\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "    int f_threads = 256;\n",
        "    if (D <= 128) f_threads = 128;\n",
        "    f_threads = round_up_warp(f_threads);\n",
        "\n",
        "    dim3 f_block(f_threads, 1, 1);\n",
        "    dim3 f_grid(B, 1, 1);\n",
        "\n",
        "\n",
        "    // Placeholder launch (won't run correctly until you set block/grid + kernel body)\n",
        "    ln_forward_kernel<<<f_grid, f_block>>>(\n",
        "        (const float*)x.data_ptr<float>(),\n",
        "        (const float*)gamma.data_ptr<float>(),\n",
        "        (const float*)beta.data_ptr<float>(),\n",
        "        (float*)y.data_ptr<float>(),\n",
        "        (float*)mean.data_ptr<float>(),\n",
        "        (float*)inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build extension\n",
        "# -----------------------------\n",
        "ext = load(\n",
        "    name=\"ln_ext_forward\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Day2 extension loaded:\", ext)\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: Python call test (disabled until TODOs are filled)\n",
        "# -----------------------------\n",
        "RUN_TEST = True\n",
        "if RUN_TEST:\n",
        "    B, D = 16, 128\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "    gamma = torch.ones(D, device=\"cuda\", dtype=torch.float32)\n",
        "    beta  = torch.zeros(D, device=\"cuda\", dtype=torch.float32)\n",
        "    y = ext.forward(x, gamma, beta, 1e-5)\n",
        "    print(\"y:\", y.shape, y.dtype, y.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcEewiv6lkF"
      },
      "source": [
        "## âœ… Step 2 â€” Register LN Backward + gradient correctness check (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ods2P8jU6lkF"
      },
      "outputs": [],
      "source": [
        "# Day 3 (ONE CELL): add backward + autograd wrapper skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Overwrite C++/CUDA files for backward-enabled extension\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// Forward returns (y, mean, inv_std) for backward reuse\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps);\n",
        "\n",
        "// Backward returns (dx, dgamma, dbeta)\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x,\n",
        "                               torch::Tensor gamma,\n",
        "                               torch::Tensor mean,\n",
        "                               torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx,\n",
        "                               torch::Tensor dgamma,\n",
        "                               torch::Tensor dbeta);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "static void check_forward_args(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "}\n",
        "\n",
        "static void check_backward_args(torch::Tensor x, torch::Tensor gamma,\n",
        "                                torch::Tensor mean, torch::Tensor inv_std,\n",
        "                                torch::Tensor dout) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps) {\n",
        "    check_forward_args(x, gamma, beta);\n",
        "    auto B = x.size(0);\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: real CUDA forward\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return {y, mean, inv_std};\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout) {\n",
        "    check_backward_args(x, gamma, mean, inv_std, dout);\n",
        "\n",
        "    auto dx = torch::empty_like(x);\n",
        "    auto dgamma = torch::zeros_like(gamma);\n",
        "    auto dbeta  = torch::zeros_like(gamma);\n",
        "\n",
        "    // TODO: real CUDA backward\n",
        "    ln_backward_cuda_launcher(x, gamma, mean, inv_std, dout, dx, dgamma, dbeta);\n",
        "\n",
        "    return {dx, dgamma, dbeta};\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "    m.def(\"backward\", &ln_backward, \"LayerNorm backward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: __shfl_down_sync\n",
        "    return v;\n",
        "}\n",
        "\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO\n",
        "}\n",
        "\n",
        "__global__ void ln_backward_kernel(const float* __restrict__ x,\n",
        "                                   const float* __restrict__ gamma,\n",
        "                                   const float* __restrict__ mean,\n",
        "                                   const float* __restrict__ inv_std,\n",
        "                                   const float* __restrict__ dout,\n",
        "                                   float* __restrict__ dx,\n",
        "                                   float* __restrict__ dgamma,\n",
        "                                   float* __restrict__ dbeta,\n",
        "                                   int B, int D) {\n",
        "    // TODO:\n",
        "    // - compute dx\n",
        "    // - reduce dgamma/dbeta (atomics or 2-pass strategy)\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta,\n",
        "                              torch::Tensor y, torch::Tensor mean, torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    dim3 block(0,0,1); // TODO\n",
        "    dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "    ln_forward_kernel<<<grid, block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(),\n",
        "        y.data_ptr<float>(), mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x, torch::Tensor gamma,\n",
        "                               torch::Tensor mean, torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx, torch::Tensor dgamma, torch::Tensor dbeta) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "    CHECK_CUDA(dx); CHECK_CONTIG(dx); CHECK_F32(dx);\n",
        "    CHECK_CUDA(dgamma); CHECK_CONTIG(dgamma); CHECK_F32(dgamma);\n",
        "    CHECK_CUDA(dbeta); CHECK_CONTIG(dbeta); CHECK_F32(dbeta);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    dim3 block(0,0,1); // TODO\n",
        "    dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "    ln_backward_kernel<<<grid, block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(),\n",
        "        mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        dout.data_ptr<float>(),\n",
        "        dx.data_ptr<float>(), dgamma.data_ptr<float>(), dbeta.data_ptr<float>(),\n",
        "        B, D\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "ext = load(\n",
        "    name=\"ln_ext_fwd_bwd\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Day3 extension loaded:\", ext)\n",
        "\n",
        "# Optional: gradient check harness (disabled until TODOs are implemented)\n",
        "RUN_GRAD_TEST = False\n",
        "if RUN_GRAD_TEST:\n",
        "    B, D = 8, 256\n",
        "    eps = 1e-5\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # TODO: compare to torch.nn.functional.layer_norm gradients\n",
        "    # - call ext.forward -> (y, mean, inv_std)\n",
        "    # - build dout\n",
        "    # - call ext.backward -> (dx, dgamma, dbeta)\n",
        "    # - compare to autograd reference\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W481Iav6lkG"
      },
      "source": [
        "## âœ… Day 4 â€” Compile / debug / edge cases (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRLEynmF6lkG"
      },
      "outputs": [],
      "source": [
        "# Day 4 (ONE CELL): edge case test scaffolding + debug aids (NO SOLUTION)\n",
        "\n",
        "import torch, math\n",
        "\n",
        "# Edge cases to test (you can expand)\n",
        "CASES = [\n",
        "    (1, 7),       # tiny D\n",
        "    (2, 33),      # not multiple of warp\n",
        "    (4, 128),\n",
        "    (16, 1024),\n",
        "    (3, 4096),    # large D\n",
        "]\n",
        "\n",
        "# Toggle when your kernels are implemented\n",
        "RUN_EDGE_TESTS = False\n",
        "\n",
        "def run_edge_suite(ext):\n",
        "    for (B, D) in CASES:\n",
        "        x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "        gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "        beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "        # TODO: call ext.forward and validate shape/dtype/device\n",
        "        # y, mean, inv = ext.forward(x, gamma, beta, 1e-5)\n",
        "        # assert y.shape == x.shape\n",
        "        # assert mean.shape == (B,)\n",
        "        # assert inv.shape == (B,)\n",
        "\n",
        "        # TODO: check numerical sanity (no NaN/Inf)\n",
        "        # assert torch.isfinite(y).all()\n",
        "\n",
        "        # TODO: backward sanity\n",
        "        # dout = torch.randn_like(x)\n",
        "        # dx, dgamma, dbeta = ext.backward(x, gamma, mean, inv, dout)\n",
        "\n",
        "        print(f\"[EdgeCase] B={B} D={D} -> TODO checks\")\n",
        "\n",
        "# If you already loaded Day3 extension as `ext`, you can run:\n",
        "if RUN_EDGE_TESTS:\n",
        "    run_edge_suite(ext)\n",
        "else:\n",
        "    print(\"Day4: Edge suite is ready. Set RUN_EDGE_TESTS=True after implementing kernels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34tExjB96lkG"
      },
      "source": [
        "## âœ… Day 5 â€” Fused GELU + Bias CUDA kernel (extension skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7HHJO6W6lkG"
      },
      "outputs": [],
      "source": [
        "# Day 6 (ONE CELL): fused Bias+GELU extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "os.makedirs(\"fused_gelu\", exist_ok=True)\n",
        "\n",
        "open(\"fused_gelu/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F16F32(x) TORCH_CHECK((x).scalar_type()==at::kHalf || (x).scalar_type()==at::kFloat, #x \" must be fp16 or fp32\")\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y);\n",
        "\n",
        "torch::Tensor fused_gelu_bias(torch::Tensor x, torch::Tensor bias) {\n",
        "    // TODO:\n",
        "    // - checks (CUDA/contig/dtype/shape)\n",
        "    // - allocate y\n",
        "    // - call launcher\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F16F32(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias); CHECK_F16F32(bias);\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    fused_gelu_bias_cuda_launcher(x, bias, y);\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &fused_gelu_bias, \"Fused Bias+GELU forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"fused_gelu/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "\n",
        "// TODO: implement GELU approximation or exact (no solution here)\n",
        "// kernel: y = GELU(x + bias)\n",
        "\n",
        "__global__ void fused_bias_gelu_kernel(/* TODO args */) {\n",
        "    // TODO\n",
        "}\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias);\n",
        "    CHECK_CUDA(y); CHECK_CONTIG(y);\n",
        "\n",
        "    // TODO: grid/block\n",
        "    dim3 block(0,0,1);\n",
        "    dim3 grid(0,0,1);\n",
        "\n",
        "    // TODO: dispatch by dtype (fp16/fp32)\n",
        "    // fused_bias_gelu_kernel<<<grid, block>>>(...);\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "fused = load(\n",
        "    name=\"fused_gelu_bias_ext\",\n",
        "    sources=[\"fused_gelu/ext.cpp\", \"fused_gelu/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Day6 extension loaded:\", fused)\n",
        "\n",
        "RUN_TEST = False\n",
        "if RUN_TEST:\n",
        "    # TODO: compare vs torch.nn.functional.gelu(x + bias)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FO5iCX6lkH"
      },
      "source": [
        "## âœ… Day 7 â€” Weekly project packaging (full LN extension) + scripts (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJWwSsM36lkH"
      },
      "outputs": [],
      "source": [
        "# Day 7 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Day7 packaging skeleton created under ./project_ln/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiY-3BfN6lkH"
      },
      "outputs": [],
      "source": [
        "# Day 7 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Day7 packaging skeleton created under ./project_ln/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}