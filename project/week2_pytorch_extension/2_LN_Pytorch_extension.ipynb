{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation (Day by Day): PyTorch LayerNorm C++/CUDA Extension\n",
        "\n",
        "This week focuses on building a **production-style PyTorch LayerNorm (LN) extension**, starting from a C++ forward wrapper and ending with a **complete forward + backward CUDA implementation**, benchmarked against PyTorchâ€™s official kernel.\n",
        "\n",
        "The goal is to understand **how real PyTorch operators are written, registered, validated, and optimized**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 2 â€” Register LN Forward (C++ Wrapper) & Python Test\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Expose a **custom LN forward implementation** to Python via a PyTorch C++ extension and validate the **Python â†’ C++ â†’ CUDA** execution path.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Write a C++ forward wrapper using ATen:\n",
        "  - Accept `at::Tensor` inputs\n",
        "  - Validate device, dtype, and layout\n",
        "  - Allocate output tensors\n",
        "  - Dispatch to a CUDA kernel\n",
        "- Register the forward function using `PYBIND11_MODULE`\n",
        "- Call the operator from Python and verify:\n",
        "  - Correct execution\n",
        "  - Correct output shape and dtype\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- PyTorch C++ extension registration\n",
        "- ATen tensor checks and allocation\n",
        "- Python â†” C++ ABI boundary\n",
        "- Kernel launch from C++\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Callable `ln_forward()` from Python\n",
        "- Successful Python test script\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 3 â€” Register LN Backward & Verify Gradient Correctness\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Extend the LN operator to support **backward propagation** and ensure it integrates correctly with PyTorchâ€™s autograd system.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement and register LN backward:\n",
        "  - Compute gradients for `dx`, `dgamma`, and `dbeta`\n",
        "  - Use CUDA kernels for gradient computation\n",
        "- Bind backward logic via:\n",
        "  - Custom `torch::autograd::Function` **or**\n",
        "  - Manual backward registration (educational setup)\n",
        "- Verify gradient correctness:\n",
        "  - Compare against PyTorch autograd results\n",
        "  - Use numerical tolerances\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Autograd mechanics\n",
        "- Forward/backward dependency management\n",
        "- Gradient reduction patterns\n",
        "- Numerical stability in backward pass\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working backward kernel\n",
        "- Gradient correctness test (PASS)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 4 â€” Compile, Debug, and Fix Edge Cases\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Harden the extension so it behaves correctly across **realistic and corner-case inputs**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Fix compilation issues:\n",
        "  - Template errors\n",
        "  - Device/dtype mismatches\n",
        "- Debug runtime errors:\n",
        "  - Illegal memory access\n",
        "  - Incorrect indexing\n",
        "- Handle edge cases:\n",
        "  - Non-multiple-of-warp feature sizes\n",
        "  - Small batch sizes\n",
        "  - Large/small variance values\n",
        "- Add assertions and sanity checks\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- CUDA debugging strategies\n",
        "- Shape- and stride-related pitfalls\n",
        "- Numerical edge cases in normalization\n",
        "- Defensive programming in C++ extensions\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Stable, crash-free extension\n",
        "- Clean compilation with `-O3`\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 5 â€” Benchmark: Custom LN vs PyTorch Official Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Quantitatively compare your LN implementation against **PyTorchâ€™s official LayerNorm**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Benchmark forward and backward:\n",
        "  - Your custom LN extension\n",
        "  - `torch.nn.LayerNorm`\n",
        "- Measure:\n",
        "  - Kernel execution time\n",
        "  - End-to-end forward/backward time\n",
        "- Use consistent input sizes and warm-up\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Fair benchmarking methodology\n",
        "- Kernel launch overhead\n",
        "- Memory-bound vs compute-bound behavior\n",
        "- Why official kernels are highly optimized\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Benchmark table or logs\n",
        "- Short performance analysis\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 6 â€” Implement Fused GELU + Bias CUDA Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Apply the same extension workflow to a **fused operator**, reinforcing kernel fusion concepts common in ML systems.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement a CUDA kernel that fuses:\n",
        "  - Bias addition\n",
        "  - GELU activation\n",
        "- Register the fused kernel as a PyTorch extension\n",
        "- Test correctness against PyTorch reference\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Kernel fusion benefits\n",
        "- Reducing memory traffic\n",
        "- Elementwise kernel optimization\n",
        "- Operator fusion in Transformers\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working fused GELU + Bias kernel\n",
        "- Python correctness test\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 7 â€” Weekly Project: Full PyTorch LN Extension\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Deliver a **complete, reusable PyTorch LN extension** suitable for learning portfolios or ML systems interviews.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Integrate:\n",
        "  - LN forward\n",
        "  - LN backward\n",
        "- Clean up codebase:\n",
        "  - Clear APIs\n",
        "  - Consistent naming\n",
        "- Add:\n",
        "  - Python test scripts\n",
        "  - Benchmark script\n",
        "  - README-style documentation\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- End-to-end operator development\n",
        "- Code organization for extensions\n",
        "- Production-style validation and benchmarking\n",
        "\n",
        "### ðŸ“¦ Final Deliverable\n",
        "- A full **PyTorch LayerNorm C++/CUDA extension**\n",
        "- Runnable from Python with forward + backward\n",
        "- Benchmarked and validated\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Weekly Takeaway\n",
        "> **This week trains you to think like an ML systems engineer: designing, registering, debugging, validating, and benchmarking a real PyTorch operatorâ€”not just writing a CUDA kernel.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "9c0e243c-2b36-492f-9412-d7ae306a0ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Fri Jan 30 11:45:42 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wyuqf4e1iZBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04da878-907d-4d60-8fff-da44c456fe92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Waiting for headers] [Wai\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Waiting for headers] [2 I\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Waiting for headers] [2 I\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,328 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,671 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,293 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,640 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,288 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,886 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [45.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,999 kB]\n",
            "Get:24 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,571 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,605 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Fetched 39.2 MB in 4s (9,569 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4-config-common cuda-tools-12-4\n",
            "  cuda-visual-tools-12-4 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gds-tools-12-4 gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libcublas-12-4 libcublas-dev-12-4 libcufft-12-4\n",
            "  libcufft-dev-12-4 libcufile-12-4 libcufile-dev-12-4 libcurand-12-4\n",
            "  libcurand-dev-12-4 libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4\n",
            "  libcusparse-dev-12-4 libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4\n",
            "  libnvfatbin-dev-12-4 libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4\n",
            "  libnvjpeg-dev-12-4 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxcomposite1 libxkbcommon-x11-0 libxtst6 libxxf86dga1\n",
            "  nsight-compute-2024.1.1 nsight-systems-2023.4.4 openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4 cuda-toolkit-12-4-config-common\n",
            "  cuda-tools-12-4 cuda-visual-tools-12-4 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gds-tools-12-4\n",
            "  gsettings-desktop-schemas libatk-bridge2.0-0 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libatk1.0-0 libatk1.0-data libatspi2.0-0\n",
            "  libcublas-12-4 libcublas-dev-12-4 libcufft-12-4 libcufft-dev-12-4\n",
            "  libcufile-12-4 libcufile-dev-12-4 libcurand-12-4 libcurand-dev-12-4\n",
            "  libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4 libcusparse-dev-12-4\n",
            "  libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4 libnvfatbin-dev-12-4\n",
            "  libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4 libnvjpeg-dev-12-4\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 libxxf86dga1 nsight-compute-2024.1.1\n",
            "  nsight-systems-2023.4.4 openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "0 upgraded, 88 newly installed, 0 to remove and 110 not upgraded.\n",
            "Need to get 3,068 MB of archives.\n",
            "After this operation, 6,782 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-12-4 12.4.127-1 [1,200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-12-4 12.4.127-1 [16.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.29+7-1ubuntu1~22.04 [42.6 MB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-12-4 12.4.127-1 [3,830 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-12-4 12.4.127-1 [49.9 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-12-4 12.4.127-1 [225 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-12-4 12.4.127-1 [4,920 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.29+7-1ubuntu1~22.04 [214 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-12-4 12.4.127-1 [2,431 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:35 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-12-4 12.4.127-1 [51.5 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-12-4 12.4.127-1 [9,148 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-12-4 12.4.1-1 [2,538 B]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-12-4 12.4.127-1 [191 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4-config-common 12.4.127-1 [16.4 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-12-4 12.4.127-1 [165 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-12-4 12.4.127-1 [28.6 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-12-4 12.4.127-1 [1,000 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvm-12-4 12.4.131-1 [19.5 MB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-crt-12-4 12.4.131-1 [78.0 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-12-4 12.4.131-1 [32.0 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-12-4 12.4.127-1 [58.8 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-12-4 12.4.1-1 [2,510 B]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-12-4 12.4.127-1 [50.0 kB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-12-4 12.4.127-1 [17.6 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-12-4 12.4.127-1 [24.0 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-12-4 12.4.5.8-1 [231 MB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-12-4 11.2.1.3-1 [171 MB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-12-4 1.9.1.3-1 [850 kB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-12-4 10.3.5.147-1 [41.4 MB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-12-4 11.6.1.9-1 [78.9 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-12-4 12.3.1.170-1 [115 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-12-4 12.2.5.30-1 [95.5 MB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-12-4 12.4.127-1 [15.5 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-12-4 12.4.127-1 [721 kB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-12-4 12.3.1.117-1 [2,327 kB]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-12-4 12.4.1-1 [2,606 B]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-12-4 12.4.127-1 [18.7 kB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-12-4 12.4.127-1 [16.9 MB]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-dev-12-4 12.4.127-1 [87.0 kB]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-12-4 12.4.5.8-1 [249 MB]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-12-4 11.2.1.3-1 [342 MB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-12-4 1.9.1.3-1 [2,435 kB]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-12-4 10.3.5.147-1 [41.6 MB]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-12-4 11.6.1.9-1 [51.4 MB]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-12-4 12.3.1.170-1 [116 MB]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-12-4 12.2.5.30-1 [92.0 MB]\n",
            "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-dev-12-4 12.4.127-1 [13.9 MB]\n",
            "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-dev-12-4 12.4.127-1 [591 kB]\n",
            "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-12-4 12.3.1.117-1 [2,025 kB]\n",
            "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-12-4 12.4.1-1 [2,646 B]\n",
            "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-12-4 12.4.127-1 [119 MB]\n",
            "Get:79 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2024.1.1 2024.1.1.4-1 [594 MB]\n",
            "Get:80 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-12-4 12.4.1-1 [4,060 B]\n",
            "Get:81 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.4.4 2023.4.4.54-234433681190v0 [316 MB]\n",
            "Get:82 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-12-4 12.4.1-1 [3,348 B]\n",
            "Get:83 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-12-4 12.4.127-1 [145 kB]\n",
            "Get:84 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-12-4 12.4.127-1 [115 MB]\n",
            "Get:85 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-12-4 12.4.1-1 [2,942 B]\n",
            "Get:86 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-12-4 1.9.1.3-1 [39.0 MB]\n",
            "Get:87 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-12-4 12.4.1-1 [2,462 B]\n",
            "Get:88 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4 12.4.1-1 [3,336 B]\n",
            "Fetched 3,068 MB in 60s (50.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package cuda-cccl-12-4.\n",
            "Preparing to unpack .../05-cuda-cccl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-12-4.\n",
            "Preparing to unpack .../06-cuda-cupti-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-12-4.\n",
            "Preparing to unpack .../07-cuda-cupti-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-12-4.\n",
            "Preparing to unpack .../08-cuda-nvdisasm-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-12-4.\n",
            "Preparing to unpack .../09-cuda-cuobjdump-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-gdb-12-4.\n",
            "Preparing to unpack .../10-cuda-gdb-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-12-4.\n",
            "Preparing to unpack .../11-cuda-nvprof-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-12-4.\n",
            "Preparing to unpack .../12-cuda-nvtx-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-12-4.\n",
            "Preparing to unpack .../13-cuda-sanitizer-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-12-4.\n",
            "Preparing to unpack .../14-cuda-command-line-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-12-4.\n",
            "Preparing to unpack .../15-cuda-cuxxfilt-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4-config-common.\n",
            "Preparing to unpack .../16-cuda-toolkit-12-4-config-common_12.4.127-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-12-4.\n",
            "Preparing to unpack .../17-cuda-cudart-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-12-4.\n",
            "Preparing to unpack .../18-cuda-driver-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-12-4.\n",
            "Preparing to unpack .../19-cuda-cudart-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvm-12-4.\n",
            "Preparing to unpack .../20-cuda-nvvm-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-crt-12-4.\n",
            "Preparing to unpack .../21-cuda-crt-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-crt-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-12-4.\n",
            "Preparing to unpack .../22-cuda-nvcc-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-12-4.\n",
            "Preparing to unpack .../23-cuda-nvprune-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-compiler-12-4.\n",
            "Preparing to unpack .../24-cuda-compiler-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-documentation-12-4.\n",
            "Preparing to unpack .../25-cuda-documentation-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-12-4.\n",
            "Preparing to unpack .../26-cuda-nvrtc-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-12-4.\n",
            "Preparing to unpack .../27-cuda-opencl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-12-4.\n",
            "Preparing to unpack .../28-libcublas-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-12-4.\n",
            "Preparing to unpack .../29-libcufft-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-12-4.\n",
            "Preparing to unpack .../30-libcufile-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-12-4.\n",
            "Preparing to unpack .../31-libcurand-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-12-4.\n",
            "Preparing to unpack .../32-libcusolver-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-12-4.\n",
            "Preparing to unpack .../33-libcusparse-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-12-4.\n",
            "Preparing to unpack .../34-libnpp-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-12-4.\n",
            "Preparing to unpack .../35-libnvjitlink-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-12-4.\n",
            "Preparing to unpack .../36-libnvfatbin-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-12-4.\n",
            "Preparing to unpack .../37-libnvjpeg-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-12-4.\n",
            "Preparing to unpack .../38-cuda-libraries-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-12-4.\n",
            "Preparing to unpack .../39-cuda-profiler-api-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-12-4.\n",
            "Preparing to unpack .../40-cuda-nvrtc-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-dev-12-4.\n",
            "Preparing to unpack .../41-cuda-opencl-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-dev-12-4.\n",
            "Preparing to unpack .../42-libcublas-dev-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-dev-12-4.\n",
            "Preparing to unpack .../43-libcufft-dev-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-dev-12-4.\n",
            "Preparing to unpack .../44-libcufile-dev-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-dev-12-4.\n",
            "Preparing to unpack .../45-libcurand-dev-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-12-4.\n",
            "Preparing to unpack .../46-libcusolver-dev-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-12-4.\n",
            "Preparing to unpack .../47-libcusparse-dev-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-dev-12-4.\n",
            "Preparing to unpack .../48-libnpp-dev-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-dev-12-4.\n",
            "Preparing to unpack .../49-libnvjitlink-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-dev-12-4.\n",
            "Preparing to unpack .../50-libnvfatbin-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-12-4.\n",
            "Preparing to unpack .../51-libnvjpeg-dev-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-12-4.\n",
            "Preparing to unpack .../52-cuda-libraries-dev-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../53-openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../54-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../55-openjdk-11-jre_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../56-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-12-4.\n",
            "Preparing to unpack .../57-cuda-nsight-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package nsight-compute-2024.1.1.\n",
            "Preparing to unpack .../58-nsight-compute-2024.1.1_2024.1.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-12-4.\n",
            "Preparing to unpack .../59-cuda-nsight-compute-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../60-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../61-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../62-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../63-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../64-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../65-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../66-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../67-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../68-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../69-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../70-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package nsight-systems-2023.4.4.\n",
            "Preparing to unpack .../71-nsight-systems-2023.4.4_2023.4.4.54-234433681190v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-12-4.\n",
            "Preparing to unpack .../72-cuda-nsight-systems-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-12-4.\n",
            "Preparing to unpack .../73-cuda-nvml-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-12-4.\n",
            "Preparing to unpack .../74-cuda-nvvp-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-12-4.\n",
            "Preparing to unpack .../75-cuda-visual-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package gds-tools-12-4.\n",
            "Preparing to unpack .../76-gds-tools-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package cuda-tools-12-4.\n",
            "Preparing to unpack .../77-cuda-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4.\n",
            "Preparing to unpack .../78-cuda-toolkit-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../79-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../80-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../81-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../82-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../83-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../84-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../85-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../86-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../87-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Setting up gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Setting up cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Setting alternatives\n",
            "Setting up libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-12-4 (10.3.5.147-1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libnpp-12-4 (12.2.5.30-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcufft-12-4 (11.2.1.3-1) ...\n",
            "Setting up libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-12-4 (12.4.5.8-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Setting up libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Setting up cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Setting up libcufile-12-4 (1.9.1.3-1) ...\n",
            "Setting alternatives\n",
            "Setting up nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Setting up cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Setting up libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Setting up cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Setting up cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Setting up libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Setting up libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Setting up cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-crt-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Setting up libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Setting up cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTirfEh46lkF"
      },
      "source": [
        "## ðŸ§± Step 1 â€” Register LN Forward (C++ wrapper) + Python call test (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhZ_FIDt6lkF",
        "outputId": "a9d44af0-7285-40ef-91ac-a4801eb03e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Step1 extension loaded: <module 'ln_ext_forward' from '/root/.cache/torch_extensions/py312_cu126/ln_ext_forward/ln_ext_forward.so'>\n",
            "y: torch.Size([16, 128]) torch.float32 cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Step 1 (ONE CELL): forward-only LN extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Install ninja if not already present\n",
        "!pip install ninja\n",
        "\n",
        "# -----------------------------\n",
        "# Write files\n",
        "# -----------------------------\n",
        "os.makedirs(\"ln_ext\", exist_ok=True)\n",
        "\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "torch::Tensor ln_forward(torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "#define CHECK_2D(x) TORCH_CHECK((x).dim() == 2, #x \" must be 2D [B, D]\")\n",
        "#define CHECK_1D(x) TORCH_CHECK((x).dim() == 1, #x \" must be 1D [D]\")\n",
        "\n",
        "torch::Tensor ln_forward(\n",
        "torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps) {\n",
        "    // TODO:\n",
        "    // - validate: x CUDA/contiguous/float32/2D\n",
        "    // - validate: gamma,beta CUDA/contiguous/float32/1D and gamma.size(0)==D\n",
        "    // - allocate y [B,D], mean [B], inv_std [B]\n",
        "    // - call ln_forward_cuda_launcher(...)\n",
        "    // - return y (or return a tuple if you prefer, but keep API consistent)\n",
        "\n",
        "    // Placeholder (compilable but not correct):\n",
        "    // This section doesn't consider gamma and beta size and all data is on GPU\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x); CHECK_2D(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma); CHECK_1D(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);  CHECK_1D(beta);\n",
        "\n",
        "\n",
        "\n",
        "    TORCH_CHECK(x.device() == gamma.device(), \"x and gamma must be on same device\");\n",
        "    TORCH_CHECK(x.device() == beta.device(),  \"x and beta must be on same device\");\n",
        "    TORCH_CHECK(x.size(0) > 0 && x.size(1) > 0, \"x must have non-zero B and D\");\n",
        "    TORCH_CHECK(eps > 0.0, \"eps must be > 0\");\n",
        "\n",
        "\n",
        "\n",
        "    auto B = x.size(0);\n",
        "    auto D = x.size(1);\n",
        "    TORCH_CHECK(gamma.size(0) == D, \"gamma must have shape [D]\");\n",
        "    TORCH_CHECK(beta.size(0)  == D, \"beta must have shape [D]\");\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: replace with real launcher call\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "// TODO: warp reduce helper\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: implement with __shfl_down_sync\n",
        "    for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);}\n",
        "    return v;\n",
        "}\n",
        "\n",
        "// TODO: forward kernel (warp reduce)\n",
        "// x: [B,D], gamma/beta: [D], y:[B,D], mean/inv_std:[B]\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO:\n",
        "    // - map row(s) to warps/blocks\n",
        "    // - compute mean via warp reduction\n",
        "    // - compute variance via warp reduction\n",
        "    // - write mean/inv_std\n",
        "    // - normalize + affine and write y\n",
        "    int row = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    int lane = tid&32;\n",
        "    int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "    int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "    const float* xrow = x + (size_t)row * D;\n",
        "    float* yrow = y + (size_t)row * D;\n",
        "\n",
        "    // step 1: each thread acculates partial sum and sumq\n",
        "    float sumx = 0.0f;\n",
        "    float sumq = 0.0f;\n",
        "    for (int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        sumx += xi;\n",
        "        sumq += xi * xi;\n",
        "    }\n",
        "\n",
        "    //step2: reduce within each warp\n",
        "    sumx = warpReduceSum(sumx);\n",
        "    sumq = warpReduceSum(sumq);\n",
        "\n",
        "    //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "    __shared__ float warp_sums[32];\n",
        "    __shared__ float warp_sumsq[32];\n",
        "\n",
        "    if (lane == 0){\n",
        "        warp_sums[warp] = sumx;\n",
        "        warp_sumsq[warp] = sumq;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    //get block sum\n",
        "    float block_sum = 0.f, block_sumsq = 0.f;\n",
        "    if(warp == 0){\n",
        "      block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "      block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "      block_sum   = warpReduceSum(block_sum);\n",
        "      block_sumsq = warpReduceSum(block_sumsq);\n",
        "    }\n",
        "\n",
        "    //step4: broadcast mean and inv_std to all threads\n",
        "    __shared__ float sh_mu, sh_inv;\n",
        "    if (tid == 0){\n",
        "        float mu = block_sum / float(D);\n",
        "        float var = block_sumsq / (float)D - mu * mu;\n",
        "        float inv = rsqrtf(var + eps);\n",
        "        sh_mu = mu;\n",
        "        sh_inv = inv;\n",
        "        mean[row] = mu;\n",
        "        inv_std[row] = inv;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    //step5: get result\n",
        "    float mu = sh_mu, inv = sh_inv;\n",
        "    for(int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        float xhat = (xi - mu) * inv;\n",
        "        yrow[i] = xhat * gamma[i] + beta[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    // TODO: choose launch config\n",
        "    // dim3 block(0,0,1); // TODO\n",
        "    // dim3 grid(0,0,1);  // TODO\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "    int f_threads = 256;\n",
        "    if (D <= 128) f_threads = 128;\n",
        "    f_threads = round_up_warp(f_threads);\n",
        "\n",
        "    dim3 f_block(f_threads, 1, 1);\n",
        "    dim3 f_grid(B, 1, 1);\n",
        "\n",
        "\n",
        "    // Placeholder launch (won't run correctly until you set block/grid + kernel body)\n",
        "    ln_forward_kernel<<<f_grid, f_block>>>(\n",
        "        (const float*)x.data_ptr<float>(),\n",
        "        (const float*)gamma.data_ptr<float>(),\n",
        "        (const float*)beta.data_ptr<float>(),\n",
        "        (float*)y.data_ptr<float>(),\n",
        "        (float*)mean.data_ptr<float>(),\n",
        "        (float*)inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build extension\n",
        "# -----------------------------\n",
        "ext = load(\n",
        "    name=\"ln_ext_forward\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Step1 extension loaded:\", ext)\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: Python call test (disabled until TODOs are filled)\n",
        "# -----------------------------\n",
        "RUN_TEST = True\n",
        "if RUN_TEST:\n",
        "    B, D = 16, 128\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "    gamma = torch.ones(D, device=\"cuda\", dtype=torch.float32)\n",
        "    beta  = torch.zeros(D, device=\"cuda\", dtype=torch.float32)\n",
        "    y = ext.forward(x, gamma, beta, 1e-5)\n",
        "    print(\"y:\", y.shape, y.dtype, y.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcEewiv6lkF"
      },
      "source": [
        "## âœ… Step 2 â€” Register LN Backward + gradient correctness check (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ods2P8jU6lkF",
        "outputId": "7ba0eced-7cbe-47f0-c16e-ee14408abbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step2 extension loaded: <module 'ln_ext_fwd_bwd' from '/root/.cache/torch_extensions/py312_cu126/ln_ext_fwd_bwd/ln_ext_fwd_bwd.so'>\n",
            "[grad test] comparing ext vs torch.autograd reference\n",
            "dx: allclose=True  max_abs=9.536743e-07  max_rel=5.937974e-04  (atol=0.001, rtol=0.001)\n",
            "dgamma: allclose=True  max_abs=9.536743e-07  max_rel=1.114033e-05  (atol=0.001, rtol=0.001)\n",
            "dbeta: allclose=True  max_abs=9.536743e-07  max_rel=3.072305e-06  (atol=0.001, rtol=0.001)\n"
          ]
        }
      ],
      "source": [
        "# Step 2 (ONE CELL): add backward + autograd wrapper skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Overwrite C++/CUDA files for backward-enabled extension\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// Forward returns (y, mean, inv_std) for backward reuse\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps);\n",
        "\n",
        "// Backward returns (dx, dgamma, dbeta)\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x,\n",
        "                               torch::Tensor gamma,\n",
        "                               torch::Tensor mean,\n",
        "                               torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx,\n",
        "                               torch::Tensor dgamma,\n",
        "                               torch::Tensor dbeta);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "static void check_forward_args(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "\n",
        "    // Dims\n",
        "    TORCH_CHECK(x.dim() == 2, \"x must be 2D [B, D], got dim=\", x.dim());\n",
        "    TORCH_CHECK(gamma.dim() == 1, \"gamma must be 1D [D], got dim=\", gamma.dim());\n",
        "    TORCH_CHECK(beta.dim() == 1,  \"beta must be 1D [D], got dim=\", beta.dim());\n",
        "\n",
        "\n",
        "    // Shapes\n",
        "    const int64_t B = x.size(0);\n",
        "    const int64_t D = x.size(1);\n",
        "    TORCH_CHECK(B > 0 && D > 0, \"x must have non-zero shape, got [\", B, \", \", D, \"]\");\n",
        "    TORCH_CHECK(gamma.size(0) == D, \"gamma must have shape [D] with D=\", D,\n",
        "                \", got gamma.size(0)=\", gamma.size(0));\n",
        "    TORCH_CHECK(beta.size(0) == D,  \"beta must have shape [D] with D=\", D,\n",
        "                \", got beta.size(0)=\", beta.size(0));\n",
        "\n",
        "    // Same device (important for multi-GPU)\n",
        "    TORCH_CHECK(x.device() == gamma.device(),\n",
        "                \"x and gamma must be on the same device, got x=\", x.device(),\n",
        "                \" gamma=\", gamma.device());\n",
        "    TORCH_CHECK(x.device() == beta.device(),\n",
        "                \"x and beta must be on the same device, got x=\", x.device(),\n",
        "                \" beta=\", beta.device());\n",
        "\n",
        "}\n",
        "\n",
        "static void check_backward_args(torch::Tensor x, torch::Tensor gamma,\n",
        "                                torch::Tensor mean, torch::Tensor inv_std,\n",
        "                                torch::Tensor dout) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "\n",
        "    // Dims\n",
        "    TORCH_CHECK(x.dim() == 2, \"x must be 2D [B, D], got dim=\", x.dim());\n",
        "    TORCH_CHECK(dout.dim() == 2, \"dout must be 2D [B, D], got dim=\", dout.dim());\n",
        "    TORCH_CHECK(gamma.dim() == 1, \"gamma must be 1D [D], got dim=\", gamma.dim());\n",
        "    TORCH_CHECK(mean.dim() == 1, \"mean must be 1D [B], got dim=\", mean.dim());\n",
        "    TORCH_CHECK(inv_std.dim() == 1, \"inv_std must be 1D [B], got dim=\", inv_std.dim());\n",
        "\n",
        "    // Shapes\n",
        "    const int64_t B = x.size(0);\n",
        "    const int64_t D = x.size(1);\n",
        "    TORCH_CHECK(B > 0 && D > 0, \"x must have non-zero shape, got [\", B, \", \", D, \"]\");\n",
        "\n",
        "    TORCH_CHECK(dout.size(0) == B && dout.size(1) == D,\n",
        "                \"dout must have shape [B, D]=[\", B, \", \", D, \"], got [\",\n",
        "                dout.size(0), \", \", dout.size(1), \"]\");\n",
        "\n",
        "    TORCH_CHECK(gamma.size(0) == D,\n",
        "                \"gamma must have shape [D] with D=\", D,\n",
        "                \", got gamma.size(0)=\", gamma.size(0));\n",
        "\n",
        "    TORCH_CHECK(mean.size(0) == B,\n",
        "                \"mean must have shape [B] with B=\", B,\n",
        "                \", got mean.size(0)=\", mean.size(0));\n",
        "\n",
        "    TORCH_CHECK(inv_std.size(0) == B,\n",
        "                \"inv_std must have shape [B] with B=\", B,\n",
        "                \", got inv_std.size(0)=\", inv_std.size(0));\n",
        "\n",
        "    // Same device for all tensors\n",
        "    const auto dev = x.device();\n",
        "    TORCH_CHECK(gamma.device() == dev,   \"gamma must be on same device as x\");\n",
        "    TORCH_CHECK(mean.device() == dev,    \"mean must be on same device as x\");\n",
        "    TORCH_CHECK(inv_std.device() == dev, \"inv_std must be on same device as x\");\n",
        "    TORCH_CHECK(dout.device() == dev,    \"dout must be on same device as x\");\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps) {\n",
        "    check_forward_args(x, gamma, beta);\n",
        "    auto B = x.size(0);\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: real CUDA forward\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return {y, mean, inv_std};\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout) {\n",
        "    check_backward_args(x, gamma, mean, inv_std, dout);\n",
        "\n",
        "    auto dx = torch::empty_like(x);\n",
        "    auto dgamma = torch::zeros_like(gamma);\n",
        "    auto dbeta  = torch::zeros_like(gamma);\n",
        "\n",
        "    // TODO: real CUDA backward\n",
        "    ln_backward_cuda_launcher(x, gamma, mean, inv_std, dout, dx, dgamma, dbeta);\n",
        "\n",
        "    return {dx, dgamma, dbeta};\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "    m.def(\"backward\", &ln_backward, \"LayerNorm backward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: __shfl_down_sync\n",
        "   for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);\n",
        "  }\n",
        "    return v;\n",
        "}\n",
        "\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "  int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "  const float* xrow = x + (size_t)row * D;\n",
        "  float* yrow = y + (size_t)row * D;\n",
        "\n",
        "  // step 1: each thread acculates partial sum and sumq\n",
        "\n",
        "  float sumx = 0.0f;\n",
        "  float sumq = 0.0f;\n",
        "  for (int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      sumx += xi;\n",
        "      sumq += xi * xi;\n",
        "  }\n",
        "\n",
        "  //step2: reduce within each warp\n",
        "  sumx = warpReduceSum(sumx);\n",
        "  sumq = warpReduceSum(sumq);\n",
        "\n",
        "  //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "  __shared__ float warp_sums[32];\n",
        "  __shared__ float warp_sumsq[32];\n",
        "\n",
        "  if (lane == 0){\n",
        "      warp_sums[warp] = sumx;\n",
        "      warp_sumsq[warp] = sumq;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  //get block sum\n",
        "  float block_sum = 0.f, block_sumsq = 0.f;\n",
        "  if(warp == 0){\n",
        "    block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "    block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "    block_sum   = warpReduceSum(block_sum);\n",
        "    block_sumsq = warpReduceSum(block_sumsq);\n",
        "  }\n",
        "\n",
        "  //step4: broadcast mean and invstd to all threads\n",
        "  __shared__ float sh_mu, sh_inv;\n",
        "  if (tid == 0){\n",
        "      float mu = block_sum / float(D);\n",
        "      float var = block_sumsq / (float)D - mu * mu;\n",
        "      float inv = rsqrtf(var + eps);\n",
        "      sh_mu = mu;\n",
        "      sh_inv = inv;\n",
        "      mean[row] = mu;\n",
        "      inv_std[row] = inv;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "\n",
        "  //step5: get result\n",
        "  float mu = sh_mu, inv = sh_inv;\n",
        "  for(int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      yrow[i] = xhat * gamma[i] + beta[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void ln_backward_kernel(const float* __restrict__ x,\n",
        "                                   const float* __restrict__ gamma,\n",
        "                                   const float* __restrict__ mean,\n",
        "                                   const float* __restrict__ inv_std,\n",
        "                                   const float* __restrict__ dout,\n",
        "                                   float* __restrict__ dx,\n",
        "                                   float* __restrict__ dgamma,\n",
        "                                   float* __restrict__ dbeta,\n",
        "                                   int B, int D) {\n",
        "    // TODO:\n",
        "    // - compute dx\n",
        "    // - reduce dgamma/dbeta (atomics or 2-pass strategy)\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5;\n",
        "  int num_warps = (blockDim.x + 31) / 32;\n",
        "\n",
        "  const float* xrow  = x  + (size_t)row * D;\n",
        "  const float* dyrow = dout + (size_t)row * D;\n",
        "  float* dxrow       = dx + (size_t)row * D;\n",
        "\n",
        "  float mu = mean[row];\n",
        "  float inv = inv_std[row];\n",
        "\n",
        "  // step1: Accumulate partial sums for s1 and s2 in FP32\n",
        "  float s1 = 0.f;   // sum(g)\n",
        "  float s2 = 0.f;   // sum(g * xhat)\n",
        "\n",
        "  for(int i = tid; i < D; i+= blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float dyi = dyrow[i];\n",
        "      float gi  = dyi * gamma[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      s1 += gi;\n",
        "      s2 += gi * xhat;\n",
        "  }\n",
        "\n",
        "  // step2: Warp reduce\n",
        "\n",
        "  s1 = warpReduceSum(s1);\n",
        "  s2 = warpReduceSum(s2);\n",
        "\n",
        "  // step3: Warp partials -> shared, then reduce with warp 0\n",
        "  __shared__ float warp_s1[32];\n",
        "  __shared__ float warp_s2[32];\n",
        "\n",
        "  if (lane == 0) {\n",
        "    warp_s1[warp] = s1;\n",
        "    warp_s2[warp] = s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float block_s1 = 0.f;\n",
        "  float block_s2 = 0.f;\n",
        "\n",
        "  if (warp == 0) {\n",
        "    block_s1 = (lane < num_warps) ? warp_s1[lane] : 0.f;\n",
        "    block_s2 = (lane < num_warps) ? warp_s2[lane] : 0.f;\n",
        "    block_s1 = warpReduceSum(block_s1);\n",
        "    block_s2 = warpReduceSum(block_s2);\n",
        "  }\n",
        "\n",
        "  // step4: Broadcast block_s1/block_s2\n",
        "  __shared__ float sh_s1, sh_s2;\n",
        "  if (tid == 0) {\n",
        "    sh_s1 = block_s1;\n",
        "    sh_s2 = block_s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float S1 = sh_s1;\n",
        "  float S2 = sh_s2;\n",
        "\n",
        "  //step5: Write dx\n",
        "\n",
        "  float invD = 1.0f / (float)D;\n",
        "\n",
        "  for (int i = tid; i < D; i += blockDim.x) {\n",
        "    float xi  = xrow[i];\n",
        "    float dyi = dyrow[i];\n",
        "    float gi  = dyi * gamma[i];\n",
        "    float xhat = (xi - mu) * inv;\n",
        "\n",
        "    float dx_i = inv * (gi - S1 * invD - xhat * (S2 * invD));\n",
        "    dxrow[i] = dx_i;\n",
        "    atomicAdd(&dbeta[i],  dyi);\n",
        "    atomicAdd(&dgamma[i], dyi * xhat);\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta,\n",
        "                              torch::Tensor y, torch::Tensor mean, torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "\n",
        "    int f_threads = 256;\n",
        "    if (D <= 128) f_threads = 128;\n",
        "    f_threads = round_up_warp(f_threads);\n",
        "\n",
        "    dim3 f_block(f_threads, 1, 1);\n",
        "    dim3 f_grid(B, 1, 1);\n",
        "\n",
        "    //dim3 block(0,0,1); // TODO\n",
        "    //dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "    ln_forward_kernel<<<f_grid, f_block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(),\n",
        "        y.data_ptr<float>(), mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x, torch::Tensor gamma,\n",
        "                               torch::Tensor mean, torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx, torch::Tensor dgamma, torch::Tensor dbeta) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "    CHECK_CUDA(dx); CHECK_CONTIG(dx); CHECK_F32(dx);\n",
        "    CHECK_CUDA(dgamma); CHECK_CONTIG(dgamma); CHECK_F32(dgamma);\n",
        "    CHECK_CUDA(dbeta); CHECK_CONTIG(dbeta); CHECK_F32(dbeta);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    //dim3 block(0,0,1); // TODO\n",
        "    //dim3 grid(0,0,1);  // TODO\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "    int b_threads = 256;\n",
        "    if (D <= 128) b_threads = 128;\n",
        "    b_threads = round_up_warp(b_threads);\n",
        "\n",
        "    dim3 b_block(b_threads, 1, 1);\n",
        "    dim3 b_grid(B, 1, 1);\n",
        "\n",
        "    ln_backward_kernel<<<b_grid, b_block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(),\n",
        "        mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        dout.data_ptr<float>(),\n",
        "        dx.data_ptr<float>(), dgamma.data_ptr<float>(), dbeta.data_ptr<float>(),\n",
        "        B, D\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "ext = load(\n",
        "    name=\"ln_ext_fwd_bwd\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Step2 extension loaded:\", ext)\n",
        "\n",
        "# Optional: gradient check harness (disabled until TODOs are implemented)\n",
        "RUN_GRAD_TEST = True\n",
        "if RUN_GRAD_TEST:\n",
        "    B, D = 8, 256\n",
        "    eps = 1e-5\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # TODO: compare to torch.nn.functional.layer_norm gradients\n",
        "    # - call ext.forward -> (y, mean, inv_std)\n",
        "    # - build dout\n",
        "    # - call ext.backward -> (dx, dgamma, dbeta)\n",
        "    # - compare to autograd reference\n",
        "\n",
        "    # Step1: Extension forward -> (y, mean, inv_std)\n",
        "    y_ext, mean_ext, inv_std_ext = ext.forward(x, gamma, beta, eps)\n",
        "\n",
        "    # Build upstream gradient dout (same shape as y)\n",
        "    dout = torch.randn_like(y_ext)\n",
        "\n",
        "    # Extension backward -> (dx, dgamma, dbeta)\n",
        "    dx_ext, dgamma_ext, dbeta_ext = ext.backward(x, gamma, mean_ext, inv_std_ext, dout)\n",
        "\n",
        "    # Step2: Autograd reference using torch.layer_norm, IMPORTANT: use the same eps and weight/bias\n",
        "    # Make separate tensors so gradients don't mix with ext path\n",
        "    x_ref = x.detach().clone().requires_grad_(True)\n",
        "    gamma_ref = gamma.detach().clone().requires_grad_(True)\n",
        "    beta_ref  = beta.detach().clone().requires_grad_(True)\n",
        "\n",
        "    y_ref = F.layer_norm(x_ref, normalized_shape=(D,), weight=gamma_ref, bias=beta_ref, eps=eps)\n",
        "\n",
        "    # Backprop with the same dout\n",
        "    y_ref.backward(dout)\n",
        "\n",
        "    dx_ref = x_ref.grad\n",
        "    dgamma_ref = gamma_ref.grad\n",
        "    dbeta_ref = beta_ref.grad\n",
        "\n",
        "    # step3: Compare\n",
        "    def report(name, a, b, atol=1e-4, rtol=1e-3):\n",
        "        diff = (a - b).abs()\n",
        "        max_abs = diff.max().item()\n",
        "        max_rel = (diff / b.abs().clamp_min(1e-12)).max().item()\n",
        "        ok = torch.allclose(a, b, atol=atol, rtol=rtol)\n",
        "        print(f\"{name}: allclose={ok}  max_abs={max_abs:.6e}  max_rel={max_rel:.6e}  \"\n",
        "              f\"(atol={atol}, rtol={rtol})\")\n",
        "        return ok\n",
        "\n",
        "    print(\"[grad test] comparing ext vs torch.autograd reference\")\n",
        "    ok_dx = report(\"dx\", dx_ext, dx_ref, atol=1e-3, rtol=1e-3)\n",
        "    ok_dg = report(\"dgamma\", dgamma_ext, dgamma_ref, atol=1e-3, rtol=1e-3)\n",
        "    ok_db = report(\"dbeta\", dbeta_ext, dbeta_ref, atol=1e-3, rtol=1e-3)\n",
        "\n",
        "    if not (ok_dx and ok_dg and ok_db):\n",
        "        # Print a few worst indices for debugging\n",
        "        def worst_idx(a, b):\n",
        "            diff = (a - b).abs().reshape(-1)\n",
        "            idx = diff.argmax().item()\n",
        "            return idx, diff[idx].item()\n",
        "\n",
        "        idx, val = worst_idx(dx_ext, dx_ref)\n",
        "        print(f\"worst dx idx(flat)={idx}, abs_diff={val:.6e}\")\n",
        "        idx, val = worst_idx(dgamma_ext, dgamma_ref)\n",
        "        print(f\"worst dgamma idx(flat)={idx}, abs_diff={val:.6e}\")\n",
        "        idx, val = worst_idx(dbeta_ext, dbeta_ref)\n",
        "        print(f\"worst dbeta idx(flat)={idx}, abs_diff={val:.6e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W481Iav6lkG"
      },
      "source": [
        "## âœ… Step 3 â€” Compile / debug / edge cases (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRLEynmF6lkG",
        "outputId": "b74444fc-2436-4139-bfab-9b610acf14bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step3: Edge suite is ready. Set RUN_EDGE_TESTS=True after implementing kernels.\n"
          ]
        }
      ],
      "source": [
        "# Step 3 (ONE CELL): edge case test scaffolding + debug aids (NO SOLUTION)\n",
        "\n",
        "import torch, math\n",
        "\n",
        "# Edge cases to test (you can expand)\n",
        "CASES = [\n",
        "    (1, 7),       # tiny D\n",
        "    (2, 33),      # not multiple of warp\n",
        "    (4, 128),\n",
        "    (16, 1024),\n",
        "    (3, 4096),    # large D\n",
        "]\n",
        "\n",
        "# Toggle when your kernels are implemented\n",
        "RUN_EDGE_TESTS = False\n",
        "\n",
        "def run_edge_suite(ext):\n",
        "    for (B, D) in CASES:\n",
        "        x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "        gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "        beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "        # TODO: call ext.forward and validate shape/dtype/device\n",
        "        # y, mean, inv = ext.forward(x, gamma, beta, 1e-5)\n",
        "        # assert y.shape == x.shape\n",
        "        # assert mean.shape == (B,)\n",
        "        # assert inv.shape == (B,)\n",
        "        y, mean, inv = ext.forward(x, gamma, beta, eps)\n",
        "        assert y.shape == x.shape, f\"y shape mismatch: {y.shape} vs {x.shape}\"\n",
        "        assert mean.shape == (B,), f\"mean shape mismatch: {mean.shape} vs {(B,)}\"\n",
        "        assert inv.shape == (B,), f\"inv_std shape mismatch: {inv.shape} vs {(B,)}\"\n",
        "\n",
        "        assert y.dtype == x.dtype\n",
        "        assert mean.dtype == x.dtype\n",
        "        assert inv.dtype == x.dtype\n",
        "\n",
        "        assert y.device.type == \"cuda\"\n",
        "        assert mean.device.type == \"cuda\"\n",
        "        assert inv.device.type == \"cuda\"\n",
        "\n",
        "        # TODO: check numerical sanity (no NaN/Inf)\n",
        "        # assert torch.isfinite(y).all()\n",
        "        assert torch.isfinite(y).all(), \"NaN/Inf found in y\"\n",
        "        assert torch.isfinite(mean).all(), \"NaN/Inf found in mean\"\n",
        "        assert torch.isfinite(inv).all(), \"NaN/Inf found in inv_std\"\n",
        "\n",
        "        # TODO: backward sanity\n",
        "        # dout = torch.randn_like(x)\n",
        "        # dx, dgamma, dbeta = ext.backward(x, gamma, mean, inv, dout)\n",
        "        dout = torch.randn_like(y)\n",
        "\n",
        "        dx, dgamma, dbeta = ext.backward(x, gamma, mean, inv, dout)\n",
        "\n",
        "        # Shape checks\n",
        "        assert dx.shape == x.shape, f\"dx shape mismatch: {dx.shape}\"\n",
        "        assert dgamma.shape == gamma.shape, f\"dgamma shape mismatch: {dgamma.shape}\"\n",
        "        assert dbeta.shape == beta.shape, f\"dbeta shape mismatch: {dbeta.shape}\"\n",
        "\n",
        "        # Numerical sanity\n",
        "        assert torch.isfinite(dx).all(), \"NaN/Inf found in dx\"\n",
        "        assert torch.isfinite(dgamma).all(), \"NaN/Inf found in dgamma\"\n",
        "        assert torch.isfinite(dbeta).all(), \"NaN/Inf found in dbeta\"\n",
        "\n",
        "        print(f\"[EdgeCase] B={B} D={D} -> TODO checks\")\n",
        "\n",
        "# If you already loaded Day3 extension as `ext`, you can run:\n",
        "if RUN_EDGE_TESTS:\n",
        "    run_edge_suite(ext)\n",
        "else:\n",
        "    print(\"Step3: Edge suite is ready. Set RUN_EDGE_TESTS=True after implementing kernels.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34tExjB96lkG"
      },
      "source": [
        "## âœ… Step 4 â€” Fused GELU + Bias CUDA kernel (extension skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_7HHJO6W6lkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b249ef9f-df4b-4347-ce4a-6e571880e91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step4 extension loaded: <module 'fused_gelu_bias_ext_v2' from '/root/.cache/torch_extensions/py312_cu126/fused_gelu_bias_ext/fused_gelu_bias_ext_v2.so'>\n",
            "[test] dtype=torch.float32 B=256 D=1024 max_err=4.734993e-04 mean_err=1.230978e-04\n",
            "[test] dtype=torch.float16 B=256 D=1024 max_err=3.906250e-03 mean_err=1.772642e-04\n"
          ]
        }
      ],
      "source": [
        "# Step 4 (ONE CELL): fused Bias+GELU extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "os.makedirs(\"fused_gelu\", exist_ok=True)\n",
        "\n",
        "open(\"fused_gelu/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F16F32(x) TORCH_CHECK((x).scalar_type()==at::kHalf || (x).scalar_type()==at::kFloat, #x \" must be fp16 or fp32\")\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y);\n",
        "\n",
        "torch::Tensor fused_gelu_bias(torch::Tensor x, torch::Tensor bias) {\n",
        "    // TODO:\n",
        "    // - checks (CUDA/contig/dtype/shape)\n",
        "    // - allocate y\n",
        "    // - call launcher\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F16F32(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias); CHECK_F16F32(bias);\n",
        "\n",
        "    TORCH_CHECK(x.scalar_type() == bias.scalar_type(),\n",
        "                \"x and bias must have the same dtype, got x=\", x.scalar_type(),\n",
        "                \" bias=\", bias.scalar_type());\n",
        "\n",
        "    TORCH_CHECK(x.device() == bias.device(),\n",
        "                \"x and bias must be on the same CUDA device\");\n",
        "\n",
        "\n",
        "    const auto B = x.size(0);\n",
        "    const auto D = x.size(1);\n",
        "    TORCH_CHECK(B > 0 && D > 0, \"x must have non-zero shape [B,D], got [\", B, \",\", D, \"]\");\n",
        "    TORCH_CHECK(bias.size(0) == D,\n",
        "                \"bias must have shape [D] with D=x.size(1). Got bias.size(0)=\",\n",
        "                bias.size(0), \" D=\", D);\n",
        "\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    fused_gelu_bias_cuda_launcher(x, bias, y);\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &fused_gelu_bias, \"Fused Bias+GELU forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"fused_gelu/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_fp16.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "\n",
        "// TODO: implement GELU approximation or exact (no solution here)\n",
        "// kernel: y = GELU(x + bias)\n",
        "#define CUDA_CHECK(call) do {                                  \\\n",
        "  cudaError_t err = (call);                                    \\\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA error: \",              \\\n",
        "              cudaGetErrorString(err));                        \\\n",
        "} while(0)\n",
        "\n",
        "\n",
        "// GELU (tanh approximation): common in fused kernels\n",
        "__device__ __forceinline__ float gelu_tanh(float x){\n",
        "        // 0.5*x*(1+tanh(sqrt(2/pi)*(x+0.044715*x^3)))\n",
        "        const float square_pi = 0.7978845608028654f;\n",
        "        const float x_factor = 0.044715f;\n",
        "        float x3 = x * x * x;\n",
        "        float t = square_pi * (x + x_factor * x3);\n",
        "        return 0.5f * x * (1.0f + tanhf(t));\n",
        "        }\n",
        "\n",
        "__global__ void fused_bias_gelu_kernel_f32(\n",
        "        const float* __restrict__ x,\n",
        "        const float* __restrict__ bias,\n",
        "        float* __restrict__ y,\n",
        "        int B, int D){\n",
        "        int idx = (int)blockIdx.x * (int)blockDim.x + (int)threadIdx.x;\n",
        "        int n = B * D;\n",
        "        if (idx < n){\n",
        "            int d = idx - (idx / D) * D; // idx % D without slow mod on some arch\n",
        "            float t = x[idx] + bias[d];\n",
        "            y[idx] = gelu_tanh(t);\n",
        "            }\n",
        "        }\n",
        "\n",
        "__global__ void fused_bias_gelu_kernel_f16(\n",
        "        const __half* __restrict__ x,\n",
        "        const __half* __restrict__ bias,\n",
        "        __half* __restrict__ y,\n",
        "        int B, int D){\n",
        "        int idx = (int)blockIdx.x * (int)blockDim.x + (int)threadIdx.x;\n",
        "        int n = B * D;\n",
        "        if (idx < n){\n",
        "            int d = idx - (idx / D) * D; // idx % D without slow mod on some arch\n",
        "            float t = __half2float(x[idx]) + __half2float(bias[d]);\n",
        "            float out = gelu_tanh(t);\n",
        "            y[idx] = __float2half_rn(out);\n",
        "            }\n",
        "        }\n",
        "//__global__ void fused_bias_gelu_kernel(/* TODO args */) {\n",
        "    // TODO\n",
        "//}\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias);\n",
        "    CHECK_CUDA(y); CHECK_CONTIG(y);\n",
        "\n",
        "\n",
        "    TORCH_CHECK(x.dim() == 2, \"x must be 2D [B,D]\");\n",
        "    TORCH_CHECK(bias.dim() == 1, \"bias must be 1D [D]\");\n",
        "    TORCH_CHECK(y.dim() == 2, \"y must be 2D [B,D]\");\n",
        "    TORCH_CHECK(x.scalar_type() == bias.scalar_type(), \"x and bias must have same dtype\");\n",
        "    TORCH_CHECK(x.scalar_type() == y.scalar_type(), \"x and y must have same dtype\");\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "    TORCH_CHECK((int)bias.size(0) == D, \"bias.size(0) must equal D\");\n",
        "\n",
        "    int n = B * D;\n",
        "    constexpr int threads = 256;\n",
        "    int blocks = (n + threads - 1) / threads;\n",
        "    dim3 block(threads, 1, 1);\n",
        "    dim3 grid(blocks, 1, 1);\n",
        "\n",
        "    // TODO: grid/block\n",
        "    //dim3 block(0,0,1);\n",
        "    //dim3 grid(0,0,1);\n",
        "\n",
        "    // TODO: dispatch by dtype (fp16/fp32)\n",
        "    // fused_bias_gelu_kernel<<<grid, block>>>(...);\n",
        "\n",
        "\n",
        "    if (x.scalar_type() == at::kFloat) {\n",
        "        fused_bias_gelu_kernel_f32<<<blocks, threads>>>(\n",
        "            x.data_ptr<float>(),\n",
        "            bias.data_ptr<float>(),\n",
        "            y.data_ptr<float>(),\n",
        "            B, D\n",
        "        );\n",
        "    } else if (x.scalar_type() == at::kHalf) {\n",
        "        fused_bias_gelu_kernel_f16<<<blocks, threads>>>(\n",
        "            (const __half*)x.data_ptr<at::Half>(),\n",
        "            (const __half*)bias.data_ptr<at::Half>(),\n",
        "            (__half*)y.data_ptr<at::Half>(),\n",
        "            B, D\n",
        "        );\n",
        "    } else {\n",
        "        TORCH_CHECK(false, \"Unsupported dtype: \", x.scalar_type());\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "fused = load(\n",
        "    name=\"fused_gelu_bias_ext\",\n",
        "    sources=[\"fused_gelu/ext.cpp\", \"fused_gelu/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Step4 extension loaded:\", fused)\n",
        "\n",
        "RUN_TEST = True\n",
        "if RUN_TEST:\n",
        "    # TODO: compare vs torch.nn.functional.gelu(x + bias)\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    def test_case(B, D, dtype):\n",
        "        x = torch.randn(B, D, device=\"cuda\", dtype=dtype)\n",
        "        bias = torch.randn(D, device=\"cuda\", dtype=dtype)\n",
        "\n",
        "        y = fused.forward(x, bias)\n",
        "        y_ref = F.gelu(x + bias)  # PyTorch reference (may be exact/approx depending on version)\n",
        "\n",
        "        # sanity\n",
        "        assert y.shape == x.shape and y.dtype == x.dtype and y.device == x.device\n",
        "        assert torch.isfinite(y).all(), \"NaN/Inf in fused output\"\n",
        "\n",
        "        diff = (y - y_ref).abs()\n",
        "        print(f\"[test] dtype={dtype} B={B} D={D} max_err={diff.max().item():.6e} mean_err={diff.mean().item():.6e}\")\n",
        "\n",
        "    test_case(256, 1024, torch.float32)\n",
        "    test_case(256, 1024, torch.float16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FO5iCX6lkH"
      },
      "source": [
        "## âœ… Step 5 â€” Weekly project packaging (full LN extension) + scripts (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJWwSsM36lkH"
      },
      "outputs": [],
      "source": [
        "# Step 5 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Step6 packaging skeleton created under ./project_ln/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiY-3BfN6lkH"
      },
      "outputs": [],
      "source": [
        "# Step 5 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Step6 packaging skeleton created under ./project_ln/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}