{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation (Day by Day): PyTorch LayerNorm C++/CUDA Extension\n",
        "\n",
        "This week focuses on building a **production-style PyTorch LayerNorm (LN) extension**, starting from a C++ forward wrapper and ending with a **complete forward + backward CUDA implementation**, benchmarked against PyTorchâ€™s official kernel.\n",
        "\n",
        "The goal is to understand **how real PyTorch operators are written, registered, validated, and optimized**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 2 â€” Register LN Forward (C++ Wrapper) & Python Test\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Expose a **custom LN forward implementation** to Python via a PyTorch C++ extension and validate the **Python â†’ C++ â†’ CUDA** execution path.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Write a C++ forward wrapper using ATen:\n",
        "  - Accept `at::Tensor` inputs\n",
        "  - Validate device, dtype, and layout\n",
        "  - Allocate output tensors\n",
        "  - Dispatch to a CUDA kernel\n",
        "- Register the forward function using `PYBIND11_MODULE`\n",
        "- Call the operator from Python and verify:\n",
        "  - Correct execution\n",
        "  - Correct output shape and dtype\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- PyTorch C++ extension registration\n",
        "- ATen tensor checks and allocation\n",
        "- Python â†” C++ ABI boundary\n",
        "- Kernel launch from C++\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Callable `ln_forward()` from Python\n",
        "- Successful Python test script\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 3 â€” Register LN Backward & Verify Gradient Correctness\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Extend the LN operator to support **backward propagation** and ensure it integrates correctly with PyTorchâ€™s autograd system.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement and register LN backward:\n",
        "  - Compute gradients for `dx`, `dgamma`, and `dbeta`\n",
        "  - Use CUDA kernels for gradient computation\n",
        "- Bind backward logic via:\n",
        "  - Custom `torch::autograd::Function` **or**\n",
        "  - Manual backward registration (educational setup)\n",
        "- Verify gradient correctness:\n",
        "  - Compare against PyTorch autograd results\n",
        "  - Use numerical tolerances\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Autograd mechanics\n",
        "- Forward/backward dependency management\n",
        "- Gradient reduction patterns\n",
        "- Numerical stability in backward pass\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working backward kernel\n",
        "- Gradient correctness test (PASS)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 4 â€” Compile, Debug, and Fix Edge Cases\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Harden the extension so it behaves correctly across **realistic and corner-case inputs**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Fix compilation issues:\n",
        "  - Template errors\n",
        "  - Device/dtype mismatches\n",
        "- Debug runtime errors:\n",
        "  - Illegal memory access\n",
        "  - Incorrect indexing\n",
        "- Handle edge cases:\n",
        "  - Non-multiple-of-warp feature sizes\n",
        "  - Small batch sizes\n",
        "  - Large/small variance values\n",
        "- Add assertions and sanity checks\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- CUDA debugging strategies\n",
        "- Shape- and stride-related pitfalls\n",
        "- Numerical edge cases in normalization\n",
        "- Defensive programming in C++ extensions\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Stable, crash-free extension\n",
        "- Clean compilation with `-O3`\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 5 â€” Benchmark: Custom LN vs PyTorch Official Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Quantitatively compare your LN implementation against **PyTorchâ€™s official LayerNorm**.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Benchmark forward and backward:\n",
        "  - Your custom LN extension\n",
        "  - `torch.nn.LayerNorm`\n",
        "- Measure:\n",
        "  - Kernel execution time\n",
        "  - End-to-end forward/backward time\n",
        "- Use consistent input sizes and warm-up\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Fair benchmarking methodology\n",
        "- Kernel launch overhead\n",
        "- Memory-bound vs compute-bound behavior\n",
        "- Why official kernels are highly optimized\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Benchmark table or logs\n",
        "- Short performance analysis\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 6 â€” Implement Fused GELU + Bias CUDA Kernel\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Apply the same extension workflow to a **fused operator**, reinforcing kernel fusion concepts common in ML systems.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Implement a CUDA kernel that fuses:\n",
        "  - Bias addition\n",
        "  - GELU activation\n",
        "- Register the fused kernel as a PyTorch extension\n",
        "- Test correctness against PyTorch reference\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- Kernel fusion benefits\n",
        "- Reducing memory traffic\n",
        "- Elementwise kernel optimization\n",
        "- Operator fusion in Transformers\n",
        "\n",
        "### ðŸ“¦ Deliverables\n",
        "- Working fused GELU + Bias kernel\n",
        "- Python correctness test\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—“ï¸ Day 7 â€” Weekly Project: Full PyTorch LN Extension\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Deliver a **complete, reusable PyTorch LN extension** suitable for learning portfolios or ML systems interviews.\n",
        "\n",
        "### ðŸ§© Tasks\n",
        "- Integrate:\n",
        "  - LN forward\n",
        "  - LN backward\n",
        "- Clean up codebase:\n",
        "  - Clear APIs\n",
        "  - Consistent naming\n",
        "- Add:\n",
        "  - Python test scripts\n",
        "  - Benchmark script\n",
        "  - README-style documentation\n",
        "\n",
        "### ðŸ§  Key Concepts\n",
        "- End-to-end operator development\n",
        "- Code organization for extensions\n",
        "- Production-style validation and benchmarking\n",
        "\n",
        "### ðŸ“¦ Final Deliverable\n",
        "- A full **PyTorch LayerNorm C++/CUDA extension**\n",
        "- Runnable from Python with forward + backward\n",
        "- Benchmarked and validated\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Weekly Takeaway\n",
        "> **This week trains you to think like an ML systems engineer: designing, registering, debugging, validating, and benchmarking a real PyTorch operatorâ€”not just writing a CUDA kernel.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "55243d70-7905-4481-fbdc-8c4f8041c753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Tue Jan 27 11:16:53 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyuqf4e1iZBl"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTirfEh46lkF"
      },
      "source": [
        "## ðŸ§± Step 1 â€” Register LN Forward (C++ wrapper) + Python call test (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhZ_FIDt6lkF",
        "outputId": "45449912-431b-4859-bc1e-3a583b07beaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Step1 extension loaded: <module 'ln_ext_forward' from '/root/.cache/torch_extensions/py312_cu126/ln_ext_forward/ln_ext_forward.so'>\n",
            "y: torch.Size([16, 128]) torch.float32 cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Day 1 (ONE CELL): forward-only LN extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Install ninja if not already present\n",
        "!pip install ninja\n",
        "\n",
        "# -----------------------------\n",
        "# Write files\n",
        "# -----------------------------\n",
        "os.makedirs(\"ln_ext\", exist_ok=True)\n",
        "\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "torch::Tensor ln_forward(torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "#define CHECK_2D(x) TORCH_CHECK((x).dim() == 2, #x \" must be 2D [B, D]\")\n",
        "#define CHECK_1D(x) TORCH_CHECK((x).dim() == 1, #x \" must be 1D [D]\")\n",
        "\n",
        "torch::Tensor ln_forward(\n",
        "torch::Tensor x,\n",
        "                         torch::Tensor gamma,\n",
        "                         torch::Tensor beta,\n",
        "                         double eps) {\n",
        "    // TODO:\n",
        "    // - validate: x CUDA/contiguous/float32/2D\n",
        "    // - validate: gamma,beta CUDA/contiguous/float32/1D and gamma.size(0)==D\n",
        "    // - allocate y [B,D], mean [B], inv_std [B]\n",
        "    // - call ln_forward_cuda_launcher(...)\n",
        "    // - return y (or return a tuple if you prefer, but keep API consistent)\n",
        "\n",
        "    // Placeholder (compilable but not correct):\n",
        "    // This section doesn't consider gamma and beta size and all data is on GPU\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x); CHECK_2D(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma); CHECK_1D(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);  CHECK_1D(beta);\n",
        "\n",
        "\n",
        "\n",
        "    TORCH_CHECK(x.device() == gamma.device(), \"x and gamma must be on same device\");\n",
        "    TORCH_CHECK(x.device() == beta.device(),  \"x and beta must be on same device\");\n",
        "    TORCH_CHECK(x.size(0) > 0 && x.size(1) > 0, \"x must have non-zero B and D\");\n",
        "    TORCH_CHECK(eps > 0.0, \"eps must be > 0\");\n",
        "\n",
        "\n",
        "\n",
        "    auto B = x.size(0);\n",
        "    auto D = x.size(1);\n",
        "    TORCH_CHECK(gamma.size(0) == D, \"gamma must have shape [D]\");\n",
        "    TORCH_CHECK(beta.size(0)  == D, \"beta must have shape [D]\");\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: replace with real launcher call\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "// TODO: warp reduce helper\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: implement with __shfl_down_sync\n",
        "    for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);}\n",
        "    return v;\n",
        "}\n",
        "\n",
        "// TODO: forward kernel (warp reduce)\n",
        "// x: [B,D], gamma/beta: [D], y:[B,D], mean/inv_std:[B]\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO:\n",
        "    // - map row(s) to warps/blocks\n",
        "    // - compute mean via warp reduction\n",
        "    // - compute variance via warp reduction\n",
        "    // - write mean/inv_std\n",
        "    // - normalize + affine and write y\n",
        "    int row = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    int lane = tid&32;\n",
        "    int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "    int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "    const float* xrow = x + (size_t)row * D;\n",
        "    float* yrow = y + (size_t)row * D;\n",
        "\n",
        "    // step 1: each thread acculates partial sum and sumq\n",
        "    float sumx = 0.0f;\n",
        "    float sumq = 0.0f;\n",
        "    for (int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        sumx += xi;\n",
        "        sumq += xi * xi;\n",
        "    }\n",
        "\n",
        "    //step2: reduce within each warp\n",
        "    sumx = warpReduceSum(sumx);\n",
        "    sumq = warpReduceSum(sumq);\n",
        "\n",
        "    //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "    __shared__ float warp_sums[32];\n",
        "    __shared__ float warp_sumsq[32];\n",
        "\n",
        "    if (lane == 0){\n",
        "        warp_sums[warp] = sumx;\n",
        "        warp_sumsq[warp] = sumq;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    //get block sum\n",
        "    float block_sum = 0.f, block_sumsq = 0.f;\n",
        "    if(warp == 0){\n",
        "      block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "      block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "      block_sum   = warpReduceSum(block_sum);\n",
        "      block_sumsq = warpReduceSum(block_sumsq);\n",
        "    }\n",
        "\n",
        "    //step4: broadcast mean and inv_std to all threads\n",
        "    __shared__ float sh_mu, sh_inv;\n",
        "    if (tid == 0){\n",
        "        float mu = block_sum / float(D);\n",
        "        float var = block_sumsq / (float)D - mu * mu;\n",
        "        float inv = rsqrtf(var + eps);\n",
        "        sh_mu = mu;\n",
        "        sh_inv = inv;\n",
        "        mean[row] = mu;\n",
        "        inv_std[row] = inv;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    //step5: get result\n",
        "    float mu = sh_mu, inv = sh_inv;\n",
        "    for(int i = tid; i < D; i += blockDim.x){\n",
        "        float xi = xrow[i];\n",
        "        float xhat = (xi - mu) * inv;\n",
        "        yrow[i] = xhat * gamma[i] + beta[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    // TODO: choose launch config\n",
        "    // dim3 block(0,0,1); // TODO\n",
        "    // dim3 grid(0,0,1);  // TODO\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "    int f_threads = 256;\n",
        "    if (D <= 128) f_threads = 128;\n",
        "    f_threads = round_up_warp(f_threads);\n",
        "\n",
        "    dim3 f_block(f_threads, 1, 1);\n",
        "    dim3 f_grid(B, 1, 1);\n",
        "\n",
        "\n",
        "    // Placeholder launch (won't run correctly until you set block/grid + kernel body)\n",
        "    ln_forward_kernel<<<f_grid, f_block>>>(\n",
        "        (const float*)x.data_ptr<float>(),\n",
        "        (const float*)gamma.data_ptr<float>(),\n",
        "        (const float*)beta.data_ptr<float>(),\n",
        "        (float*)y.data_ptr<float>(),\n",
        "        (float*)mean.data_ptr<float>(),\n",
        "        (float*)inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build extension\n",
        "# -----------------------------\n",
        "ext = load(\n",
        "    name=\"ln_ext_forward\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Step1 extension loaded:\", ext)\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: Python call test (disabled until TODOs are filled)\n",
        "# -----------------------------\n",
        "RUN_TEST = True\n",
        "if RUN_TEST:\n",
        "    B, D = 16, 128\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "    gamma = torch.ones(D, device=\"cuda\", dtype=torch.float32)\n",
        "    beta  = torch.zeros(D, device=\"cuda\", dtype=torch.float32)\n",
        "    y = ext.forward(x, gamma, beta, 1e-5)\n",
        "    print(\"y:\", y.shape, y.dtype, y.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcEewiv6lkF"
      },
      "source": [
        "## âœ… Step 2 â€” Register LN Backward + gradient correctness check (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ods2P8jU6lkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3815aee0-85fa-4e34-cbd9-b8c526e3cf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step2 extension loaded: <module 'ln_ext_fwd_bwd_v2' from '/root/.cache/torch_extensions/py312_cu126/ln_ext_fwd_bwd/ln_ext_fwd_bwd_v2.so'>\n",
            "[grad test] comparing ext vs torch.autograd reference\n",
            "dx: allclose=True  max_abs=9.536743e-07  max_rel=8.172718e-06  (atol=0.001, rtol=0.001)\n",
            "dgamma: allclose=True  max_abs=9.536743e-07  max_rel=3.466087e-06  (atol=0.001, rtol=0.001)\n",
            "dbeta: allclose=True  max_abs=9.536743e-07  max_rel=3.913940e-06  (atol=0.001, rtol=0.001)\n"
          ]
        }
      ],
      "source": [
        "# Day 2 (ONE CELL): add backward + autograd wrapper skeleton (NO SOLUTION)\n",
        "\n",
        "import os, textwrap, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Overwrite C++/CUDA files for backward-enabled extension\n",
        "open(\"ln_ext/ext.h\", \"w\").write(r\"\"\"\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// Forward returns (y, mean, inv_std) for backward reuse\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps);\n",
        "\n",
        "// Backward returns (dx, dgamma, dbeta)\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout);\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x,\n",
        "                              torch::Tensor gamma,\n",
        "                              torch::Tensor beta,\n",
        "                              torch::Tensor y,\n",
        "                              torch::Tensor mean,\n",
        "                              torch::Tensor inv_std,\n",
        "                              double eps);\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x,\n",
        "                               torch::Tensor gamma,\n",
        "                               torch::Tensor mean,\n",
        "                               torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx,\n",
        "                               torch::Tensor dgamma,\n",
        "                               torch::Tensor dbeta);\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "static void check_forward_args(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "\n",
        "    // Dims\n",
        "    TORCH_CHECK(x.dim() == 2, \"x must be 2D [B, D], got dim=\", x.dim());\n",
        "    TORCH_CHECK(gamma.dim() == 1, \"gamma must be 1D [D], got dim=\", gamma.dim());\n",
        "    TORCH_CHECK(beta.dim() == 1,  \"beta must be 1D [D], got dim=\", beta.dim());\n",
        "\n",
        "\n",
        "    // Shapes\n",
        "    const int64_t B = x.size(0);\n",
        "    const int64_t D = x.size(1);\n",
        "    TORCH_CHECK(B > 0 && D > 0, \"x must have non-zero shape, got [\", B, \", \", D, \"]\");\n",
        "    TORCH_CHECK(gamma.size(0) == D, \"gamma must have shape [D] with D=\", D,\n",
        "                \", got gamma.size(0)=\", gamma.size(0));\n",
        "    TORCH_CHECK(beta.size(0) == D,  \"beta must have shape [D] with D=\", D,\n",
        "                \", got beta.size(0)=\", beta.size(0));\n",
        "\n",
        "    // Same device (important for multi-GPU)\n",
        "    TORCH_CHECK(x.device() == gamma.device(),\n",
        "                \"x and gamma must be on the same device, got x=\", x.device(),\n",
        "                \" gamma=\", gamma.device());\n",
        "    TORCH_CHECK(x.device() == beta.device(),\n",
        "                \"x and beta must be on the same device, got x=\", x.device(),\n",
        "                \" beta=\", beta.device());\n",
        "\n",
        "}\n",
        "\n",
        "static void check_backward_args(torch::Tensor x, torch::Tensor gamma,\n",
        "                                torch::Tensor mean, torch::Tensor inv_std,\n",
        "                                torch::Tensor dout) {\n",
        "    // TODO: add full checks (dims, shapes)\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "\n",
        "    // Dims\n",
        "    TORCH_CHECK(x.dim() == 2, \"x must be 2D [B, D], got dim=\", x.dim());\n",
        "    TORCH_CHECK(dout.dim() == 2, \"dout must be 2D [B, D], got dim=\", dout.dim());\n",
        "    TORCH_CHECK(gamma.dim() == 1, \"gamma must be 1D [D], got dim=\", gamma.dim());\n",
        "    TORCH_CHECK(mean.dim() == 1, \"mean must be 1D [B], got dim=\", mean.dim());\n",
        "    TORCH_CHECK(inv_std.dim() == 1, \"inv_std must be 1D [B], got dim=\", inv_std.dim());\n",
        "\n",
        "    // Shapes\n",
        "    const int64_t B = x.size(0);\n",
        "    const int64_t D = x.size(1);\n",
        "    TORCH_CHECK(B > 0 && D > 0, \"x must have non-zero shape, got [\", B, \", \", D, \"]\");\n",
        "\n",
        "    TORCH_CHECK(dout.size(0) == B && dout.size(1) == D,\n",
        "                \"dout must have shape [B, D]=[\", B, \", \", D, \"], got [\",\n",
        "                dout.size(0), \", \", dout.size(1), \"]\");\n",
        "\n",
        "    TORCH_CHECK(gamma.size(0) == D,\n",
        "                \"gamma must have shape [D] with D=\", D,\n",
        "                \", got gamma.size(0)=\", gamma.size(0));\n",
        "\n",
        "    TORCH_CHECK(mean.size(0) == B,\n",
        "                \"mean must have shape [B] with B=\", B,\n",
        "                \", got mean.size(0)=\", mean.size(0));\n",
        "\n",
        "    TORCH_CHECK(inv_std.size(0) == B,\n",
        "                \"inv_std must have shape [B] with B=\", B,\n",
        "                \", got inv_std.size(0)=\", inv_std.size(0));\n",
        "\n",
        "    // Same device for all tensors\n",
        "    const auto dev = x.device();\n",
        "    TORCH_CHECK(gamma.device() == dev,   \"gamma must be on same device as x\");\n",
        "    TORCH_CHECK(mean.device() == dev,    \"mean must be on same device as x\");\n",
        "    TORCH_CHECK(inv_std.device() == dev, \"inv_std must be on same device as x\");\n",
        "    TORCH_CHECK(dout.device() == dev,    \"dout must be on same device as x\");\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_forward(torch::Tensor x,\n",
        "                                      torch::Tensor gamma,\n",
        "                                      torch::Tensor beta,\n",
        "                                      double eps) {\n",
        "    check_forward_args(x, gamma, beta);\n",
        "    auto B = x.size(0);\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    auto mean = torch::empty({B}, x.options());\n",
        "    auto inv_std = torch::empty({B}, x.options());\n",
        "\n",
        "    // TODO: real CUDA forward\n",
        "    ln_forward_cuda_launcher(x, gamma, beta, y, mean, inv_std, eps);\n",
        "\n",
        "    return {y, mean, inv_std};\n",
        "}\n",
        "\n",
        "std::vector<torch::Tensor> ln_backward(torch::Tensor x,\n",
        "                                       torch::Tensor gamma,\n",
        "                                       torch::Tensor mean,\n",
        "                                       torch::Tensor inv_std,\n",
        "                                       torch::Tensor dout) {\n",
        "    check_backward_args(x, gamma, mean, inv_std, dout);\n",
        "\n",
        "    auto dx = torch::empty_like(x);\n",
        "    auto dgamma = torch::zeros_like(gamma);\n",
        "    auto dbeta  = torch::zeros_like(gamma);\n",
        "\n",
        "    // TODO: real CUDA backward\n",
        "    ln_backward_cuda_launcher(x, gamma, mean, inv_std, dout, dx, dgamma, dbeta);\n",
        "\n",
        "    return {dx, dgamma, dbeta};\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &ln_forward, \"LayerNorm forward (CUDA, skeleton)\");\n",
        "    m.def(\"backward\", &ln_backward, \"LayerNorm backward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"ln_ext/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F32(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "    // TODO: __shfl_down_sync\n",
        "   for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);\n",
        "  }\n",
        "    return v;\n",
        "}\n",
        "\n",
        "__global__ void ln_forward_kernel(const float* __restrict__ x,\n",
        "                                  const float* __restrict__ gamma,\n",
        "                                  const float* __restrict__ beta,\n",
        "                                  float* __restrict__ y,\n",
        "                                  float* __restrict__ mean,\n",
        "                                  float* __restrict__ inv_std,\n",
        "                                  int B, int D, float eps) {\n",
        "    // TODO\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "  int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "  const float* xrow = x + (size_t)row * D;\n",
        "  float* yrow = y + (size_t)row * D;\n",
        "\n",
        "  // step 1: each thread acculates partial sum and sumq\n",
        "\n",
        "  float sumx = 0.0f;\n",
        "  float sumq = 0.0f;\n",
        "  for (int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      sumx += xi;\n",
        "      sumq += xi * xi;\n",
        "  }\n",
        "\n",
        "  //step2: reduce within each warp\n",
        "  sumx = warpReduceSum(sumx);\n",
        "  sumq = warpReduceSum(sumq);\n",
        "\n",
        "  //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "  __shared__ float warp_sums[32];\n",
        "  __shared__ float warp_sumsq[32];\n",
        "\n",
        "  if (lane == 0){\n",
        "      warp_sums[warp] = sumx;\n",
        "      warp_sumsq[warp] = sumq;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  //get block sum\n",
        "  float block_sum = 0.f, block_sumsq = 0.f;\n",
        "  if(warp == 0){\n",
        "    block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "    block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "    block_sum   = warpReduceSum(block_sum);\n",
        "    block_sumsq = warpReduceSum(block_sumsq);\n",
        "  }\n",
        "\n",
        "  //step4: broadcast mean and invstd to all threads\n",
        "  __shared__ float sh_mu, sh_inv;\n",
        "  if (tid == 0){\n",
        "      float mu = block_sum / float(D);\n",
        "      float var = block_sumsq / (float)D - mu * mu;\n",
        "      float inv = rsqrtf(var + eps);\n",
        "      sh_mu = mu;\n",
        "      sh_inv = inv;\n",
        "      mean[row] = mu;\n",
        "      inv_std[row] = inv;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "\n",
        "  //step5: get result\n",
        "  float mu = sh_mu, inv = sh_inv;\n",
        "  for(int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      yrow[i] = xhat * gamma[i] + beta[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void ln_backward_kernel(const float* __restrict__ x,\n",
        "                                   const float* __restrict__ gamma,\n",
        "                                   const float* __restrict__ mean,\n",
        "                                   const float* __restrict__ inv_std,\n",
        "                                   const float* __restrict__ dout,\n",
        "                                   float* __restrict__ dx,\n",
        "                                   float* __restrict__ dgamma,\n",
        "                                   float* __restrict__ dbeta,\n",
        "                                   int B, int D) {\n",
        "    // TODO:\n",
        "    // - compute dx\n",
        "    // - reduce dgamma/dbeta (atomics or 2-pass strategy)\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5;\n",
        "  int num_warps = (blockDim.x + 31) / 32;\n",
        "\n",
        "  const float* xrow  = x  + (size_t)row * D;\n",
        "  const float* dyrow = dout + (size_t)row * D;\n",
        "  float* dxrow       = dx + (size_t)row * D;\n",
        "\n",
        "  float mu = mean[row];\n",
        "  float inv = inv_std[row];\n",
        "\n",
        "  // step1: Accumulate partial sums for s1 and s2 in FP32\n",
        "  float s1 = 0.f;   // sum(g)\n",
        "  float s2 = 0.f;   // sum(g * xhat)\n",
        "\n",
        "  for(int i = tid; i < D; i+= blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float dyi = dyrow[i];\n",
        "      float gi  = dyi * gamma[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      s1 += gi;\n",
        "      s2 += gi * xhat;\n",
        "  }\n",
        "\n",
        "  // step2: Warp reduce\n",
        "\n",
        "  s1 = warpReduceSum(s1);\n",
        "  s2 = warpReduceSum(s2);\n",
        "\n",
        "  // step3: Warp partials -> shared, then reduce with warp 0\n",
        "  __shared__ float warp_s1[32];\n",
        "  __shared__ float warp_s2[32];\n",
        "\n",
        "  if (lane == 0) {\n",
        "    warp_s1[warp] = s1;\n",
        "    warp_s2[warp] = s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float block_s1 = 0.f;\n",
        "  float block_s2 = 0.f;\n",
        "\n",
        "  if (warp == 0) {\n",
        "    block_s1 = (lane < num_warps) ? warp_s1[lane] : 0.f;\n",
        "    block_s2 = (lane < num_warps) ? warp_s2[lane] : 0.f;\n",
        "    block_s1 = warpReduceSum(block_s1);\n",
        "    block_s2 = warpReduceSum(block_s2);\n",
        "  }\n",
        "\n",
        "  // step4: Broadcast block_s1/block_s2\n",
        "  __shared__ float sh_s1, sh_s2;\n",
        "  if (tid == 0) {\n",
        "    sh_s1 = block_s1;\n",
        "    sh_s2 = block_s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float S1 = sh_s1;\n",
        "  float S2 = sh_s2;\n",
        "\n",
        "  //step5: Write dx\n",
        "\n",
        "  float invD = 1.0f / (float)D;\n",
        "\n",
        "  for (int i = tid; i < D; i += blockDim.x) {\n",
        "    float xi  = xrow[i];\n",
        "    float dyi = dyrow[i];\n",
        "    float gi  = dyi * gamma[i];\n",
        "    float xhat = (xi - mu) * inv;\n",
        "\n",
        "    float dx_i = inv * (gi - S1 * invD - xhat * (S2 * invD));\n",
        "    dxrow[i] = dx_i;\n",
        "    atomicAdd(&dbeta[i],  dyi);\n",
        "    atomicAdd(&dgamma[i], dyi * xhat);\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "void ln_forward_cuda_launcher(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta,\n",
        "                              torch::Tensor y, torch::Tensor mean, torch::Tensor inv_std,\n",
        "                              double eps) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(beta);  CHECK_CONTIG(beta);  CHECK_F32(beta);\n",
        "    CHECK_CUDA(y);     CHECK_CONTIG(y);     CHECK_F32(y);\n",
        "    CHECK_CUDA(mean);  CHECK_CONTIG(mean);  CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "\n",
        "    int f_threads = 256;\n",
        "    if (D <= 128) f_threads = 128;\n",
        "    f_threads = round_up_warp(f_threads);\n",
        "\n",
        "    dim3 f_block(f_threads, 1, 1);\n",
        "    dim3 f_grid(B, 1, 1);\n",
        "\n",
        "    //dim3 block(0,0,1); // TODO\n",
        "    //dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "    ln_forward_kernel<<<f_grid, f_block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(),\n",
        "        y.data_ptr<float>(), mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        B, D, (float)eps\n",
        "    );\n",
        "}\n",
        "\n",
        "void ln_backward_cuda_launcher(torch::Tensor x, torch::Tensor gamma,\n",
        "                               torch::Tensor mean, torch::Tensor inv_std,\n",
        "                               torch::Tensor dout,\n",
        "                               torch::Tensor dx, torch::Tensor dgamma, torch::Tensor dbeta) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F32(x);\n",
        "    CHECK_CUDA(gamma); CHECK_CONTIG(gamma); CHECK_F32(gamma);\n",
        "    CHECK_CUDA(mean); CHECK_CONTIG(mean); CHECK_F32(mean);\n",
        "    CHECK_CUDA(inv_std); CHECK_CONTIG(inv_std); CHECK_F32(inv_std);\n",
        "    CHECK_CUDA(dout); CHECK_CONTIG(dout); CHECK_F32(dout);\n",
        "    CHECK_CUDA(dx); CHECK_CONTIG(dx); CHECK_F32(dx);\n",
        "    CHECK_CUDA(dgamma); CHECK_CONTIG(dgamma); CHECK_F32(dgamma);\n",
        "    CHECK_CUDA(dbeta); CHECK_CONTIG(dbeta); CHECK_F32(dbeta);\n",
        "\n",
        "    int B = (int)x.size(0);\n",
        "    int D = (int)x.size(1);\n",
        "\n",
        "    //dim3 block(0,0,1); // TODO\n",
        "    //dim3 grid(0,0,1);  // TODO\n",
        "    auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "    int b_threads = 256;\n",
        "    if (D <= 128) b_threads = 128;\n",
        "    b_threads = round_up_warp(b_threads);\n",
        "\n",
        "    dim3 b_block(b_threads, 1, 1);\n",
        "    dim3 b_grid(B, 1, 1);\n",
        "\n",
        "    ln_backward_kernel<<<b_grid, b_block>>>(\n",
        "        x.data_ptr<float>(), gamma.data_ptr<float>(),\n",
        "        mean.data_ptr<float>(), inv_std.data_ptr<float>(),\n",
        "        dout.data_ptr<float>(),\n",
        "        dx.data_ptr<float>(), dgamma.data_ptr<float>(), dbeta.data_ptr<float>(),\n",
        "        B, D\n",
        "    );\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "ext = load(\n",
        "    name=\"ln_ext_fwd_bwd\",\n",
        "    sources=[\"ln_ext/ext.cpp\", \"ln_ext/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Step2 extension loaded:\", ext)\n",
        "\n",
        "# Optional: gradient check harness (disabled until TODOs are implemented)\n",
        "RUN_GRAD_TEST = True\n",
        "if RUN_GRAD_TEST:\n",
        "    B, D = 8, 256\n",
        "    eps = 1e-5\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "    beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # TODO: compare to torch.nn.functional.layer_norm gradients\n",
        "    # - call ext.forward -> (y, mean, inv_std)\n",
        "    # - build dout\n",
        "    # - call ext.backward -> (dx, dgamma, dbeta)\n",
        "    # - compare to autograd reference\n",
        "\n",
        "    # Step1: Extension forward -> (y, mean, inv_std)\n",
        "    y_ext, mean_ext, inv_std_ext = ext.forward(x, gamma, beta, eps)\n",
        "\n",
        "    # Build upstream gradient dout (same shape as y)\n",
        "    dout = torch.randn_like(y_ext)\n",
        "\n",
        "    # Extension backward -> (dx, dgamma, dbeta)\n",
        "    dx_ext, dgamma_ext, dbeta_ext = ext.backward(x, gamma, mean_ext, inv_std_ext, dout)\n",
        "\n",
        "    # Step2: Autograd reference using torch.layer_norm, IMPORTANT: use the same eps and weight/bias\n",
        "    # Make separate tensors so gradients don't mix with ext path\n",
        "    x_ref = x.detach().clone().requires_grad_(True)\n",
        "    gamma_ref = gamma.detach().clone().requires_grad_(True)\n",
        "    beta_ref  = beta.detach().clone().requires_grad_(True)\n",
        "\n",
        "    y_ref = F.layer_norm(x_ref, normalized_shape=(D,), weight=gamma_ref, bias=beta_ref, eps=eps)\n",
        "\n",
        "    # Backprop with the same dout\n",
        "    y_ref.backward(dout)\n",
        "\n",
        "    dx_ref = x_ref.grad\n",
        "    dgamma_ref = gamma_ref.grad\n",
        "    dbeta_ref = beta_ref.grad\n",
        "\n",
        "    # step3: Compare\n",
        "    def report(name, a, b, atol=1e-4, rtol=1e-3):\n",
        "        diff = (a - b).abs()\n",
        "        max_abs = diff.max().item()\n",
        "        max_rel = (diff / b.abs().clamp_min(1e-12)).max().item()\n",
        "        ok = torch.allclose(a, b, atol=atol, rtol=rtol)\n",
        "        print(f\"{name}: allclose={ok}  max_abs={max_abs:.6e}  max_rel={max_rel:.6e}  \"\n",
        "              f\"(atol={atol}, rtol={rtol})\")\n",
        "        return ok\n",
        "\n",
        "    print(\"[grad test] comparing ext vs torch.autograd reference\")\n",
        "    ok_dx = report(\"dx\", dx_ext, dx_ref, atol=1e-3, rtol=1e-3)\n",
        "    ok_dg = report(\"dgamma\", dgamma_ext, dgamma_ref, atol=1e-3, rtol=1e-3)\n",
        "    ok_db = report(\"dbeta\", dbeta_ext, dbeta_ref, atol=1e-3, rtol=1e-3)\n",
        "\n",
        "    if not (ok_dx and ok_dg and ok_db):\n",
        "        # Print a few worst indices for debugging\n",
        "        def worst_idx(a, b):\n",
        "            diff = (a - b).abs().reshape(-1)\n",
        "            idx = diff.argmax().item()\n",
        "            return idx, diff[idx].item()\n",
        "\n",
        "        idx, val = worst_idx(dx_ext, dx_ref)\n",
        "        print(f\"worst dx idx(flat)={idx}, abs_diff={val:.6e}\")\n",
        "        idx, val = worst_idx(dgamma_ext, dgamma_ref)\n",
        "        print(f\"worst dgamma idx(flat)={idx}, abs_diff={val:.6e}\")\n",
        "        idx, val = worst_idx(dbeta_ext, dbeta_ref)\n",
        "        print(f\"worst dbeta idx(flat)={idx}, abs_diff={val:.6e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W481Iav6lkG"
      },
      "source": [
        "## âœ… Day 4 â€” Compile / debug / edge cases (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRLEynmF6lkG"
      },
      "outputs": [],
      "source": [
        "# Day 4 (ONE CELL): edge case test scaffolding + debug aids (NO SOLUTION)\n",
        "\n",
        "import torch, math\n",
        "\n",
        "# Edge cases to test (you can expand)\n",
        "CASES = [\n",
        "    (1, 7),       # tiny D\n",
        "    (2, 33),      # not multiple of warp\n",
        "    (4, 128),\n",
        "    (16, 1024),\n",
        "    (3, 4096),    # large D\n",
        "]\n",
        "\n",
        "# Toggle when your kernels are implemented\n",
        "RUN_EDGE_TESTS = False\n",
        "\n",
        "def run_edge_suite(ext):\n",
        "    for (B, D) in CASES:\n",
        "        x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "        gamma = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "        beta  = torch.randn(D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "        # TODO: call ext.forward and validate shape/dtype/device\n",
        "        # y, mean, inv = ext.forward(x, gamma, beta, 1e-5)\n",
        "        # assert y.shape == x.shape\n",
        "        # assert mean.shape == (B,)\n",
        "        # assert inv.shape == (B,)\n",
        "\n",
        "        # TODO: check numerical sanity (no NaN/Inf)\n",
        "        # assert torch.isfinite(y).all()\n",
        "\n",
        "        # TODO: backward sanity\n",
        "        # dout = torch.randn_like(x)\n",
        "        # dx, dgamma, dbeta = ext.backward(x, gamma, mean, inv, dout)\n",
        "\n",
        "        print(f\"[EdgeCase] B={B} D={D} -> TODO checks\")\n",
        "\n",
        "# If you already loaded Day3 extension as `ext`, you can run:\n",
        "if RUN_EDGE_TESTS:\n",
        "    run_edge_suite(ext)\n",
        "else:\n",
        "    print(\"Day4: Edge suite is ready. Set RUN_EDGE_TESTS=True after implementing kernels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34tExjB96lkG"
      },
      "source": [
        "## âœ… Day 5 â€” Fused GELU + Bias CUDA kernel (extension skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7HHJO6W6lkG"
      },
      "outputs": [],
      "source": [
        "# Day 6 (ONE CELL): fused Bias+GELU extension skeleton (NO SOLUTION)\n",
        "\n",
        "import os, torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "os.makedirs(\"fused_gelu\", exist_ok=True)\n",
        "\n",
        "open(\"fused_gelu/ext.cpp\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_F16F32(x) TORCH_CHECK((x).scalar_type()==at::kHalf || (x).scalar_type()==at::kFloat, #x \" must be fp16 or fp32\")\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y);\n",
        "\n",
        "torch::Tensor fused_gelu_bias(torch::Tensor x, torch::Tensor bias) {\n",
        "    // TODO:\n",
        "    // - checks (CUDA/contig/dtype/shape)\n",
        "    // - allocate y\n",
        "    // - call launcher\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x); CHECK_F16F32(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias); CHECK_F16F32(bias);\n",
        "\n",
        "    auto y = torch::empty_like(x);\n",
        "    fused_gelu_bias_cuda_launcher(x, bias, y);\n",
        "    return y;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &fused_gelu_bias, \"Fused Bias+GELU forward (CUDA, skeleton)\");\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "open(\"fused_gelu/ext_cuda.cu\", \"w\").write(r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be CUDA\")\n",
        "#define CHECK_CONTIG(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "\n",
        "// TODO: implement GELU approximation or exact (no solution here)\n",
        "// kernel: y = GELU(x + bias)\n",
        "\n",
        "__global__ void fused_bias_gelu_kernel(/* TODO args */) {\n",
        "    // TODO\n",
        "}\n",
        "\n",
        "void fused_gelu_bias_cuda_launcher(torch::Tensor x, torch::Tensor bias, torch::Tensor y) {\n",
        "    CHECK_CUDA(x); CHECK_CONTIG(x);\n",
        "    CHECK_CUDA(bias); CHECK_CONTIG(bias);\n",
        "    CHECK_CUDA(y); CHECK_CONTIG(y);\n",
        "\n",
        "    // TODO: grid/block\n",
        "    dim3 block(0,0,1);\n",
        "    dim3 grid(0,0,1);\n",
        "\n",
        "    // TODO: dispatch by dtype (fp16/fp32)\n",
        "    // fused_bias_gelu_kernel<<<grid, block>>>(...);\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "fused = load(\n",
        "    name=\"fused_gelu_bias_ext\",\n",
        "    sources=[\"fused_gelu/ext.cpp\", \"fused_gelu/ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Day6 extension loaded:\", fused)\n",
        "\n",
        "RUN_TEST = False\n",
        "if RUN_TEST:\n",
        "    # TODO: compare vs torch.nn.functional.gelu(x + bias)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FO5iCX6lkH"
      },
      "source": [
        "## âœ… Day 7 â€” Weekly project packaging (full LN extension) + scripts (skeleton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJWwSsM36lkH"
      },
      "outputs": [],
      "source": [
        "# Day 7 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Day7 packaging skeleton created under ./project_ln/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiY-3BfN6lkH"
      },
      "outputs": [],
      "source": [
        "# Day 7 (ONE CELL): project packaging skeleton (NO SOLUTION)\n",
        "# Creates placeholders for README, tests, benchmark, and Nsight Compute script.\n",
        "\n",
        "import os, textwrap\n",
        "\n",
        "os.makedirs(\"project_ln\", exist_ok=True)\n",
        "\n",
        "open(\"project_ln/README.md\", \"w\").write(r\"\"\"\n",
        "# PyTorch LayerNorm C++/CUDA Extension (Skeleton)\n",
        "\n",
        "## What you should have by end of Week\n",
        "- LN forward (CUDA)\n",
        "- LN backward (CUDA)\n",
        "- Python API: forward/backward or autograd wrapper\n",
        "- Correctness tests vs PyTorch\n",
        "- Benchmarks vs torch.nn.LayerNorm\n",
        "- Nsight Compute profiling commands\n",
        "\n",
        "## TODO\n",
        "- Document build steps (Colab and local)\n",
        "- Add usage examples\n",
        "- Add performance notes and profiling screenshots\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/test_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch\n",
        "\n",
        "def test_forward(ext):\n",
        "    # TODO: compare ext.forward vs torch layer_norm\n",
        "    pass\n",
        "\n",
        "def test_backward(ext):\n",
        "    # TODO: compare gradients vs autograd\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: import your built extension module and run tests\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/bench_ln.py\", \"w\").write(r\"\"\"\n",
        "import torch, time\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_fn(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def main():\n",
        "    # TODO: load ext\n",
        "    # TODO: build benchmark cases\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "open(\"project_ln/run_ncu.sh\", \"w\").write(r\"\"\"#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# TODO:\n",
        "# - Build your extension (if building outside Colab JIT)\n",
        "# - Run Nsight Compute on forward/backward kernels\n",
        "\n",
        "# Example:\n",
        "# ncu --set full --kernel-name \"ln_forward_kernel\" -o ncu_ln_fwd python project_ln/bench_ln.py\n",
        "# ncu --set full --kernel-name \"ln_backward_kernel\" -o ncu_ln_bwd python project_ln/bench_ln.py\n",
        "\n",
        "echo \"Edit this script with your kernel names and driver script.\"\n",
        "\"\"\")\n",
        "\n",
        "os.system(\"chmod +x project_ln/run_ncu.sh\")\n",
        "print(\"Day7 packaging skeleton created under ./project_ln/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}