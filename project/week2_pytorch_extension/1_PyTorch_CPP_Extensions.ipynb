{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# üìò Task Explanation: PyTorch C++ Extensions ‚Äî TensorAccessor & ATen API\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to understand the **foundations of PyTorch C++ extensions** and learn how to write **custom C++/CUDA operators** that integrate seamlessly with PyTorch.\n",
        "\n",
        "This task focuses on:\n",
        "- How PyTorch exposes tensors to C++/CUDA\n",
        "- How to safely and efficiently access tensor data\n",
        "- How custom operators interact with PyTorch‚Äôs autograd system\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: Why PyTorch C++ Extensions?\n",
        "While Python is ideal for model definition and experimentation, **performance-critical operators** (e.g., LayerNorm, Softmax, fused kernels) are often implemented in **C++/CUDA**.\n",
        "\n",
        "PyTorch C++ extensions allow you to:\n",
        "- Write custom high-performance kernels\n",
        "- Call them directly from Python\n",
        "- Register forward and backward functions\n",
        "- Participate fully in PyTorch‚Äôs autograd system\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî PyTorch C++ Extension Basics\n",
        "\n",
        "### Task\n",
        "Learn the basic structure of a PyTorch C++ extension and how it is built and loaded.\n",
        "\n",
        "You should understand:\n",
        "- How to create a C++ extension using `torch.utils.cpp_extension`\n",
        "- How Python code loads compiled shared libraries\n",
        "- The role of `PYBIND11_MODULE` in binding C++ functions to Python\n",
        "- The difference between:\n",
        "  - Pure C++ extensions\n",
        "  - C++ + CUDA extensions\n",
        "\n",
        "### Key Concepts\n",
        "- `setup.py` or `load()` workflow\n",
        "- CMake / NVCC integration\n",
        "- ABI compatibility with PyTorch\n",
        "- CPU vs CUDA dispatch\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî TensorAccessor\n",
        "\n",
        "### What Is TensorAccessor?\n",
        "`TensorAccessor` is a lightweight wrapper that provides **type-safe and bounds-aware access** to tensor data inside CUDA kernels.\n",
        "\n",
        "It allows you to:\n",
        "- Index tensors using `tensor[i][j]` syntax\n",
        "- Avoid manual pointer arithmetic\n",
        "- Improve code readability and safety\n",
        "\n",
        "### Task\n",
        "Learn how to:\n",
        "- Convert a PyTorch tensor to a `TensorAccessor`\n",
        "- Use `TensorAccessor` inside CUDA kernels\n",
        "- Understand layout assumptions (contiguous, strides)\n",
        "\n",
        "### Key Considerations\n",
        "- Tensor must be contiguous (or you must handle strides explicitly)\n",
        "- Access patterns affect memory coalescing\n",
        "- TensorAccessor does not perform automatic bounds checking on device\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî ATen API\n",
        "\n",
        "### What Is ATen?\n",
        "**ATen** is PyTorch‚Äôs core C++ tensor library.  \n",
        "It provides:\n",
        "- Tensor creation and manipulation\n",
        "- Device and dtype abstraction\n",
        "- Dispatch to CPU or CUDA implementations\n",
        "\n",
        "### Task\n",
        "Learn how to:\n",
        "- Use `at::Tensor` in C++ code\n",
        "- Access tensor metadata (shape, dtype, device)\n",
        "- Launch CUDA kernels using ATen utilities\n",
        "- Write device-agnostic code where possible\n",
        "\n",
        "### Common ATen Operations\n",
        "- `tensor.data_ptr<T>()`\n",
        "- `tensor.size(dim)`\n",
        "- `tensor.stride(dim)`\n",
        "- `at::zeros_like`, `at::empty`\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- How does PyTorch pass tensors from Python to C++?\n",
        "- What are the differences between `data_ptr` and `TensorAccessor`?\n",
        "- When should you prefer ATen APIs over raw CUDA code?\n",
        "- How does PyTorch ensure correct device and dtype dispatch?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A minimal PyTorch C++ extension that can be imported in Python\n",
        "2. A C++ function that:\n",
        "   - Accepts `at::Tensor` inputs\n",
        "   - Accesses tensor data using `TensorAccessor`\n",
        "3. A basic CUDA kernel launched via ATen\n",
        "4. A short write-up explaining:\n",
        "   - Data flow from Python ‚Üí C++ ‚Üí CUDA\n",
        "   - Why this approach is used in real ML systems\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How PyTorch integrates Python, C++, and CUDA\n",
        "- How tensors are represented internally\n",
        "- How high-performance ML operators are built\n",
        "- How to extend PyTorch beyond Python\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "PyTorch C++ extensions are used in:\n",
        "- Custom fused operators\n",
        "- FlashAttention and fused LN kernels\n",
        "- High-performance training and inference backends\n",
        "\n",
        "Mastering TensorAccessor and ATen is a **key step toward ML Systems and CUDA kernel engineering roles**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaway\n",
        "> **PyTorch C++ extensions bridge the gap between Python productivity and C++/CUDA performance, enabling production-grade ML operators.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "3d3b09e3-100f-4842-9e32-14b2dae86ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Jan 25 12:24:44 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyuqf4e1iZBl",
        "outputId": "9b9b9a43-78e3-4a66-bf1b-c03158c3eca8"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTkXOOjUHMhX"
      },
      "source": [
        "# üß© Task: PyTorch C++/CUDA Extension on Colab\n",
        "\n",
        "## üéØ Goal\n",
        "In this task, you will build a **minimal PyTorch C++/CUDA extension** directly in **Google Colab** that:\n",
        "\n",
        "- Compiles C++ + CUDA code using PyTorch utilities\n",
        "- Uses **ATen API** in C++\n",
        "- Uses **TensorAccessor** inside a CUDA kernel\n",
        "- Is callable from Python\n",
        "\n",
        "‚ö†Ô∏è This is a **skeleton only**. You must fill in all TODO sections.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Environment Assumptions\n",
        "- Google Colab with **GPU enabled**\n",
        "- CUDA already available via PyTorch\n",
        "- No local files required (everything written via `%%writefile`)\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Step 1 ‚Äî Create C++ Interface (ext.h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnGkyLiaCIfU",
        "outputId": "00a5973c-eb04-4ca0-97cd-b7ac90d3ba65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ext.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile ext.h\n",
        "#pragma once\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// C++ forward declaration\n",
        "torch::Tensor my_op_forward(torch::Tensor input);\n",
        "\n",
        "// CUDA launcher declaration\n",
        "void my_op_cuda_launcher(torch::Tensor input, torch::Tensor output);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMsW5yJrHMhX"
      },
      "source": [
        "## üß± Step 2 ‚Äî C++ Wrapper with ATen (ext.cpp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3p85_jhHMhX",
        "outputId": "cab1116f-5161-44e6-a4d0-5318e09a7959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ext.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile ext.cpp\n",
        "#include <torch/extension.h>\n",
        "#include \"ext.h\"\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: ATen wrapper\n",
        "// Requirements:\n",
        "//  - Validate input device (CUDA), dtype (float32), contiguous, 2D\n",
        "//  - Allocate output tensor with same shape/device/dtype\n",
        "//  - Call CUDA launcher\n",
        "//  - Return output\n",
        "// ------------------------------------------------------------\n",
        "torch::Tensor my_op_forward(torch::Tensor input) {\n",
        "    // TODO: remove early return\n",
        "    // return torch::Tensor();\n",
        "\n",
        "    //Device check\n",
        "    TORCH_CHECK(input.is_cuda(), \"my_op_forward: input must be a CUDA tensor\");\n",
        "\n",
        "    // Dtype check (float32)\n",
        "    TORCH_CHECK(input.scalar_type() == at::kFloat,\n",
        "                \"my_op_forward: input must be float32 (torch.float32)\");\n",
        "\n",
        "    // Layout / contiguity check\n",
        "    TORCH_CHECK(input.is_contiguous(),\n",
        "                \"my_op_forward: input must be contiguous (call .contiguous())\");\n",
        "\n",
        "    // Shape check: 2D\n",
        "    TORCH_CHECK(input.dim() == 2,\n",
        "                \"my_op_forward: input must be 2D, got dim=\", input.dim());\n",
        "\n",
        "    // Allocate output (same shape/device/dtype)\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    // Launch CUDA kernel (implemented in .cu)\n",
        "    my_op_cuda_launcher(input, output);\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// PyBind\n",
        "// ------------------------------------------------------------\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &my_op_forward, \"MyOp forward (CUDA)\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdBMpNhrHMhY"
      },
      "source": [
        "## üß± Step 3 ‚Äî Write CUDA Kernel + TensorAccessor (ext_cuda.cu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpYkpNPsHMhY",
        "outputId": "7289023c-2880-4440-90d4-be57906120d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ext_cuda.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ext_cuda.cu\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK((x).is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK((x).is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_FLOAT(x) TORCH_CHECK((x).scalar_type() == at::ScalarType::Float, #x \" must be float32\")\n",
        "#define CHECK_2D(x) TORCH_CHECK((x).dim() == 2, #x \" must be 2D\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x); CHECK_FLOAT(x); CHECK_2D(x)\n",
        "\n",
        "// Macro for checking CUDA errors\n",
        "#define CUDA_CHECK(call)                                                          \\\n",
        "  do {                                                                            \\\n",
        "    cudaError_t cudaStatus = call;                                                \\\n",
        "    if (cudaStatus != cudaSuccess) {                                              \\\n",
        "      fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(cudaStatus), \\\n",
        "              __FILE__, __LINE__);                                                \\\n",
        "      throw std::runtime_error(cudaGetErrorString(cudaStatus));                   \\\n",
        "    }                                                                             \\\n",
        "  } while (0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: CUDA kernel using TensorAccessor\n",
        "// Input/Output shape: [B, D]\n",
        "// Task: elementwise transform y[b,d] = f(x[b,d]) (you choose f)\n",
        "// Requirements:\n",
        "//  - Use TensorAccessor (PackedTensorAccessor32)\n",
        "//  - Correct indexing (b,d)\n",
        "//  - Bounds checks\n",
        "// ------------------------------------------------------------\n",
        "__global__ void my_kernel(\n",
        "    torch::PackedTensorAccessor32<float, 2, torch::RestrictPtrTraits> x,\n",
        "    torch::PackedTensorAccessor32<float, 2, torch::RestrictPtrTraits> y,\n",
        "    int B, int D\n",
        ") {\n",
        "    // TODO:\n",
        "    // - compute b, d from blockIdx/threadIdx\n",
        "    // - if (b < B && d < D) { y[b][d] = ...; }\n",
        "\n",
        "    // Map: threadIdx.x -> d, threadIdx.y -> b within a tile\n",
        "    const int d = static_cast<int>(blockIdx.x) * static_cast<int>(blockDim.x) +\n",
        "                  static_cast<int>(threadIdx.x);\n",
        "    const int b = static_cast<int>(blockIdx.y) * static_cast<int>(blockDim.y) +\n",
        "                  static_cast<int>(threadIdx.y);\n",
        "\n",
        "    if (b < B && d < D) {\n",
        "        const float v = x[b][d];\n",
        "        y[b][d] = v * v;  // f(v)\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: CUDA launcher\n",
        "// Requirements:\n",
        "//  - Validate tensors\n",
        "//  - Choose block/grid\n",
        "//  - Create accessors from tensors\n",
        "//  - Launch kernel\n",
        "// ------------------------------------------------------------\n",
        "void my_op_cuda_launcher(torch::Tensor input, torch::Tensor output) {\n",
        "    CHECK_INPUT(input);\n",
        "    CHECK_INPUT(output);\n",
        "\n",
        "    // TODO: get B, D from input sizes\n",
        "    // int B = ...\n",
        "    // int D = ...\n",
        "    // Input/Output must be [B, D]\n",
        "    TORCH_CHECK(input.dim() == 2, \"my_op_cuda_launcher: input must be 2D\");\n",
        "    TORCH_CHECK(output.dim() == 2, \"my_op_cuda_launcher: output must be 2D\");\n",
        "    TORCH_CHECK(input.size(0) == output.size(0) && input.size(1) == output.size(1),\n",
        "                \"my_op_cuda_launcher: input/output shapes must match\");\n",
        "\n",
        "    const int B = static_cast<int>(input.size(0));\n",
        "    const int D = static_cast<int>(input.size(1));\n",
        "\n",
        "\n",
        "    // TODO: choose dim3 block, grid\n",
        "    // dim3 block(...);\n",
        "    // dim3 grid(...);\n",
        "    // 2D tile for (b, d)\n",
        "    // - x dimension covers D\n",
        "    // - y dimension covers B\n",
        "    constexpr int TX = 32;  // columns\n",
        "    constexpr int TY = 8;   // rows\n",
        "    dim3 block(TX, TY);\n",
        "    dim3 grid((D + TX - 1) / TX, (B + TY - 1) / TY);\n",
        "\n",
        "    // Create accessors (PackedTensorAccessor32)\n",
        "    auto x_acc = input.packed_accessor32<float, 2, torch::RestrictPtrTraits>();\n",
        "    auto y_acc = output.packed_accessor32<float, 2, torch::RestrictPtrTraits>();\n",
        "\n",
        "    // TODO: launch kernel with accessors\n",
        "    // my_kernel<<<grid, block>>>(..., ..., B, D);\n",
        "    // Launch\n",
        "    my_kernel<<<grid, block>>>(x_acc, y_acc, B, D);\n",
        "\n",
        "    // Optional but strongly recommended for catching launch errors early\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BPtQSp4HMhY"
      },
      "source": [
        "##  Step 4 ‚Äî Colab Cell 4 ‚Äî Build Extension (JIT Compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsiUVgetLVCB",
        "outputId": "45bd0d61-df20-43e7-daa7-b347ea6cb6e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX8kHNCUHMhY",
        "outputId": "6b2d810b-da91-47e9-a6b3-eb073edc322c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extension loaded: <module 'tensor_accessor_ext_v1' from '/root/.cache/torch_extensions/py312_cu126/tensor_accessor_ext/tensor_accessor_ext_v1.so'>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "# TIP: set verbose=True if compilation issues\n",
        "ext = load(\n",
        "    name=\"tensor_accessor_ext\",\n",
        "    sources=[\"ext.cpp\", \"ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"Extension loaded:\", ext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_js9-JMQHMhY"
      },
      "source": [
        "##  Step 5 ‚Äî CPU Reference + Test Harness (Correctness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jna8iPQnHMhY",
        "outputId": "62eca759-5bcc-4a02-a999-16baa83aea75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[correctness] B=256, D=1024, atol=1e-05, rtol=0.0001\n",
            "  max_abs_err = 0.000000e+00\n",
            "  max_rel_err = 0.000000e+00\n",
            "  allclose    = True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: CPU reference (must match your CUDA kernel's f(x))\n",
        "# Requirements:\n",
        "#  - input: x [B, D] on CPU\n",
        "#  - output: y [B, D] on CPU\n",
        "# ------------------------------------------------------------\n",
        "def my_op_cpu_reference(x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: implement same math as CUDA kernel\n",
        "    assert x.device.type == \"cpu\", \"CPU reference expects a CPU tensor\"\n",
        "    assert x.dtype == torch.float32, \"CPU reference expects float32\"\n",
        "    assert x.dim() == 2, \"CPU reference expects 2D [B, D]\"\n",
        "    return x * x  # f(x)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: correctness check\n",
        "# Requirements:\n",
        "#  - create test tensor on CUDA\n",
        "#  - run ext.forward\n",
        "#  - compare with CPU reference (move tensors appropriately)\n",
        "#  - print max error\n",
        "# ------------------------------------------------------------\n",
        "def test_correctness(B=256, D=1024, atol=1e-5, rtol=1e-4):\n",
        "    # TODO\n",
        "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
        "\n",
        "    # Create input on CUDA\n",
        "    x_cuda = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "    # Run extension forward (CUDA)\n",
        "    y_cuda = ext.forward(x_cuda)\n",
        "\n",
        "    # CPU reference: move input to CPU, compute, then compare on CPU\n",
        "    x_cpu = x_cuda.detach().cpu()\n",
        "    y_ref_cpu = my_op_cpu_reference(x_cpu)\n",
        "\n",
        "    y_cuda_cpu = y_cuda.detach().cpu()\n",
        "    diff = (y_cuda_cpu - y_ref_cpu).abs()\n",
        "    max_err = diff.max().item()\n",
        "\n",
        "    # Relative error (avoid div by 0)\n",
        "    denom = y_ref_cpu.abs().clamp_min(1e-12)\n",
        "    rel = (diff / denom).max().item()\n",
        "\n",
        "    ok = torch.allclose(y_cuda_cpu, y_ref_cpu, atol=atol, rtol=rtol)\n",
        "\n",
        "    print(f\"[correctness] B={B}, D={D}, atol={atol}, rtol={rtol}\")\n",
        "    print(f\"  max_abs_err = {max_err:.6e}\")\n",
        "    print(f\"  max_rel_err = {rel:.6e}\")\n",
        "    print(f\"  allclose    = {ok}\")\n",
        "\n",
        "    if not ok:\n",
        "        # helpful extra info\n",
        "        idx = diff.argmax().item()\n",
        "        b = idx // D\n",
        "        d = idx % D\n",
        "        print(f\"  worst at (b={b}, d={d}): y_cuda={y_cuda_cpu[b,d].item():.6e}, \"\n",
        "              f\"y_ref={y_ref_cpu[b,d].item():.6e}, diff={diff[b,d].item():.6e}\")\n",
        "\n",
        "test_correctness()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qqq9RgJHMhZ"
      },
      "source": [
        "##  Step 6 ‚Äî Benchmark (CUDA Events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL43ss9zHMhZ",
        "outputId": "02d568da-d268-447b-e67c-233abd131224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[benchmark] B=4096, D=1024, iters=200, warmup=20\n",
            "  total   = 31.330 ms\n",
            "  avg/iter= 0.156652 ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TODO: benchmark\n",
        "# Requirements:\n",
        "#  - time ext.forward(x) with CUDA events\n",
        "#  - include warmup\n",
        "#  - print average ms\n",
        "# ------------------------------------------------------------\n",
        "def benchmark(B=4096, D=1024, iters=200, warmup=20):\n",
        "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
        "\n",
        "    x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "    # Warmup (important for CUDA context, caching, JIT, etc.)\n",
        "    for _ in range(warmup):\n",
        "        y = ext.forward(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # CUDA events timing\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        y = ext.forward(x)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    total_ms = start.elapsed_time(end)\n",
        "    avg_ms = total_ms / iters\n",
        "\n",
        "    print(f\"[benchmark] B={B}, D={D}, iters={iters}, warmup={warmup}\")\n",
        "    print(f\"  total   = {total_ms:.3f} ms\")\n",
        "    print(f\"  avg/iter= {avg_ms:.6f} ms\")\n",
        "\n",
        "benchmark()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnBwCa4nHMhZ"
      },
      "source": [
        "##  Step 7 ‚Äî Nsight Compute Profiling Script (Colab-Friendly Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7X7CBHJHMhZ",
        "outputId": "a451ada5-4457-4962-854a-f82b114a9834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting run_ncu_ext.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_ncu_ext.sh\n",
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Nsight Compute profiling helper for the PyTorch C++/CUDA extension\n",
        "#\n",
        "# Notes:\n",
        "#  - Build is handled by Python JIT (torch.utils.cpp_extension.load)\n",
        "#  - This script assumes Nsight Compute (ncu) is available in PATH\n",
        "#  - You must ensure the Python workload actually launches `my_kernel`\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# -----------------------\n",
        "# User-adjustable params\n",
        "# -----------------------\n",
        "KERNEL_NAME=\"my_kernel\"        # must exactly match the CUDA kernel symbol\n",
        "OUT=\"ncu_report\"               # output base name (ncu_report.ncu-rep)\n",
        "DRIVER=\"driver.py\"             # Python script that calls ext.forward(...)\n",
        "SET=\"full\"                     # or: speedOfLight, memoryWorkloadAnalysis, etc.\n",
        "\n",
        "# -----------------------\n",
        "# Sanity checks\n",
        "# -----------------------\n",
        "if ! command -v ncu &>/dev/null; then\n",
        "  echo \"[WARN] Nsight Compute (ncu) not found in PATH.\"\n",
        "  echo \"       If you're on Colab, ncu is usually NOT available.\"\n",
        "  echo \"       Run this script on a local machine or cloud VM with Nsight Compute installed.\"\n",
        "  exit 0\n",
        "fi\n",
        "\n",
        "if [[ ! -f \"${DRIVER}\" ]]; then\n",
        "  echo \"[ERROR] ${DRIVER} not found.\"\n",
        "  echo \"Create a driver that imports the extension and calls ext.forward(x).\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "# -----------------------\n",
        "# Run profiling\n",
        "# -----------------------\n",
        "echo \"[INFO] Profiling kernel: ${KERNEL_NAME}\"\n",
        "echo \"[INFO] Driver: ${DRIVER}\"\n",
        "echo \"[INFO] Output: ${OUT}.ncu-rep\"\n",
        "\n",
        "# --kernel-name-base demangled is usually best for C++ kernels\n",
        "# --launch-skip/--launch-count help avoid profiling warmup launches\n",
        "ncu \\\n",
        "  --set \"${SET}\" \\\n",
        "  --kernel-name \"${KERNEL_NAME}\" \\\n",
        "  --kernel-name-base demangled \\\n",
        "  --launch-skip 1 \\\n",
        "  --launch-count 1 \\\n",
        "  -o \"${OUT}\" \\\n",
        "  python \"${DRIVER}\"\n",
        "\n",
        "echo \"[DONE] Nsight Compute report generated: ${OUT}.ncu-rep\"\n",
        "echo \"Open it with: ncu-ui ${OUT}.ncu-rep\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdhncMh3HMhZ",
        "outputId": "e0eb25f2-53b3-43e5-f88e-71c7e786584d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR] driver.py not found.\n",
            "Create a driver that imports the extension and calls ext.forward(x).\n"
          ]
        }
      ],
      "source": [
        "!chmod +x run_ncu_ext.sh\n",
        "!./run_ncu_ext.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61d2019d",
        "outputId": "61cef93b-42af-45ba-a2e0-aed8e8aec353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile driver.py\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Load the extension (must match parameters in cell pX8kHNCUHMhY)\n",
        "ext = load(\n",
        "    name=\"tensor_accessor_ext\",\n",
        "    sources=[\"ext.cpp\", \"ext_cuda.cu\"],\n",
        "    extra_cflags=[\"-O3\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=True # Set to True for verbose compilation output if needed\n",
        ")\n",
        "\n",
        "# Create a dummy tensor and call the forward pass to trigger the kernel\n",
        "# Use parameters that ensure the kernel is actually launched\n",
        "B, D = 256, 1024 # Example dimensions\n",
        "x = torch.randn(B, D, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "# Call the extension's forward method\n",
        "y = ext.forward(x)\n",
        "\n",
        "# Ensure CUDA operations are complete before exiting, especially for profiling\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"driver.py executed: ext.forward called successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea584f37"
      },
      "source": [
        "Now that `driver.py` has been created, you can run the Nsight Compute profiling script again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e496fdb",
        "outputId": "fcf51ef5-5784-4b73-ba06-88542e86a830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: ./run_ncu_ext.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./run_ncu_ext.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
