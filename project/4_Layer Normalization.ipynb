{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation: Layer Normalization (LN) CUDA Kernels  \n",
        "*(Forward with Warp-Level Reduction & Backward with Gradient Propagation)*\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "The objective of this task is to design **CUDA kernel skeletons (without solution)** for **Layer Normalization (LN)**, covering both:\n",
        "- **LN forward pass** using **warp-level reduction**\n",
        "- **LN backward pass** for **gradient propagation**\n",
        "\n",
        "This task focuses on **kernel structure, parallel decomposition, and data flow**, rather than providing a completed implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Background: What Is Layer Normalization?\n",
        "Layer Normalization is widely used in modern neural networks, especially in:\n",
        "- Transformers (LLMs)\n",
        "- RNNs and sequence models\n",
        "- MLP blocks\n",
        "\n",
        "For an input vector \\( x \\in \\mathbb{R}^D \\), LN computes:\n",
        "\\[\n",
        "\\mu = \\frac{1}{D}\\sum_{i=1}^{D} x_i, \\quad\n",
        "\\sigma^2 = \\frac{1}{D}\\sum_{i=1}^{D}(x_i - \\mu)^2\n",
        "\\]\n",
        "\\[\n",
        "y_i = \\gamma \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
        "\\]\n",
        "\n",
        "LN is **reduction-heavy** and **memory-sensitive**, making it an ideal case for:\n",
        "- Warp-level reductions\n",
        "- Register reuse\n",
        "- Careful kernel design\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part A â€” LN Forward Kernel (Warp-Level Reduction)\n",
        "\n",
        "### Task\n",
        "Design a CUDA kernel skeleton for the **LN forward pass**, where:\n",
        "- Each block (or warp) processes one LN row (one feature vector)\n",
        "- Mean and variance are computed using **warp-level reduction**\n",
        "- Normalization and affine transform (`gamma`, `beta`) are applied\n",
        "\n",
        "### Key Design Requirements\n",
        "- Use **warp-level primitives** (e.g., shuffle-based reduction)\n",
        "- Avoid block-wide synchronization when possible\n",
        "- Minimize global memory traffic\n",
        "- Structure the kernel so that:\n",
        "  - Reduction â†’ normalization â†’ write-back stages are clear\n",
        "\n",
        "### What You Should Think About\n",
        "- Mapping rows to warps or blocks\n",
        "- How partial sums are accumulated\n",
        "- Where registers vs shared memory would be used\n",
        "- Numerical stability considerations (but not implementation)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part B â€” LN Backward Kernel (Gradient Propagation)\n",
        "\n",
        "### Task\n",
        "Design a CUDA kernel skeleton for the **LN backward pass**, computing gradients for:\n",
        "- Input (`dx`)\n",
        "- Scale (`dgamma`)\n",
        "- Bias (`dbeta`)\n",
        "\n",
        "### Key Design Requirements\n",
        "- Clearly separate gradient paths:\n",
        "  - Gradient through normalization\n",
        "  - Gradient through mean and variance\n",
        "- Use parallel reduction patterns where needed\n",
        "- Ensure correct data dependencies between steps\n",
        "- Maintain a clean structure that mirrors the math of LN backward\n",
        "\n",
        "### What You Should Think About\n",
        "- Which gradients require reductions across the feature dimension\n",
        "- Which values from the forward pass must be reused\n",
        "- How to organize the kernel to avoid race conditions\n",
        "- Where warp-level vs block-level reduction might be required\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Deliverables\n",
        "You should produce:\n",
        "1. A **CUDA kernel skeleton** for LN forward (warp-level reduction)\n",
        "2. A **CUDA kernel skeleton** for LN backward (gradient propagation)\n",
        "3. Clearly marked `TODO` sections indicating:\n",
        "   - Reduction logic\n",
        "   - Memory access patterns\n",
        "   - Synchronization points (if any)\n",
        "\n",
        "> âš ï¸ No actual implementation logic is required â€” only the kernel structure.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How LN maps to GPU parallelism\n",
        "- Why warp-level reduction is well-suited for LN\n",
        "- How forward and backward passes differ in data flow\n",
        "- How real ML kernels are structured internally\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Relevance to ML Systems\n",
        "Layer Normalization is a **core operator** in:\n",
        "- Transformer blocks\n",
        "- FlashAttention pipelines\n",
        "- Fused LN + residual kernels\n",
        "- High-performance inference and training engines\n",
        "\n",
        "Being able to design LN kernel skeletons means you can:\n",
        "- Read and understand optimized kernels (e.g., in PyTorch, Triton)\n",
        "- Reason about performance bottlenecks\n",
        "- Communicate effectively as an **ML Systems / GPU Kernel Engineer**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Key Takeaway\n",
        "> **This task trains you to think in terms of GPU kernel structure and data flow for a real, production-critical ML operatorâ€”without relying on a finished implementation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "e10b272a-0af1-439d-a206-a36319bf4766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Mon Jan 19 15:53:40 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   32C    P0             58W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJj_KPxso5zD",
        "outputId": "c707c574-f1bd-4e55-d69d-fcc0e04ba1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ncu_stall_profile_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ln_warp_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cstring>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "static inline float frand01(unsigned int &state) {\n",
        "  state = 1664525u * state + 1013904223u;\n",
        "  return (state & 0x00FFFFFF) / float(0x01000000);\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference: LayerNorm forward\n",
        "// y = (x - mean) * inv_std * gamma + beta\n",
        "// saves mean and inv_std per row\n",
        "// ============================================================\n",
        "static void ln_forward_cpu(const float* x, const float* gamma, const float* beta,\n",
        "                           float* y, float* mean, float* inv_std,\n",
        "                           int B, int D, float eps) {\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    float m = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) m += xb[i];\n",
        "    m /= (float)D;\n",
        "\n",
        "    float v = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float d = xb[i] - m;\n",
        "      v += d * d;\n",
        "    }\n",
        "    v /= (float)D;\n",
        "\n",
        "    float inv = 1.0f / std::sqrt(v + eps);\n",
        "    mean[b] = m;\n",
        "    inv_std[b] = inv;\n",
        "\n",
        "    float* yb = y + b * D;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xn = (xb[i] - m) * inv;\n",
        "      yb[i] = xn * gamma[i] + beta[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference: LayerNorm backward\n",
        "// Given dout, x, gamma, mean, inv_std -> compute dx, dgamma, dbeta\n",
        "// NOTE: This is reference only. You can keep it as-is.\n",
        "// ============================================================\n",
        "static void ln_backward_cpu(const float* x, const float* gamma,\n",
        "                            const float* mean, const float* inv_std,\n",
        "                            const float* dout,\n",
        "                            float* dx, float* dgamma, float* dbeta,\n",
        "                            int B, int D) {\n",
        "  // dgamma, dbeta accumulation across batch\n",
        "  for (int i = 0; i < D; ++i) { dgamma[i] = 0.0f; dbeta[i] = 0.0f; }\n",
        "\n",
        "  // dx per element\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    const float* db = dout + b * D;\n",
        "    float* dxb = dx + b * D;\n",
        "\n",
        "    float m = mean[b];\n",
        "    float inv = inv_std[b];\n",
        "\n",
        "    // xhat, dY*gamma\n",
        "    // dx formula for LN (per-row):\n",
        "    // dx = (1/D) * inv * (D*dxhat - sum(dxhat) - xhat*sum(dxhat*xhat))\n",
        "    // where dxhat = dout * gamma, xhat = (x - mean) * inv\n",
        "    float sum_dxhat = 0.0f;\n",
        "    float sum_dxhat_xhat = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      float dxhat = db[i] * gamma[i];\n",
        "      sum_dxhat += dxhat;\n",
        "      sum_dxhat_xhat += dxhat * xhat;\n",
        "\n",
        "      dgamma[i] += db[i] * xhat;\n",
        "      dbeta[i]  += db[i];\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      float dxhat = db[i] * gamma[i];\n",
        "      float val = ( (float)D * dxhat - sum_dxhat - xhat * sum_dxhat_xhat );\n",
        "      dxb[i] = (inv / (float)D) * val;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Warp-level reduction helper using __shfl_down_sync\n",
        "// ============================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "  // TODO: implement with __shfl_down_sync\n",
        "  return v;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: LN forward CUDA kernel (warp reduce)\n",
        "// Suggested mapping:\n",
        "//  - One row per block or per warp (your choice)\n",
        "//  - Compute mean and inv_std using warp-level reductions\n",
        "//  - Write y and save mean/inv_std\n",
        "// ============================================================\n",
        "__global__ void ln_forward_warp(const float* __restrict__ x,\n",
        "                               const float* __restrict__ gamma,\n",
        "                               const float* __restrict__ beta,\n",
        "                               float* __restrict__ y,\n",
        "                               float* __restrict__ mean,\n",
        "                               float* __restrict__ inv_std,\n",
        "                               int B, int D, float eps) {\n",
        "  // TODO\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: LN backward CUDA kernel (gradient propagation)\n",
        "// Inputs: x, gamma, mean, inv_std, dout\n",
        "// Outputs: dx, dgamma, dbeta\n",
        "//\n",
        "// Notes:\n",
        "//  - dgamma/dbeta are reduced across batch (need atomics OR 2-pass reduction)\n",
        "//  - This skeleton does NOT prescribe a final strategy; you choose.\n",
        "// ============================================================\n",
        "__global__ void ln_backward_warp(const float* __restrict__ x,\n",
        "                                const float* __restrict__ gamma,\n",
        "                                const float* __restrict__ mean,\n",
        "                                const float* __restrict__ inv_std,\n",
        "                                const float* __restrict__ dout,\n",
        "                                float* __restrict__ dx,\n",
        "                                float* __restrict__ dgamma,\n",
        "                                float* __restrict__ dbeta,\n",
        "                                int B, int D) {\n",
        "  // TODO\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Utilities: correctness + benchmark\n",
        "// ============================================================\n",
        "static bool check_allclose(const float* a, const float* b, int n, float rtol, float atol,\n",
        "                           const char* name) {\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    float av = a[i], bv = b[i];\n",
        "    float diff = std::fabs(av - bv);\n",
        "    float tol = atol + rtol * std::fabs(bv);\n",
        "    if (diff > tol || std::isnan(av) || std::isnan(bv)) {\n",
        "      printf(\"Mismatch %s at %d: got=%g ref=%g diff=%g tol=%g\\n\", name, i, av, bv, diff, tol);\n",
        "      return false;\n",
        "    }\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "template <typename Kernel, typename... Args>\n",
        "static float bench_kernel_ms(Kernel k, dim3 grid, dim3 block,\n",
        "                             int warmup, int iters, Args... args) {\n",
        "  for (int i = 0; i < warmup; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < iters; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "  return ms / iters;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // Args: ./ln <B> <D> <iters>\n",
        "  int B = (argc > 1) ? std::atoi(argv[1]) : 4096;\n",
        "  int D = (argc > 2) ? std::atoi(argv[2]) : 1024;\n",
        "  int iters = (argc > 3) ? std::atoi(argv[3]) : 200;\n",
        "  float eps = 1e-5f;\n",
        "\n",
        "  printf(\"LN Skeleton: B=%d D=%d iters=%d\\n\", B, D, iters);\n",
        "\n",
        "  size_t bytes_x = (size_t)B * D * sizeof(float);\n",
        "  size_t bytes_d = (size_t)D * sizeof(float);\n",
        "  size_t bytes_b = (size_t)B * sizeof(float);\n",
        "\n",
        "  // Host buffers\n",
        "  float* hx = (float*)std::malloc(bytes_x);\n",
        "  float* hgamma = (float*)std::malloc(bytes_d);\n",
        "  float* hbeta  = (float*)std::malloc(bytes_d);\n",
        "  float* hy_ref = (float*)std::malloc(bytes_x);\n",
        "  float* hy_gpu = (float*)std::malloc(bytes_x);\n",
        "  float* hmean_ref = (float*)std::malloc(bytes_b);\n",
        "  float* hinv_ref  = (float*)std::malloc(bytes_b);\n",
        "  float* hmean_gpu = (float*)std::malloc(bytes_b);\n",
        "  float* hinv_gpu  = (float*)std::malloc(bytes_b);\n",
        "\n",
        "  float* hdout = (float*)std::malloc(bytes_x);\n",
        "  float* hdx_ref = (float*)std::malloc(bytes_x);\n",
        "  float* hdx_gpu = (float*)std::malloc(bytes_x);\n",
        "  float* hdg_ref = (float*)std::malloc(bytes_d);\n",
        "  float* hdb_ref = (float*)std::malloc(bytes_d);\n",
        "  float* hdg_gpu = (float*)std::malloc(bytes_d);\n",
        "  float* hdb_gpu = (float*)std::malloc(bytes_d);\n",
        "\n",
        "  if (!hx||!hgamma||!hbeta||!hy_ref||!hy_gpu||!hmean_ref||!hinv_ref||!hmean_gpu||!hinv_gpu||\n",
        "      !hdout||!hdx_ref||!hdx_gpu||!hdg_ref||!hdb_ref||!hdg_gpu||!hdb_gpu) {\n",
        "    fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  // Init data\n",
        "  unsigned int st = 123u;\n",
        "  for (int i = 0; i < B * D; ++i) hx[i] = (frand01(st) - 0.5f) * 2.0f;\n",
        "  for (int i = 0; i < D; ++i) {\n",
        "    hgamma[i] = 0.5f + frand01(st);\n",
        "    hbeta[i]  = (frand01(st) - 0.5f) * 0.1f;\n",
        "  }\n",
        "  for (int i = 0; i < B * D; ++i) hdout[i] = (frand01(st) - 0.5f) * 2.0f;\n",
        "\n",
        "  // CPU refs\n",
        "  ln_forward_cpu(hx, hgamma, hbeta, hy_ref, hmean_ref, hinv_ref, B, D, eps);\n",
        "  ln_backward_cpu(hx, hgamma, hmean_ref, hinv_ref, hdout, hdx_ref, hdg_ref, hdb_ref, B, D);\n",
        "\n",
        "  // Device buffers\n",
        "  float *dx=nullptr, *dgamma=nullptr, *dbeta=nullptr;\n",
        "  float *x=nullptr, *gamma=nullptr, *beta=nullptr, *y=nullptr, *mean=nullptr, *inv=nullptr, *dout=nullptr;\n",
        "  CUDA_CHECK(cudaMalloc(&x, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&gamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&beta, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&y, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&mean, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&dout, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&dx, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&dgamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&dbeta, bytes_d));\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(x, hx, bytes_x, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(gamma, hgamma, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(beta, hbeta, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(dout, hdout, bytes_x, cudaMemcpyHostToDevice));\n",
        "\n",
        "  CUDA_CHECK(cudaMemset(y, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(mean, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(dx, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(dgamma, 0, bytes_d));\n",
        "  CUDA_CHECK(cudaMemset(dbeta, 0, bytes_d));\n",
        "\n",
        "  // ------------------------------------------------------------\n",
        "  // TODO: Choose launch config(s)\n",
        "  // Forward:\n",
        "  dim3 f_block(0, 0, 1); // TODO\n",
        "  dim3 f_grid(0, 0, 1);  // TODO\n",
        "  // Backward:\n",
        "  dim3 b_block(0, 0, 1); // TODO\n",
        "  dim3 b_grid(0, 0, 1);  // TODO\n",
        "  // ------------------------------------------------------------\n",
        "\n",
        "  // ---------------------------\n",
        "  // Correctness: forward\n",
        "  // ---------------------------\n",
        "  ln_forward_warp<<<f_grid, f_block>>>(x, gamma, beta, y, mean, inv, B, D, eps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hy_gpu, y, bytes_x, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hmean_gpu, mean, bytes_b, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hinv_gpu, inv, bytes_b, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  bool ok_y = check_allclose(hy_gpu, hy_ref, B*D, 1e-3f, 1e-3f, \"y\");\n",
        "  bool ok_m = check_allclose(hmean_gpu, hmean_ref, B,   1e-4f, 1e-4f, \"mean\");\n",
        "  bool ok_i = check_allclose(hinv_gpu,  hinv_ref,  B,   1e-4f, 1e-4f, \"inv_std\");\n",
        "  printf(\"Forward correctness: %s\\n\", (ok_y && ok_m && ok_i) ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "  // ---------------------------\n",
        "  // Correctness: backward\n",
        "  // ---------------------------\n",
        "  CUDA_CHECK(cudaMemset(dx, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(dgamma, 0, bytes_d));\n",
        "  CUDA_CHECK(cudaMemset(dbeta, 0, bytes_d));\n",
        "\n",
        "  ln_backward_warp<<<b_grid, b_block>>>(x, gamma, mean, inv, dout, dx, dgamma, dbeta, B, D);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hdx_gpu, dx, bytes_x, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hdg_gpu, dgamma, bytes_d, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hdb_gpu, dbeta, bytes_d, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  bool ok_dx = check_allclose(hdx_gpu, hdx_ref, B*D, 1e-2f, 1e-2f, \"dx\");\n",
        "  bool ok_dg = check_allclose(hdg_gpu, hdg_ref, D,   1e-2f, 1e-2f, \"dgamma\");\n",
        "  bool ok_db = check_allclose(hdb_gpu, hdb_ref, D,   1e-2f, 1e-2f, \"dbeta\");\n",
        "  printf(\"Backward correctness: %s\\n\", (ok_dx && ok_dg && ok_db) ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "  // ---------------------------\n",
        "  // Benchmark: forward/backward\n",
        "  // ---------------------------\n",
        "  int warmup = 20;\n",
        "  int benchIters = iters;\n",
        "\n",
        "  float f_ms = bench_kernel_ms(ln_forward_warp, f_grid, f_block, warmup, benchIters,\n",
        "                               x, gamma, beta, y, mean, inv, B, D, eps);\n",
        "\n",
        "  float b_ms = bench_kernel_ms(ln_backward_warp, b_grid, b_block, warmup, benchIters,\n",
        "                               x, gamma, mean, inv, dout, dx, dgamma, dbeta, B, D);\n",
        "\n",
        "  printf(\"Benchmark forward:  %.4f ms\\n\", f_ms);\n",
        "  printf(\"Benchmark backward: %.4f ms\\n\", b_ms);\n",
        "\n",
        "  // Cleanup\n",
        "  CUDA_CHECK(cudaFree(x));\n",
        "  CUDA_CHECK(cudaFree(gamma));\n",
        "  CUDA_CHECK(cudaFree(beta));\n",
        "  CUDA_CHECK(cudaFree(y));\n",
        "  CUDA_CHECK(cudaFree(mean));\n",
        "  CUDA_CHECK(cudaFree(inv));\n",
        "  CUDA_CHECK(cudaFree(dout));\n",
        "  CUDA_CHECK(cudaFree(dx));\n",
        "  CUDA_CHECK(cudaFree(dgamma));\n",
        "  CUDA_CHECK(cudaFree(dbeta));\n",
        "\n",
        "  std::free(hx);\n",
        "  std::free(hgamma);\n",
        "  std::free(hbeta);\n",
        "  std::free(hy_ref);\n",
        "  std::free(hy_gpu);\n",
        "  std::free(hmean_ref);\n",
        "  std::free(hinv_ref);\n",
        "  std::free(hmean_gpu);\n",
        "  std::free(hinv_gpu);\n",
        "  std::free(hdout);\n",
        "  std::free(hdx_ref);\n",
        "  std::free(hdx_gpu);\n",
        "  std::free(hdg_ref);\n",
        "  std::free(hdb_ref);\n",
        "  std::free(hdg_gpu);\n",
        "  std::free(hdb_gpu);\n",
        "\n",
        "  return (ok_y && ok_m && ok_i && ok_dx && ok_dg && ok_db) ? 0 : 2;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqbVBUnpdIt",
        "outputId": "b20fd245-bfc7-45b0-d689-d845976fefbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. stride=1 iters=256 mode=0 out0=32.896004\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 ln_warp_skeleton.cu -o ln_warp_skeleton\n",
        "!./ln_warp_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UptEolh9KF4",
        "outputId": "54ed7aef-c5d4-499c-a670-060255e30958"
      },
      "outputs": [],
      "source": [
        "!ncu --set default --kernel-name \"ln_forward_warp\" -o ncu_ln_fwd ./ln_warp_skeleton 4096 1024 50\n",
        "!ncu --set default --kernel-name \"ln_backward_warp\" -o ncu_ln_bwd ./ln_warp_skeleton 4096 1024 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Additional Task â€” Accuracy Comparison: Mixed Precision (FP16/BF16 I/O, FP32 Reduction) vs. Pure Low Precision\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Compare numerical accuracy between two LayerNorm implementations:\n",
        "\n",
        "1) **Mixed precision (recommended in practice)**  \n",
        "- **Input/Output:** FP16 or BF16  \n",
        "- **Reductions (mean/variance + key accumulations):** FP32  \n",
        "- **Output:** cast back to FP16/BF16  \n",
        "\n",
        "2) **Pure low precision (no accuracy fine-tune)**  \n",
        "- **Input/Output:** FP16 or BF16  \n",
        "- **Reductions and accumulations:** also FP16/BF16 (or minimal FP32 usage)  \n",
        "- No special stabilization beyond the baseline formula\n",
        "\n",
        "The goal is to quantify how much accuracy you gain by doing **FP32 reductions** while keeping storage bandwidth low.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  What You Should Implement / Compare\n",
        "You should create two kernel variants (forward + backward optional):\n",
        "\n",
        "#### Variant A â€” Mixed Precision (FP32 Reduction)\n",
        "- Load `x`, `gamma`, `beta` as FP16/BF16\n",
        "- Convert to FP32 in registers for:\n",
        "  - sum / sumsq reductions\n",
        "  - normalization math\n",
        "  - key gradient reductions (if backward included)\n",
        "- Compute `y` in FP32 then cast to FP16/BF16 and store\n",
        "\n",
        "#### Variant B â€” Pure Low Precision (No Fine-tune)\n",
        "- Load and compute in FP16/BF16 as much as possible\n",
        "- Compute mean/variance in FP16/BF16 (or minimal conversion)\n",
        "- Store output directly in FP16/BF16\n",
        "- No additional tricks to improve stability\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Š Metrics to Report (Accuracy)\n",
        "For forward:\n",
        "- **Max absolute error**: `max |y - y_ref|`\n",
        "- **Mean absolute error (MAE)**: `mean |y - y_ref|`\n",
        "- **Relative error** (optional): `|y - y_ref| / (|y_ref| + eps)`\n",
        "- **Row-wise stats drift** (optional):\n",
        "  - mean error per row\n",
        "  - variance error per row\n",
        "\n",
        "For backward (optional but valuable):\n",
        "- Error metrics for:\n",
        "  - `dx`\n",
        "  - `dgamma`\n",
        "  - `dbeta`\n",
        "\n",
        "Reference:\n",
        "- Use **FP32 CPU reference** as the ground truth.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§ª Experimental Setup Guidelines\n",
        "Test multiple regimes to stress numerical stability:\n",
        "- Different feature sizes `D`: e.g., 256 / 1024 / 4096\n",
        "- Different batch sizes `B`: e.g., 256 / 4096\n",
        "- Input distributions:\n",
        "  - standard normal\n",
        "  - wider variance (e.g., scaled normal)\n",
        "  - adversarial-ish ranges (e.g., large magnitude values)\n",
        "\n",
        "Run each configuration for both:\n",
        "- **FP16**\n",
        "- **BF16**\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Deliverables\n",
        "1. Two LN kernel variants:\n",
        "   - Mixed precision (FP32 reduction)\n",
        "   - Pure low precision (no fine-tune)\n",
        "2. An evaluation script/report that outputs:\n",
        "   - error metrics for forward (and backward if included)\n",
        "   - a short conclusion: when pure low precision breaks down\n",
        "3. (Optional) Performance numbers:\n",
        "   - runtime (ms)\n",
        "   - throughput\n",
        "   - trade-off: accuracy vs speed\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ“ Expected Outcome\n",
        "You should be able to clearly demonstrate:\n",
        "\n",
        "- **FP32 reduction dramatically improves stability**, especially for large `D`\n",
        "- Pure FP16/BF16 reductions can introduce:\n",
        "  - noticeable output drift\n",
        "  - unstable variance estimates\n",
        "  - larger gradient errors (if backward is included)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Key Takeaway\n",
        "> **Storing activations in FP16/BF16 is bandwidth-efficient, but performing reductions in FP32 is often necessary to preserve numerical accuracyâ€”especially in LayerNorm and other reduction-heavy operators.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ln_mixed_precision_accuracy_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cstring>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#if defined(__CUDA_ARCH__) || defined(__CUDACC__)\n",
        "  #include <cuda_fp16.h>\n",
        "#endif\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Build-time dtype selection:\n",
        "//   -DUSE_FP16=1   => use half for IO\n",
        "//   -DUSE_BF16=1   => use __nv_bfloat16 for IO (requires CUDA bf16 support)\n",
        "// Exactly one of USE_FP16 / USE_BF16 should be 1.\n",
        "// ------------------------------------------------------------\n",
        "#ifndef USE_FP16\n",
        "#define USE_FP16 0\n",
        "#endif\n",
        "#ifndef USE_BF16\n",
        "#define USE_BF16 0\n",
        "#endif\n",
        "\n",
        "#if (USE_FP16 + USE_BF16) != 1\n",
        "#error \"Define exactly one: -DUSE_FP16=1 or -DUSE_BF16=1\"\n",
        "#endif\n",
        "\n",
        "#if USE_BF16\n",
        "  #include <cuda_bf16.h>\n",
        "  using half_t = __nv_bfloat16;\n",
        "#else\n",
        "  using half_t = half;\n",
        "#endif\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: type conversion helpers\n",
        "// Requirements:\n",
        "//  - load half_t -> float\n",
        "//  - store float -> half_t\n",
        "// Note: for BF16 you can use __bfloat162float / __float2bfloat16\n",
        "// ------------------------------------------------------------\n",
        "__device__ __forceinline__ float to_float(half_t x) {\n",
        "  // TODO\n",
        "  return 0.0f;\n",
        "}\n",
        "__device__ __forceinline__ half_t from_float(float x) {\n",
        "  // TODO\n",
        "  return (half_t)0;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference in FP32 (ground truth)\n",
        "// y = (x - mean) * inv_std * gamma + beta\n",
        "// x, gamma, beta provided as float (already converted)\n",
        "// ============================================================\n",
        "static void ln_forward_cpu_f32(const float* x, const float* gamma, const float* beta,\n",
        "                               float* y, float* mean, float* inv_std,\n",
        "                               int B, int D, float eps) {\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    float m = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) m += xb[i];\n",
        "    m /= (float)D;\n",
        "\n",
        "    float v = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float d = xb[i] - m;\n",
        "      v += d * d;\n",
        "    }\n",
        "    v /= (float)D;\n",
        "\n",
        "    float inv = 1.0f / std::sqrt(v + eps);\n",
        "    mean[b] = m;\n",
        "    inv_std[b] = inv;\n",
        "\n",
        "    float* yb = y + b * D;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      yb[i] = xhat * gamma[i] + beta[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Warp reduction (sum) helper using __shfl_down_sync\n",
        "// ============================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "  // TODO\n",
        "  return v;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Variant A: Mixed precision IO + FP32 reduction + cast back\n",
        "// - Input/Output: half_t\n",
        "// - Reductions and core math: float\n",
        "// - Store output: half_t\n",
        "// Saves mean and inv_std as float for debugging/accuracy measurement.\n",
        "// ============================================================\n",
        "__global__ void ln_forward_mixed_fp32red(const half_t* __restrict__ x,\n",
        "                                        const half_t* __restrict__ gamma,\n",
        "                                        const half_t* __restrict__ beta,\n",
        "                                        half_t* __restrict__ y,\n",
        "                                        float* __restrict__ mean,\n",
        "                                        float* __restrict__ inv_std,\n",
        "                                        int B, int D, float eps) {\n",
        "  // TODO:\n",
        "  // - map one row per block/warp (your choice)\n",
        "  // - accumulate sum and sumsq in FP32\n",
        "  // - compute mean and inv_std in FP32\n",
        "  // - normalize + affine in FP32\n",
        "  // - cast output back to half_t\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Variant B: Pure low precision (no accuracy fine-tune)\n",
        "// - Input/Output: half_t\n",
        "// - Reduction/accumulation: low precision as much as possible\n",
        "// - No special stabilization beyond the basic formula\n",
        "// Saves mean and inv_std as float for comparison (optional).\n",
        "// ============================================================\n",
        "__global__ void ln_forward_pure_lowp(const half_t* __restrict__ x,\n",
        "                                    const half_t* __restrict__ gamma,\n",
        "                                    const half_t* __restrict__ beta,\n",
        "                                    half_t* __restrict__ y,\n",
        "                                    float* __restrict__ mean,\n",
        "                                    float* __restrict__ inv_std,\n",
        "                                    int B, int D, float eps) {\n",
        "  // TODO:\n",
        "  // - do reductions in low precision (or minimally cast)\n",
        "  // - compute mean/var in low precision path\n",
        "  // - normalize + affine mostly in low precision\n",
        "  // - cast/store output\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Utilities: random init + conversions for CPU reference\n",
        "// ============================================================\n",
        "static inline float frand(unsigned int &s) {\n",
        "  s = 1664525u * s + 1013904223u;\n",
        "  return (s & 0x00FFFFFF) / float(0x01000000); // [0,1)\n",
        "}\n",
        "\n",
        "static void init_half_data(half_t* hx, half_t* hgamma, half_t* hbeta,\n",
        "                           int B, int D, float scale_x) {\n",
        "  unsigned int st = 123u;\n",
        "  for (int i = 0; i < B * D; ++i) {\n",
        "    float v = (frand(st) - 0.5f) * 2.0f * scale_x;\n",
        "    // TODO: convert float -> half_t\n",
        "    hx[i] = (half_t)0;\n",
        "  }\n",
        "  for (int i = 0; i < D; ++i) {\n",
        "    float g = 0.5f + frand(st);\n",
        "    float b = (frand(st) - 0.5f) * 0.1f;\n",
        "    // TODO: convert float -> half_t\n",
        "    hgamma[i] = (half_t)0;\n",
        "    hbeta[i]  = (half_t)0;\n",
        "  }\n",
        "}\n",
        "\n",
        "static void half_to_float_host(const half_t* in, float* out, int n) {\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    // TODO: convert half_t -> float (host-side)\n",
        "    out[i] = 0.0f;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Accuracy metrics\n",
        "// ============================================================\n",
        "struct ErrStats {\n",
        "  double max_abs = 0.0;\n",
        "  double mae = 0.0;\n",
        "  double max_rel = 0.0;\n",
        "};\n",
        "\n",
        "static ErrStats compute_error(const float* y, const float* yref, int n, float eps) {\n",
        "  ErrStats s;\n",
        "  double sum_abs = 0.0;\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    double a = (double)y[i];\n",
        "    double b = (double)yref[i];\n",
        "    double diff = std::fabs(a - b);\n",
        "    double rel = diff / (std::fabs(b) + eps);\n",
        "    sum_abs += diff;\n",
        "    if (diff > s.max_abs) s.max_abs = diff;\n",
        "    if (rel  > s.max_rel) s.max_rel = rel;\n",
        "  }\n",
        "  s.mae = sum_abs / (double)n;\n",
        "  return s;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Benchmark helper\n",
        "// ============================================================\n",
        "template <typename Kernel, typename... Args>\n",
        "static float bench_kernel_ms(Kernel k, dim3 grid, dim3 block,\n",
        "                             int warmup, int iters, Args... args) {\n",
        "  for (int i = 0; i < warmup; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < iters; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "  return ms / iters;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // Args: ./ln_acc <B> <D> <iters> <scale_x>\n",
        "  int B = (argc > 1) ? std::atoi(argv[1]) : 4096;\n",
        "  int D = (argc > 2) ? std::atoi(argv[2]) : 1024;\n",
        "  int iters = (argc > 3) ? std::atoi(argv[3]) : 200;\n",
        "  float scale_x = (argc > 4) ? std::atof(argv[4]) : 1.0f;\n",
        "  float eps = 1e-5f;\n",
        "\n",
        "  printf(\"LN Accuracy Skeleton: B=%d D=%d iters=%d scale_x=%g\\n\", B, D, iters, scale_x);\n",
        "\n",
        "  size_t bytes_x = (size_t)B * D * sizeof(half_t);\n",
        "  size_t bytes_d = (size_t)D * sizeof(half_t);\n",
        "  size_t bytes_y = (size_t)B * D * sizeof(half_t);\n",
        "  size_t bytes_b = (size_t)B * sizeof(float);\n",
        "\n",
        "  // Host (half)\n",
        "  half_t* hx = (half_t*)std::malloc(bytes_x);\n",
        "  half_t* hgamma = (half_t*)std::malloc(bytes_d);\n",
        "  half_t* hbeta  = (half_t*)std::malloc(bytes_d);\n",
        "  half_t* hy_mixed = (half_t*)std::malloc(bytes_y);\n",
        "  half_t* hy_pure  = (half_t*)std::malloc(bytes_y);\n",
        "\n",
        "  // Host (float ref + float views)\n",
        "  float* hx_f32 = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hgamma_f32 = (float*)std::malloc((size_t)D * sizeof(float));\n",
        "  float* hbeta_f32  = (float*)std::malloc((size_t)D * sizeof(float));\n",
        "  float* hy_ref = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hmean_ref = (float*)std::malloc((size_t)B * sizeof(float));\n",
        "  float* hinv_ref  = (float*)std::malloc((size_t)B * sizeof(float));\n",
        "\n",
        "  float* hy_mixed_f32 = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hy_pure_f32  = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "\n",
        "  if (!hx||!hgamma||!hbeta||!hy_mixed||!hy_pure||!hx_f32||!hgamma_f32||!hbeta_f32||\n",
        "      !hy_ref||!hmean_ref||!hinv_ref||!hy_mixed_f32||!hy_pure_f32) {\n",
        "    fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  // Init\n",
        "  init_half_data(hx, hgamma, hbeta, B, D, scale_x);\n",
        "\n",
        "  // Convert to float for CPU ref\n",
        "  half_to_float_host(hx, hx_f32, B * D);\n",
        "  half_to_float_host(hgamma, hgamma_f32, D);\n",
        "  half_to_float_host(hbeta,  hbeta_f32,  D);\n",
        "\n",
        "  // CPU reference FP32\n",
        "  ln_forward_cpu_f32(hx_f32, hgamma_f32, hbeta_f32, hy_ref, hmean_ref, hinv_ref, B, D, eps);\n",
        "\n",
        "  // Device alloc\n",
        "  half_t *dx=nullptr, *dg=nullptr, *db=nullptr; // unused, placeholder\n",
        "  half_t *x=nullptr, *gamma=nullptr, *beta=nullptr, *y_mixed=nullptr, *y_pure=nullptr;\n",
        "  float *mean_mixed=nullptr, *inv_mixed=nullptr, *mean_pure=nullptr, *inv_pure=nullptr;\n",
        "\n",
        "  CUDA_CHECK(cudaMalloc(&x, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&gamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&beta, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&y_mixed, bytes_y));\n",
        "  CUDA_CHECK(cudaMalloc(&y_pure,  bytes_y));\n",
        "\n",
        "  CUDA_CHECK(cudaMalloc(&mean_mixed, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv_mixed,  bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&mean_pure,  bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv_pure,   bytes_b));\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(x, hx, bytes_x, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(gamma, hgamma, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(beta,  hbeta,  bytes_d, cudaMemcpyHostToDevice));\n",
        "\n",
        "  CUDA_CHECK(cudaMemset(y_mixed, 0, bytes_y));\n",
        "  CUDA_CHECK(cudaMemset(y_pure,  0, bytes_y));\n",
        "  CUDA_CHECK(cudaMemset(mean_mixed, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv_mixed,  0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(mean_pure,  0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv_pure,   0, bytes_b));\n",
        "\n",
        "  // ------------------------------------------------------------\n",
        "  // TODO: choose launch config (same for both variants)\n",
        "  // ------------------------------------------------------------\n",
        "  dim3 block(0,0,1); // TODO\n",
        "  dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "  // Run both variants once\n",
        "  ln_forward_mixed_fp32red<<<grid, block>>>(x, gamma, beta, y_mixed, mean_mixed, inv_mixed, B, D, eps);\n",
        "  ln_forward_pure_lowp<<<grid, block>>>(x, gamma, beta, y_pure, mean_pure, inv_pure, B, D, eps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hy_mixed, y_mixed, bytes_y, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hy_pure,  y_pure,  bytes_y, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // Convert outputs to float for error metrics\n",
        "  half_to_float_host(hy_mixed, hy_mixed_f32, B * D);\n",
        "  half_to_float_host(hy_pure,  hy_pure_f32,  B * D);\n",
        "\n",
        "  ErrStats e_mixed = compute_error(hy_mixed_f32, hy_ref, B*D, 1e-8f);\n",
        "  ErrStats e_pure  = compute_error(hy_pure_f32,  hy_ref, B*D, 1e-8f);\n",
        "\n",
        "  printf(\"Accuracy vs FP32 ref:\\n\");\n",
        "  printf(\"  Mixed (FP32 reduction): max_abs=%.6g  mae=%.6g  max_rel=%.6g\\n\",\n",
        "         e_mixed.max_abs, e_mixed.mae, e_mixed.max_rel);\n",
        "  printf(\"  Pure  (lowp reduction): max_abs=%.6g  mae=%.6g  max_rel=%.6g\\n\",\n",
        "         e_pure.max_abs, e_pure.mae, e_pure.max_rel);\n",
        "\n",
        "  // Benchmark\n",
        "  int warmup = 20;\n",
        "  int benchIters = iters;\n",
        "\n",
        "  float ms_mixed = bench_kernel_ms(ln_forward_mixed_fp32red, grid, block, warmup, benchIters,\n",
        "                                   x, gamma, beta, y_mixed, mean_mixed, inv_mixed, B, D, eps);\n",
        "\n",
        "  float ms_pure  = bench_kernel_ms(ln_forward_pure_lowp, grid, block, warmup, benchIters,\n",
        "                                   x, gamma, beta, y_pure, mean_pure, inv_pure, B, D, eps);\n",
        "\n",
        "  printf(\"Benchmark:\\n\");\n",
        "  printf(\"  Mixed (FP32 reduction): %.4f ms\\n\", ms_mixed);\n",
        "  printf(\"  Pure  (lowp reduction): %.4f ms\\n\", ms_pure);\n",
        "\n",
        "  // Cleanup\n",
        "  CUDA_CHECK(cudaFree(x));\n",
        "  CUDA_CHECK(cudaFree(gamma));\n",
        "  CUDA_CHECK(cudaFree(beta));\n",
        "  CUDA_CHECK(cudaFree(y_mixed));\n",
        "  CUDA_CHECK(cudaFree(y_pure));\n",
        "  CUDA_CHECK(cudaFree(mean_mixed));\n",
        "  CUDA_CHECK(cudaFree(inv_mixed));\n",
        "  CUDA_CHECK(cudaFree(mean_pure));\n",
        "  CUDA_CHECK(cudaFree(inv_pure));\n",
        "\n",
        "  std::free(hx);\n",
        "  std::free(hgamma);\n",
        "  std::free(hbeta);\n",
        "  std::free(hy_mixed);\n",
        "  std::free(hy_pure);\n",
        "  std::free(hx_f32);\n",
        "  std::free(hgamma_f32);\n",
        "  std::free(hbeta_f32);\n",
        "  std::free(hy_ref);\n",
        "  std::free(hmean_ref);\n",
        "  std::free(hinv_ref);\n",
        "  std::free(hy_mixed_f32);\n",
        "  std::free(hy_pure_f32);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 ln_mixed_precision_accuracy_skeleton.cu -o ln_mixed_precision_accuracy_skeleton\n",
        "!./ln_mixed_precision_accuracy_skeleton\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
