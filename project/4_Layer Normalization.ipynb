{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXGn4pqo8-o"
      },
      "source": [
        "# ðŸ“˜ Task Explanation: Layer Normalization (LN) CUDA Kernels  \n",
        "*(Forward with Warp-Level Reduction & Backward with Gradient Propagation)*\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "The objective of this task is to design **CUDA kernel skeletons (without solution)** for **Layer Normalization (LN)**, covering both:\n",
        "- **LN forward pass** using **warp-level reduction**\n",
        "- **LN backward pass** for **gradient propagation**\n",
        "\n",
        "This task focuses on **kernel structure, parallel decomposition, and data flow**, rather than providing a completed implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Background: What Is Layer Normalization?\n",
        "Layer Normalization is widely used in modern neural networks, especially in:\n",
        "- Transformers (LLMs)\n",
        "- RNNs and sequence models\n",
        "- MLP blocks\n",
        "\n",
        "For an input vector \\( x \\in \\mathbb{R}^D \\), LN computes:\n",
        "\\[\n",
        "\\mu = \\frac{1}{D}\\sum_{i=1}^{D} x_i, \\quad\n",
        "\\sigma^2 = \\frac{1}{D}\\sum_{i=1}^{D}(x_i - \\mu)^2\n",
        "\\]\n",
        "\\[\n",
        "y_i = \\gamma \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
        "\\]\n",
        "\n",
        "LN is **reduction-heavy** and **memory-sensitive**, making it an ideal case for:\n",
        "- Warp-level reductions\n",
        "- Register reuse\n",
        "- Careful kernel design\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part A â€” LN Forward Kernel (Warp-Level Reduction)\n",
        "\n",
        "### Task\n",
        "Design a CUDA kernel skeleton for the **LN forward pass**, where:\n",
        "- Each block (or warp) processes one LN row (one feature vector)\n",
        "- Mean and variance are computed using **warp-level reduction**\n",
        "- Normalization and affine transform (`gamma`, `beta`) are applied\n",
        "\n",
        "### Key Design Requirements\n",
        "- Use **warp-level primitives** (e.g., shuffle-based reduction)\n",
        "- Avoid block-wide synchronization when possible\n",
        "- Minimize global memory traffic\n",
        "- Structure the kernel so that:\n",
        "  - Reduction â†’ normalization â†’ write-back stages are clear\n",
        "\n",
        "### What You Should Think About\n",
        "- Mapping rows to warps or blocks\n",
        "- How partial sums are accumulated\n",
        "- Where registers vs shared memory would be used\n",
        "- Numerical stability considerations (but not implementation)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part B â€” LN Backward Kernel (Gradient Propagation)\n",
        "\n",
        "### Task\n",
        "Design a CUDA kernel skeleton for the **LN backward pass**, computing gradients for:\n",
        "- Input (`dx`)\n",
        "- Scale (`dgamma`)\n",
        "- Bias (`dbeta`)\n",
        "\n",
        "### Key Design Requirements\n",
        "- Clearly separate gradient paths:\n",
        "  - Gradient through normalization\n",
        "  - Gradient through mean and variance\n",
        "- Use parallel reduction patterns where needed\n",
        "- Ensure correct data dependencies between steps\n",
        "- Maintain a clean structure that mirrors the math of LN backward\n",
        "\n",
        "### What You Should Think About\n",
        "- Which gradients require reductions across the feature dimension\n",
        "- Which values from the forward pass must be reused\n",
        "- How to organize the kernel to avoid race conditions\n",
        "- Where warp-level vs block-level reduction might be required\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Deliverables\n",
        "You should produce:\n",
        "1. A **CUDA kernel skeleton** for LN forward (warp-level reduction)\n",
        "2. A **CUDA kernel skeleton** for LN backward (gradient propagation)\n",
        "3. Clearly marked `TODO` sections indicating:\n",
        "   - Reduction logic\n",
        "   - Memory access patterns\n",
        "   - Synchronization points (if any)\n",
        "\n",
        "> âš ï¸ No actual implementation logic is required â€” only the kernel structure.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How LN maps to GPU parallelism\n",
        "- Why warp-level reduction is well-suited for LN\n",
        "- How forward and backward passes differ in data flow\n",
        "- How real ML kernels are structured internally\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Relevance to ML Systems\n",
        "Layer Normalization is a **core operator** in:\n",
        "- Transformer blocks\n",
        "- FlashAttention pipelines\n",
        "- Fused LN + residual kernels\n",
        "- High-performance inference and training engines\n",
        "\n",
        "Being able to design LN kernel skeletons means you can:\n",
        "- Read and understand optimized kernels (e.g., in PyTorch, Triton)\n",
        "- Reason about performance bottlenecks\n",
        "- Communicate effectively as an **ML Systems / GPU Kernel Engineer**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Key Takeaway\n",
        "> **This task trains you to think in terms of GPU kernel structure and data flow for a real, production-critical ML operatorâ€”without relying on a finished implementation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pBKltrp9GP",
        "outputId": "4eb776bf-2e9c-499a-e5ef-6ad8791ff21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Wed Jan 21 12:31:33 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyuqf4e1iZBl",
        "outputId": "a42aa667-4249-440f-9ef8-13fc13e1386f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Connecting to security.ub\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Connecting to security.ub\r0% [Connecting to archive.ubuntu.com (91.189.92.24)] [Connecting to security.ub\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,302 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [354 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,877 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,630 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [45.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,601 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Fetched 38.7 MB in 3s (11.4 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4-config-common cuda-tools-12-4\n",
            "  cuda-visual-tools-12-4 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gds-tools-12-4 gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libcublas-12-4 libcublas-dev-12-4 libcufft-12-4\n",
            "  libcufft-dev-12-4 libcufile-12-4 libcufile-dev-12-4 libcurand-12-4\n",
            "  libcurand-dev-12-4 libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4\n",
            "  libcusparse-dev-12-4 libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4\n",
            "  libnvfatbin-dev-12-4 libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4\n",
            "  libnvjpeg-dev-12-4 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxcomposite1 libxkbcommon-x11-0 libxtst6 libxxf86dga1\n",
            "  nsight-compute-2024.1.1 nsight-systems-2023.4.4 openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4 cuda-toolkit-12-4-config-common\n",
            "  cuda-tools-12-4 cuda-visual-tools-12-4 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gds-tools-12-4\n",
            "  gsettings-desktop-schemas libatk-bridge2.0-0 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libatk1.0-0 libatk1.0-data libatspi2.0-0\n",
            "  libcublas-12-4 libcublas-dev-12-4 libcufft-12-4 libcufft-dev-12-4\n",
            "  libcufile-12-4 libcufile-dev-12-4 libcurand-12-4 libcurand-dev-12-4\n",
            "  libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4 libcusparse-dev-12-4\n",
            "  libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4 libnvfatbin-dev-12-4\n",
            "  libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4 libnvjpeg-dev-12-4\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 libxxf86dga1 nsight-compute-2024.1.1\n",
            "  nsight-systems-2023.4.4 openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "0 upgraded, 88 newly installed, 0 to remove and 100 not upgraded.\n",
            "Need to get 3,068 MB of archives.\n",
            "After this operation, 6,782 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-12-4 12.4.127-1 [1,200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-12-4 12.4.127-1 [16.8 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.29+7-1ubuntu1~22.04 [42.6 MB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-12-4 12.4.127-1 [3,830 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-12-4 12.4.127-1 [49.9 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-12-4 12.4.127-1 [225 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.29+7-1ubuntu1~22.04 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-12-4 12.4.127-1 [4,920 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-12-4 12.4.127-1 [2,431 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-12-4 12.4.127-1 [51.5 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-12-4 12.4.127-1 [9,148 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-12-4 12.4.1-1 [2,538 B]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-12-4 12.4.127-1 [191 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4-config-common 12.4.127-1 [16.4 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-12-4 12.4.127-1 [165 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-12-4 12.4.127-1 [28.6 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-12-4 12.4.127-1 [1,000 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvm-12-4 12.4.131-1 [19.5 MB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-crt-12-4 12.4.131-1 [78.0 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-12-4 12.4.131-1 [32.0 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-12-4 12.4.127-1 [58.8 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-12-4 12.4.1-1 [2,510 B]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-12-4 12.4.127-1 [50.0 kB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-12-4 12.4.127-1 [17.6 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-12-4 12.4.127-1 [24.0 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-12-4 12.4.5.8-1 [231 MB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-12-4 11.2.1.3-1 [171 MB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-12-4 1.9.1.3-1 [850 kB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-12-4 10.3.5.147-1 [41.4 MB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-12-4 11.6.1.9-1 [78.9 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-12-4 12.3.1.170-1 [115 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-12-4 12.2.5.30-1 [95.5 MB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-12-4 12.4.127-1 [15.5 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-12-4 12.4.127-1 [721 kB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-12-4 12.3.1.117-1 [2,327 kB]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-12-4 12.4.1-1 [2,606 B]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-12-4 12.4.127-1 [18.7 kB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-12-4 12.4.127-1 [16.9 MB]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-dev-12-4 12.4.127-1 [87.0 kB]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-12-4 12.4.5.8-1 [249 MB]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-12-4 11.2.1.3-1 [342 MB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-12-4 1.9.1.3-1 [2,435 kB]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-12-4 10.3.5.147-1 [41.6 MB]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-12-4 11.6.1.9-1 [51.4 MB]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-12-4 12.3.1.170-1 [116 MB]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-12-4 12.2.5.30-1 [92.0 MB]\n",
            "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-dev-12-4 12.4.127-1 [13.9 MB]\n",
            "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-dev-12-4 12.4.127-1 [591 kB]\n",
            "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-12-4 12.3.1.117-1 [2,025 kB]\n",
            "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-12-4 12.4.1-1 [2,646 B]\n",
            "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-12-4 12.4.127-1 [119 MB]\n",
            "Get:79 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2024.1.1 2024.1.1.4-1 [594 MB]\n",
            "Get:80 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-12-4 12.4.1-1 [4,060 B]\n",
            "Get:81 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.4.4 2023.4.4.54-234433681190v0 [316 MB]\n",
            "Get:82 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-12-4 12.4.1-1 [3,348 B]\n",
            "Get:83 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-12-4 12.4.127-1 [145 kB]\n",
            "Get:84 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-12-4 12.4.127-1 [115 MB]\n",
            "Get:85 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-12-4 12.4.1-1 [2,942 B]\n",
            "Get:86 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-12-4 1.9.1.3-1 [39.0 MB]\n",
            "Get:87 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-12-4 12.4.1-1 [2,462 B]\n",
            "Get:88 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4 12.4.1-1 [3,336 B]\n",
            "Fetched 3,068 MB in 58s (53.3 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package cuda-cccl-12-4.\n",
            "Preparing to unpack .../05-cuda-cccl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-12-4.\n",
            "Preparing to unpack .../06-cuda-cupti-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-12-4.\n",
            "Preparing to unpack .../07-cuda-cupti-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-12-4.\n",
            "Preparing to unpack .../08-cuda-nvdisasm-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-12-4.\n",
            "Preparing to unpack .../09-cuda-cuobjdump-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-gdb-12-4.\n",
            "Preparing to unpack .../10-cuda-gdb-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-12-4.\n",
            "Preparing to unpack .../11-cuda-nvprof-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-12-4.\n",
            "Preparing to unpack .../12-cuda-nvtx-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-12-4.\n",
            "Preparing to unpack .../13-cuda-sanitizer-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-12-4.\n",
            "Preparing to unpack .../14-cuda-command-line-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-12-4.\n",
            "Preparing to unpack .../15-cuda-cuxxfilt-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4-config-common.\n",
            "Preparing to unpack .../16-cuda-toolkit-12-4-config-common_12.4.127-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-12-4.\n",
            "Preparing to unpack .../17-cuda-cudart-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-12-4.\n",
            "Preparing to unpack .../18-cuda-driver-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-12-4.\n",
            "Preparing to unpack .../19-cuda-cudart-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvm-12-4.\n",
            "Preparing to unpack .../20-cuda-nvvm-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-crt-12-4.\n",
            "Preparing to unpack .../21-cuda-crt-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-crt-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-12-4.\n",
            "Preparing to unpack .../22-cuda-nvcc-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-12-4.\n",
            "Preparing to unpack .../23-cuda-nvprune-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-compiler-12-4.\n",
            "Preparing to unpack .../24-cuda-compiler-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-documentation-12-4.\n",
            "Preparing to unpack .../25-cuda-documentation-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-12-4.\n",
            "Preparing to unpack .../26-cuda-nvrtc-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-12-4.\n",
            "Preparing to unpack .../27-cuda-opencl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-12-4.\n",
            "Preparing to unpack .../28-libcublas-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-12-4.\n",
            "Preparing to unpack .../29-libcufft-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-12-4.\n",
            "Preparing to unpack .../30-libcufile-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-12-4.\n",
            "Preparing to unpack .../31-libcurand-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-12-4.\n",
            "Preparing to unpack .../32-libcusolver-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-12-4.\n",
            "Preparing to unpack .../33-libcusparse-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-12-4.\n",
            "Preparing to unpack .../34-libnpp-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-12-4.\n",
            "Preparing to unpack .../35-libnvjitlink-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-12-4.\n",
            "Preparing to unpack .../36-libnvfatbin-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-12-4.\n",
            "Preparing to unpack .../37-libnvjpeg-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-12-4.\n",
            "Preparing to unpack .../38-cuda-libraries-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-12-4.\n",
            "Preparing to unpack .../39-cuda-profiler-api-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-12-4.\n",
            "Preparing to unpack .../40-cuda-nvrtc-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-dev-12-4.\n",
            "Preparing to unpack .../41-cuda-opencl-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-dev-12-4.\n",
            "Preparing to unpack .../42-libcublas-dev-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-dev-12-4.\n",
            "Preparing to unpack .../43-libcufft-dev-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-dev-12-4.\n",
            "Preparing to unpack .../44-libcufile-dev-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-dev-12-4.\n",
            "Preparing to unpack .../45-libcurand-dev-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-12-4.\n",
            "Preparing to unpack .../46-libcusolver-dev-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-12-4.\n",
            "Preparing to unpack .../47-libcusparse-dev-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-dev-12-4.\n",
            "Preparing to unpack .../48-libnpp-dev-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-dev-12-4.\n",
            "Preparing to unpack .../49-libnvjitlink-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-dev-12-4.\n",
            "Preparing to unpack .../50-libnvfatbin-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-12-4.\n",
            "Preparing to unpack .../51-libnvjpeg-dev-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-12-4.\n",
            "Preparing to unpack .../52-cuda-libraries-dev-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../53-openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../54-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../55-openjdk-11-jre_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../56-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-12-4.\n",
            "Preparing to unpack .../57-cuda-nsight-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package nsight-compute-2024.1.1.\n",
            "Preparing to unpack .../58-nsight-compute-2024.1.1_2024.1.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-12-4.\n",
            "Preparing to unpack .../59-cuda-nsight-compute-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../60-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../61-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../62-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../63-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../64-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../65-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../66-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../67-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../68-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../69-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../70-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package nsight-systems-2023.4.4.\n",
            "Preparing to unpack .../71-nsight-systems-2023.4.4_2023.4.4.54-234433681190v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-12-4.\n",
            "Preparing to unpack .../72-cuda-nsight-systems-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-12-4.\n",
            "Preparing to unpack .../73-cuda-nvml-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-12-4.\n",
            "Preparing to unpack .../74-cuda-nvvp-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-12-4.\n",
            "Preparing to unpack .../75-cuda-visual-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package gds-tools-12-4.\n",
            "Preparing to unpack .../76-gds-tools-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package cuda-tools-12-4.\n",
            "Preparing to unpack .../77-cuda-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4.\n",
            "Preparing to unpack .../78-cuda-toolkit-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../79-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../80-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../81-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../82-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../83-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../84-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../85-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../86-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../87-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Setting up gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Setting up cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Setting alternatives\n",
            "Setting up libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-12-4 (10.3.5.147-1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libnpp-12-4 (12.2.5.30-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcufft-12-4 (11.2.1.3-1) ...\n",
            "Setting up libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-12-4 (12.4.5.8-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Setting up libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Setting up cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Setting up libcufile-12-4 (1.9.1.3-1) ...\n",
            "Setting alternatives\n",
            "Setting up nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Setting up cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Setting up libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Setting up cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Setting up cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Setting up libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Setting up libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Setting up cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-crt-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Setting up libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Setting up cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJj_KPxso5zD",
        "outputId": "173ef954-8ed6-4f41-e717-c9d334fd0b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ln_warp_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile ln_warp_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cstring>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "static inline float frand01(unsigned int &state) {\n",
        "  state = 1664525u * state + 1013904223u;\n",
        "  return (state & 0x00FFFFFF) / float(0x01000000);\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference: LayerNorm forward\n",
        "// y = (x - mean) * inv_std * gamma + beta\n",
        "// saves mean and inv_std per row\n",
        "// ============================================================\n",
        "static void ln_forward_cpu(const float* x, const float* gamma, const float* beta,\n",
        "                           float* y, float* mean, float* inv_std,\n",
        "                           int B, int D, float eps) {\n",
        "\n",
        "  // mean value\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    float m = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) m += xb[i];\n",
        "    m /= (float)D;\n",
        "\n",
        "\n",
        "  //variance\n",
        "    float v = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float d = xb[i] - m;\n",
        "      v += d * d;\n",
        "    }\n",
        "    v /= (float)D;\n",
        "\n",
        "    float inv = 1.0f / std::sqrt(v + eps);\n",
        "    mean[b] = m;\n",
        "    inv_std[b] = inv;\n",
        "\n",
        "  //forward propagation\n",
        "    float* yb = y + b * D;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xn = (xb[i] - m) * inv;\n",
        "      yb[i] = xn * gamma[i] + beta[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference: LayerNorm backward\n",
        "// Given dout, x, gamma, mean, inv_std -> compute dx, dgamma, dbeta\n",
        "// NOTE: This is reference only. You can keep it as-is.\n",
        "// ============================================================\n",
        "static void ln_backward_cpu(const float* x, const float* gamma,\n",
        "                            const float* mean, const float* inv_std,\n",
        "                            const float* dout,\n",
        "                            float* dx, float* dgamma, float* dbeta,\n",
        "                            int B, int D) {\n",
        "  // dgamma, dbeta accumulation across batch\n",
        "  for (int i = 0; i < D; ++i) { dgamma[i] = 0.0f; dbeta[i] = 0.0f; }\n",
        "\n",
        "  // dx per element\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    const float* db = dout + b * D;\n",
        "    float* dxb = dx + b * D;\n",
        "\n",
        "    float m = mean[b];\n",
        "    float inv = inv_std[b];\n",
        "\n",
        "    // xhat, dY*gamma\n",
        "    // dx formula for LN (per-row):\n",
        "    // dx = (1/D) * inv * (D*dxhat - sum(dxhat) - xhat*sum(dxhat*xhat))\n",
        "    // where dxhat = dout * gamma, xhat = (x - mean) * inv\n",
        "    float sum_dxhat = 0.0f;\n",
        "    float sum_dxhat_xhat = 0.0f;\n",
        "\n",
        "\n",
        "    //summation of gi and gx\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      float dxhat = db[i] * gamma[i];\n",
        "      sum_dxhat += dxhat;\n",
        "      sum_dxhat_xhat += dxhat * xhat;\n",
        "\n",
        "      dgamma[i] += db[i] * xhat;\n",
        "      dbeta[i]  += db[i];\n",
        "    }\n",
        "\n",
        "    //back propagation\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      float dxhat = db[i] * gamma[i];\n",
        "      float val = ( (float)D * dxhat - sum_dxhat - xhat * sum_dxhat_xhat );\n",
        "      dxb[i] = (inv / (float)D) * val;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Warp-level reduction helper using __shfl_down_sync\n",
        "// ============================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "  // TODO: implement with __shfl_down_sync\n",
        "  for(int offset = 16; offset > 0; offset >>= 1){\n",
        "    v += __shfl_down_sync(0xffffffff, v, offset);\n",
        "  }\n",
        "\n",
        "  return v;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: LN forward CUDA kernel (warp reduce)\n",
        "// Suggested mapping:\n",
        "//  - One row per block or per warp (your choice)\n",
        "//  - Compute mean and invstd using warp-level reductions\n",
        "//  - Write y and save mean/invstd\n",
        "// ============================================================\n",
        "__global__ void ln_forward_warp(const float* __restrict__ x,\n",
        "                               const float* __restrict__ gamma,\n",
        "                               const float* __restrict__ beta,\n",
        "                               float* __restrict__ y,\n",
        "                               float* __restrict__ mean,\n",
        "                               float* __restrict__ invstd,\n",
        "                               int B, int D, float eps) {\n",
        "  // TODO\n",
        "\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5; // consider why it is like this instead of tid << 5\n",
        "  int num_warp = (blockDim.x + 31)/ 32;\n",
        "\n",
        "  const float* xrow = x + (size_t)row * D;\n",
        "  float* yrow = y + (size_t)row * D;\n",
        "\n",
        "  // step 1: each thread acculates partial sum and sumq\n",
        "\n",
        "  float sumx = 0.0f;\n",
        "  float sumq = 0.0f;\n",
        "  for (int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      sumx += xi;\n",
        "      sumq += xi * xi;\n",
        "  }\n",
        "\n",
        "  //step2: reduce within each warp\n",
        "  sumx = warpReduceSum(sumx);\n",
        "  sumq = warpReduceSum(sumq);\n",
        "\n",
        "  //step3: write warp partials to shared memory, then reduce again using warp 0\n",
        "  __shared__ float warp_sums[32];\n",
        "  __shared__ float warp_sumsq[32];\n",
        "\n",
        "  if (lane == 0){\n",
        "      warp_sums[warp] = sumx;\n",
        "      warp_sumsq[warp] = sumq;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  //get block sum\n",
        "  float block_sum = 0.f, block_sumsq = 0.f;\n",
        "  if(warp == 0){\n",
        "    block_sum = (lane < num_warp) ? warp_sums[lane]  : 0.f;\n",
        "    block_sumsq = (lane < num_warp) ? warp_sumsq[lane] : 0.f;\n",
        "\n",
        "    block_sum   = warpReduceSum(block_sum);\n",
        "    block_sumsq = warpReduceSum(block_sumsq);\n",
        "  }\n",
        "\n",
        "  //step4: broadcast mean and invstd to all threads\n",
        "  __shared__ float sh_mu, sh_inv;\n",
        "  if (tid == 0){\n",
        "      float mu = block_sum / float(D);\n",
        "      float var = block_sumsq / (float)D - mu * mu;\n",
        "      float inv = rsqrtf(var + eps);\n",
        "      sh_mu = mu;\n",
        "      sh_inv = inv;\n",
        "      mean[row] = mu;\n",
        "      invstd[row] = inv;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "\n",
        "  //step5: get result\n",
        "  float mu = sh_mu, inv = sh_inv;\n",
        "  for(int i = tid; i < D; i += blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      yrow[i] = xhat * gamma[i] + beta[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: LN backward CUDA kernel (gradient propagation)\n",
        "// Inputs: x, gamma, mean, inv_std, dout\n",
        "// Outputs: dx, dgamma, dbeta\n",
        "//\n",
        "// Notes:\n",
        "//  - dgamma/dbeta are reduced across batch (need atomics OR 2-pass reduction)\n",
        "//  - This skeleton does NOT prescribe a final strategy; you choose.\n",
        "// ============================================================\n",
        "__global__ void ln_backward_warp(const float* __restrict__ x,\n",
        "                                const float* __restrict__ gamma,\n",
        "                                const float* __restrict__ mean,\n",
        "                                const float* __restrict__ inv_std,\n",
        "                                const float* __restrict__ dout,\n",
        "                                float* __restrict__ dx,\n",
        "                                float* __restrict__ dgamma,\n",
        "                                float* __restrict__ dbeta,\n",
        "                                int B, int D) {\n",
        "  // TODO\n",
        "  int row = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "  int lane = tid & 31;\n",
        "  int warp = tid >> 5;\n",
        "  int num_warps = (blockDim.x + 31) / 32;\n",
        "\n",
        "  const float* xrow  = x  + (size_t)row * D;\n",
        "  const float* dyrow = dout + (size_t)row * D;\n",
        "  float* dxrow       = dx + (size_t)row * D;\n",
        "\n",
        "  float mu = mean[row];\n",
        "  float inv = inv_std[row];\n",
        "\n",
        "  // step1: Accumulate partial sums for s1 and s2 in FP32\n",
        "  float s1 = 0.f;   // sum(g)\n",
        "  float s2 = 0.f;   // sum(g * xhat)\n",
        "\n",
        "  for(int i = tid; i < D; i+= blockDim.x){\n",
        "      float xi = xrow[i];\n",
        "      float dyi = dyrow[i];\n",
        "      float gi  = dyi * gamma[i];\n",
        "      float xhat = (xi - mu) * inv;\n",
        "      s1 += gi;\n",
        "      s2 += gi * xhat;\n",
        "  }\n",
        "\n",
        "  // step2: Warp reduce\n",
        "\n",
        "  s1 = warpReduceSum(s1);\n",
        "  s2 = warpReduceSum(s2);\n",
        "\n",
        "  // step3: Warp partials -> shared, then reduce with warp 0\n",
        "  __shared__ float warp_s1[32];\n",
        "  __shared__ float warp_s2[32];\n",
        "\n",
        "  if (lane == 0) {\n",
        "    warp_s1[warp] = s1;\n",
        "    warp_s2[warp] = s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float block_s1 = 0.f;\n",
        "  float block_s2 = 0.f;\n",
        "\n",
        "  if (warp == 0) {\n",
        "    block_s1 = (lane < num_warps) ? warp_s1[lane] : 0.f;\n",
        "    block_s2 = (lane < num_warps) ? warp_s2[lane] : 0.f;\n",
        "    block_s1 = warpReduceSum(block_s1);\n",
        "    block_s2 = warpReduceSum(block_s2);\n",
        "  }\n",
        "\n",
        "  // step4: Broadcast block_s1/block_s2\n",
        "  __shared__ float sh_s1, sh_s2;\n",
        "  if (tid == 0) {\n",
        "    sh_s1 = block_s1;\n",
        "    sh_s2 = block_s2;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  float S1 = sh_s1;\n",
        "  float S2 = sh_s2;\n",
        "\n",
        "  //step5: Write dx\n",
        "\n",
        "  float invD = 1.0f / (float)D;\n",
        "\n",
        "  for (int i = tid; i < D; i += blockDim.x) {\n",
        "    float xi  = xrow[i];\n",
        "    float dyi = dyrow[i];\n",
        "    float gi  = dyi * gamma[i];\n",
        "    float xhat = (xi - mu) * inv;\n",
        "\n",
        "    float dx_i = inv * (gi - S1 * invD - xhat * (S2 * invD));\n",
        "    dxrow[i] = dx_i;\n",
        "    atomicAdd(&dbeta[i],  dyi);\n",
        "    atomicAdd(&dgamma[i], dyi * xhat);\n",
        "\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Utilities: correctness + benchmark\n",
        "// ============================================================\n",
        "static bool check_allclose(const float* a, const float* b, int n, float rtol, float atol,\n",
        "                           const char* name) {\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    float av = a[i], bv = b[i];\n",
        "    float diff = std::fabs(av - bv);\n",
        "    float tol = atol + rtol * std::fabs(bv);\n",
        "    if (diff > tol || std::isnan(av) || std::isnan(bv)) {\n",
        "      printf(\"Mismatch %s at %d: got=%g ref=%g diff=%g tol=%g\\n\", name, i, av, bv, diff, tol);\n",
        "      return false;\n",
        "    }\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "template <typename Kernel, typename... Args>\n",
        "static float bench_kernel_ms(Kernel k, dim3 grid, dim3 block,\n",
        "                             int warmup, int iters, Args... args) {\n",
        "  for (int i = 0; i < warmup; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < iters; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "  return ms / iters;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // Args: ./ln <B> <D> <iters>\n",
        "  int B = (argc > 1) ? std::atoi(argv[1]) : 4096;\n",
        "  int D = (argc > 2) ? std::atoi(argv[2]) : 1024;\n",
        "  int iters = (argc > 3) ? std::atoi(argv[3]) : 200;\n",
        "  float eps = 1e-5f;\n",
        "\n",
        "  printf(\"LN Skeleton: B=%d D=%d iters=%d\\n\", B, D, iters);\n",
        "\n",
        "  size_t bytes_x = (size_t)B * D * sizeof(float);\n",
        "  size_t bytes_d = (size_t)D * sizeof(float);\n",
        "  size_t bytes_b = (size_t)B * sizeof(float);\n",
        "\n",
        "  // Host buffers\n",
        "  float* hx = (float*)std::malloc(bytes_x);\n",
        "  float* hgamma = (float*)std::malloc(bytes_d);\n",
        "  float* hbeta  = (float*)std::malloc(bytes_d);\n",
        "  float* hy_ref = (float*)std::malloc(bytes_x);\n",
        "  float* hy_gpu = (float*)std::malloc(bytes_x);\n",
        "  float* hmean_ref = (float*)std::malloc(bytes_b);\n",
        "  float* hinv_ref  = (float*)std::malloc(bytes_b);\n",
        "  float* hmean_gpu = (float*)std::malloc(bytes_b);\n",
        "  float* hinv_gpu  = (float*)std::malloc(bytes_b);\n",
        "\n",
        "  float* hdout = (float*)std::malloc(bytes_x);\n",
        "  float* hdx_ref = (float*)std::malloc(bytes_x);\n",
        "  float* hdx_gpu = (float*)std::malloc(bytes_x);\n",
        "  float* hdg_ref = (float*)std::malloc(bytes_d);\n",
        "  float* hdb_ref = (float*)std::malloc(bytes_d);\n",
        "  float* hdg_gpu = (float*)std::malloc(bytes_d);\n",
        "  float* hdb_gpu = (float*)std::malloc(bytes_d);\n",
        "\n",
        "  if (!hx||!hgamma||!hbeta||!hy_ref||!hy_gpu||!hmean_ref||!hinv_ref||!hmean_gpu||!hinv_gpu||\n",
        "      !hdout||!hdx_ref||!hdx_gpu||!hdg_ref||!hdb_ref||!hdg_gpu||!hdb_gpu) {\n",
        "    fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  // Init data\n",
        "  unsigned int st = 123u;\n",
        "  for (int i = 0; i < B * D; ++i) hx[i] = (frand01(st) - 0.5f) * 2.0f;\n",
        "  for (int i = 0; i < D; ++i) {\n",
        "    hgamma[i] = 0.5f + frand01(st);\n",
        "    hbeta[i]  = (frand01(st) - 0.5f) * 0.1f;\n",
        "  }\n",
        "  for (int i = 0; i < B * D; ++i) hdout[i] = (frand01(st) - 0.5f) * 2.0f;\n",
        "\n",
        "  // CPU refs\n",
        "  ln_forward_cpu(hx, hgamma, hbeta, hy_ref, hmean_ref, hinv_ref, B, D, eps);\n",
        "  ln_backward_cpu(hx, hgamma, hmean_ref, hinv_ref, hdout, hdx_ref, hdg_ref, hdb_ref, B, D);\n",
        "\n",
        "  // Device buffers\n",
        "  float *dx=nullptr, *dgamma=nullptr, *dbeta=nullptr;\n",
        "  float *x=nullptr, *gamma=nullptr, *beta=nullptr, *y=nullptr, *mean=nullptr, *inv=nullptr, *dout=nullptr;\n",
        "  CUDA_CHECK(cudaMalloc(&x, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&gamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&beta, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&y, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&mean, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&dout, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&dx, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&dgamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&dbeta, bytes_d));\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(x, hx, bytes_x, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(gamma, hgamma, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(beta, hbeta, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(dout, hdout, bytes_x, cudaMemcpyHostToDevice));\n",
        "\n",
        "  CUDA_CHECK(cudaMemset(y, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(mean, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(dx, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(dgamma, 0, bytes_d));\n",
        "  CUDA_CHECK(cudaMemset(dbeta, 0, bytes_d));\n",
        "\n",
        "  // ------------------------------------------------------------\n",
        "  // TODO: Choose launch config(s)\n",
        "  // Forward:\n",
        "  // dim3 f_block(0, 0, 1); // TODO\n",
        "  // dim3 f_grid(0, 0, 1);  // TODO\n",
        "  // Backward:\n",
        "  // dim3 b_block(0, 0, 1); // TODO\n",
        "  // dim3 b_grid(0, 0, 1);  // TODO\n",
        "  // ------------------------------------------------------------\n",
        "  auto round_up_warp = [](int x) { return (x + 31) & ~31; };\n",
        "\n",
        "  int f_threads = 256;\n",
        "  if (D <= 128) f_threads = 128;\n",
        "  f_threads = round_up_warp(f_threads);\n",
        "\n",
        "  dim3 f_block(f_threads, 1, 1);\n",
        "  dim3 f_grid(B, 1, 1);\n",
        "\n",
        "  int b_threads = 256;\n",
        "  if (D <= 128) b_threads = 128;\n",
        "  b_threads = round_up_warp(b_threads);\n",
        "\n",
        "  dim3 b_block(b_threads, 1, 1);\n",
        "  dim3 b_grid(B, 1, 1);\n",
        "\n",
        "  // ---------------------------\n",
        "  // Correctness: forward\n",
        "  // ---------------------------\n",
        "  ln_forward_warp<<<f_grid, f_block>>>(x, gamma, beta, y, mean, inv, B, D, eps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hy_gpu, y, bytes_x, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hmean_gpu, mean, bytes_b, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hinv_gpu, inv, bytes_b, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  bool ok_y = check_allclose(hy_gpu, hy_ref, B*D, 1e-3f, 1e-3f, \"y\");\n",
        "  bool ok_m = check_allclose(hmean_gpu, hmean_ref, B,   1e-4f, 1e-4f, \"mean\");\n",
        "  bool ok_i = check_allclose(hinv_gpu,  hinv_ref,  B,   1e-4f, 1e-4f, \"inv_std\");\n",
        "  printf(\"Forward correctness: %s\\n\", (ok_y && ok_m && ok_i) ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "  // ---------------------------\n",
        "  // Correctness: backward\n",
        "  // ---------------------------\n",
        "  CUDA_CHECK(cudaMemset(dx, 0, bytes_x));\n",
        "  CUDA_CHECK(cudaMemset(dgamma, 0, bytes_d));\n",
        "  CUDA_CHECK(cudaMemset(dbeta, 0, bytes_d));\n",
        "\n",
        "  ln_backward_warp<<<b_grid, b_block>>>(x, gamma, mean, inv, dout, dx, dgamma, dbeta, B, D);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hdx_gpu, dx, bytes_x, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hdg_gpu, dgamma, bytes_d, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hdb_gpu, dbeta, bytes_d, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  bool ok_dx = check_allclose(hdx_gpu, hdx_ref, B*D, 1e-2f, 1e-2f, \"dx\");\n",
        "  bool ok_dg = check_allclose(hdg_gpu, hdg_ref, D,   1e-2f, 1e-2f, \"dgamma\");\n",
        "  bool ok_db = check_allclose(hdb_gpu, hdb_ref, D,   1e-2f, 1e-2f, \"dbeta\");\n",
        "  printf(\"Backward correctness: %s\\n\", (ok_dx && ok_dg && ok_db) ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "  // ---------------------------\n",
        "  // Benchmark: forward/backward\n",
        "  // ---------------------------\n",
        "  int warmup = 20;\n",
        "  int benchIters = iters;\n",
        "\n",
        "  float f_ms = bench_kernel_ms(ln_forward_warp, f_grid, f_block, warmup, benchIters,\n",
        "                               x, gamma, beta, y, mean, inv, B, D, eps);\n",
        "\n",
        "  float b_ms = bench_kernel_ms(ln_backward_warp, b_grid, b_block, warmup, benchIters,\n",
        "                               x, gamma, mean, inv, dout, dx, dgamma, dbeta, B, D);\n",
        "\n",
        "  printf(\"Benchmark forward:  %.4f ms\\n\", f_ms);\n",
        "  printf(\"Benchmark backward: %.4f ms\\n\", b_ms);\n",
        "\n",
        "  // Cleanup\n",
        "  CUDA_CHECK(cudaFree(x));\n",
        "  CUDA_CHECK(cudaFree(gamma));\n",
        "  CUDA_CHECK(cudaFree(beta));\n",
        "  CUDA_CHECK(cudaFree(y));\n",
        "  CUDA_CHECK(cudaFree(mean));\n",
        "  CUDA_CHECK(cudaFree(inv));\n",
        "  CUDA_CHECK(cudaFree(dout));\n",
        "  CUDA_CHECK(cudaFree(dx));\n",
        "  CUDA_CHECK(cudaFree(dgamma));\n",
        "  CUDA_CHECK(cudaFree(dbeta));\n",
        "\n",
        "  std::free(hx);\n",
        "  std::free(hgamma);\n",
        "  std::free(hbeta);\n",
        "  std::free(hy_ref);\n",
        "  std::free(hy_gpu);\n",
        "  std::free(hmean_ref);\n",
        "  std::free(hinv_ref);\n",
        "  std::free(hmean_gpu);\n",
        "  std::free(hinv_gpu);\n",
        "  std::free(hdout);\n",
        "  std::free(hdx_ref);\n",
        "  std::free(hdx_gpu);\n",
        "  std::free(hdg_ref);\n",
        "  std::free(hdb_ref);\n",
        "  std::free(hdg_gpu);\n",
        "  std::free(hdb_gpu);\n",
        "\n",
        "  return (ok_y && ok_m && ok_i && ok_dx && ok_dg && ok_db) ? 0 : 2;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqbVBUnpdIt",
        "outputId": "e0d2116f-7218-4d51-caf6-e9f306c261df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LN Skeleton: B=4096 D=1024 iters=200\n",
            "Forward correctness: PASS\n",
            "Backward correctness: PASS\n",
            "Benchmark forward:  0.2118 ms\n",
            "Benchmark backward: 0.2893 ms\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 ln_warp_skeleton.cu -o ln_warp_skeleton\n",
        "!./ln_warp_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UptEolh9KF4"
      },
      "outputs": [],
      "source": [
        "!ncu --set default --kernel-name \"ln_forward_warp\" -o ncu_ln_fwd ./ln_warp_skeleton 4096 1024 50\n",
        "!ncu --set default --kernel-name \"ln_backward_warp\" -o ncu_ln_bwd ./ln_warp_skeleton 4096 1024 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsMfK4faiZBn"
      },
      "source": [
        "## ðŸ§© Additional Task â€” Accuracy Comparison: Mixed Precision (FP16/BF16 I/O, FP32 Reduction) vs. Pure Low Precision\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "Compare numerical accuracy between two LayerNorm implementations:\n",
        "\n",
        "1) **Mixed precision (recommended in practice)**  \n",
        "- **Input/Output:** FP16 or BF16  \n",
        "- **Reductions (mean/variance + key accumulations):** FP32  \n",
        "- **Output:** cast back to FP16/BF16  \n",
        "\n",
        "2) **Pure low precision (no accuracy fine-tune)**  \n",
        "- **Input/Output:** FP16 or BF16  \n",
        "- **Reductions and accumulations:** also FP16/BF16 (or minimal FP32 usage)  \n",
        "- No special stabilization beyond the baseline formula\n",
        "\n",
        "The goal is to quantify how much accuracy you gain by doing **FP32 reductions** while keeping storage bandwidth low.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  What You Should Implement / Compare\n",
        "You should create two kernel variants (forward + backward optional):\n",
        "\n",
        "#### Variant A â€” Mixed Precision (FP32 Reduction)\n",
        "- Load `x`, `gamma`, `beta` as FP16/BF16\n",
        "- Convert to FP32 in registers for:\n",
        "  - sum / sumsq reductions\n",
        "  - normalization math\n",
        "  - key gradient reductions (if backward included)\n",
        "- Compute `y` in FP32 then cast to FP16/BF16 and store\n",
        "\n",
        "#### Variant B â€” Pure Low Precision (No Fine-tune)\n",
        "- Load and compute in FP16/BF16 as much as possible\n",
        "- Compute mean/variance in FP16/BF16 (or minimal conversion)\n",
        "- Store output directly in FP16/BF16\n",
        "- No additional tricks to improve stability\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Š Metrics to Report (Accuracy)\n",
        "For forward:\n",
        "- **Max absolute error**: `max |y - y_ref|`\n",
        "- **Mean absolute error (MAE)**: `mean |y - y_ref|`\n",
        "- **Relative error** (optional): `|y - y_ref| / (|y_ref| + eps)`\n",
        "- **Row-wise stats drift** (optional):\n",
        "  - mean error per row\n",
        "  - variance error per row\n",
        "\n",
        "For backward (optional but valuable):\n",
        "- Error metrics for:\n",
        "  - `dx`\n",
        "  - `dgamma`\n",
        "  - `dbeta`\n",
        "\n",
        "Reference:\n",
        "- Use **FP32 CPU reference** as the ground truth.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§ª Experimental Setup Guidelines\n",
        "Test multiple regimes to stress numerical stability:\n",
        "- Different feature sizes `D`: e.g., 256 / 1024 / 4096\n",
        "- Different batch sizes `B`: e.g., 256 / 4096\n",
        "- Input distributions:\n",
        "  - standard normal\n",
        "  - wider variance (e.g., scaled normal)\n",
        "  - adversarial-ish ranges (e.g., large magnitude values)\n",
        "\n",
        "Run each configuration for both:\n",
        "- **FP16**\n",
        "- **BF16**\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Deliverables\n",
        "1. Two LN kernel variants:\n",
        "   - Mixed precision (FP32 reduction)\n",
        "   - Pure low precision (no fine-tune)\n",
        "2. An evaluation script/report that outputs:\n",
        "   - error metrics for forward (and backward if included)\n",
        "   - a short conclusion: when pure low precision breaks down\n",
        "3. (Optional) Performance numbers:\n",
        "   - runtime (ms)\n",
        "   - throughput\n",
        "   - trade-off: accuracy vs speed\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ“ Expected Outcome\n",
        "You should be able to clearly demonstrate:\n",
        "\n",
        "- **FP32 reduction dramatically improves stability**, especially for large `D`\n",
        "- Pure FP16/BF16 reductions can introduce:\n",
        "  - noticeable output drift\n",
        "  - unstable variance estimates\n",
        "  - larger gradient errors (if backward is included)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Key Takeaway\n",
        "> **Storing activations in FP16/BF16 is bandwidth-efficient, but performing reductions in FP32 is often necessary to preserve numerical accuracyâ€”especially in LayerNorm and other reduction-heavy operators.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsjAl5OciZBn"
      },
      "outputs": [],
      "source": [
        "%%writefile ln_mixed_precision_accuracy_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cstring>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#if defined(__CUDA_ARCH__) || defined(__CUDACC__)\n",
        "  #include <cuda_fp16.h>\n",
        "#endif\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Build-time dtype selection:\n",
        "//   -DUSE_FP16=1   => use half for IO\n",
        "//   -DUSE_BF16=1   => use __nv_bfloat16 for IO (requires CUDA bf16 support)\n",
        "// Exactly one of USE_FP16 / USE_BF16 should be 1.\n",
        "// ------------------------------------------------------------\n",
        "#ifndef USE_FP16\n",
        "#define USE_FP16 0\n",
        "#endif\n",
        "#ifndef USE_BF16\n",
        "#define USE_BF16 0\n",
        "#endif\n",
        "\n",
        "#if (USE_FP16 + USE_BF16) != 1\n",
        "#error \"Define exactly one: -DUSE_FP16=1 or -DUSE_BF16=1\"\n",
        "#endif\n",
        "\n",
        "#if USE_BF16\n",
        "  #include <cuda_bf16.h>\n",
        "  using half_t = __nv_bfloat16;\n",
        "#else\n",
        "  using half_t = half;\n",
        "#endif\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: type conversion helpers\n",
        "// Requirements:\n",
        "//  - load half_t -> float\n",
        "//  - store float -> half_t\n",
        "// Note: for BF16 you can use __bfloat162float / __float2bfloat16\n",
        "// ------------------------------------------------------------\n",
        "__device__ __forceinline__ float to_float(half_t x) {\n",
        "  // TODO\n",
        "  return 0.0f;\n",
        "}\n",
        "__device__ __forceinline__ half_t from_float(float x) {\n",
        "  // TODO\n",
        "  return (half_t)0;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// CPU reference in FP32 (ground truth)\n",
        "// y = (x - mean) * inv_std * gamma + beta\n",
        "// x, gamma, beta provided as float (already converted)\n",
        "// ============================================================\n",
        "static void ln_forward_cpu_f32(const float* x, const float* gamma, const float* beta,\n",
        "                               float* y, float* mean, float* inv_std,\n",
        "                               int B, int D, float eps) {\n",
        "  for (int b = 0; b < B; ++b) {\n",
        "    const float* xb = x + b * D;\n",
        "    float m = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) m += xb[i];\n",
        "    m /= (float)D;\n",
        "\n",
        "    float v = 0.0f;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float d = xb[i] - m;\n",
        "      v += d * d;\n",
        "    }\n",
        "    v /= (float)D;\n",
        "\n",
        "    float inv = 1.0f / std::sqrt(v + eps);\n",
        "    mean[b] = m;\n",
        "    inv_std[b] = inv;\n",
        "\n",
        "    float* yb = y + b * D;\n",
        "    for (int i = 0; i < D; ++i) {\n",
        "      float xhat = (xb[i] - m) * inv;\n",
        "      yb[i] = xhat * gamma[i] + beta[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// TODO: Warp reduction (sum) helper using __shfl_down_sync\n",
        "// ============================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float v) {\n",
        "  // TODO\n",
        "  return v;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Variant A: Mixed precision IO + FP32 reduction + cast back\n",
        "// - Input/Output: half_t\n",
        "// - Reductions and core math: float\n",
        "// - Store output: half_t\n",
        "// Saves mean and inv_std as float for debugging/accuracy measurement.\n",
        "// ============================================================\n",
        "__global__ void ln_forward_mixed_fp32red(const half_t* __restrict__ x,\n",
        "                                        const half_t* __restrict__ gamma,\n",
        "                                        const half_t* __restrict__ beta,\n",
        "                                        half_t* __restrict__ y,\n",
        "                                        float* __restrict__ mean,\n",
        "                                        float* __restrict__ inv_std,\n",
        "                                        int B, int D, float eps) {\n",
        "  // TODO:\n",
        "  // - map one row per block/warp (your choice)\n",
        "  // - accumulate sum and sumsq in FP32\n",
        "  // - compute mean and inv_std in FP32\n",
        "  // - normalize + affine in FP32\n",
        "  // - cast output back to half_t\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Variant B: Pure low precision (no accuracy fine-tune)\n",
        "// - Input/Output: half_t\n",
        "// - Reduction/accumulation: low precision as much as possible\n",
        "// - No special stabilization beyond the basic formula\n",
        "// Saves mean and inv_std as float for comparison (optional).\n",
        "// ============================================================\n",
        "__global__ void ln_forward_pure_lowp(const half_t* __restrict__ x,\n",
        "                                    const half_t* __restrict__ gamma,\n",
        "                                    const half_t* __restrict__ beta,\n",
        "                                    half_t* __restrict__ y,\n",
        "                                    float* __restrict__ mean,\n",
        "                                    float* __restrict__ inv_std,\n",
        "                                    int B, int D, float eps) {\n",
        "  // TODO:\n",
        "  // - do reductions in low precision (or minimally cast)\n",
        "  // - compute mean/var in low precision path\n",
        "  // - normalize + affine mostly in low precision\n",
        "  // - cast/store output\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Utilities: random init + conversions for CPU reference\n",
        "// ============================================================\n",
        "static inline float frand(unsigned int &s) {\n",
        "  s = 1664525u * s + 1013904223u;\n",
        "  return (s & 0x00FFFFFF) / float(0x01000000); // [0,1)\n",
        "}\n",
        "\n",
        "static void init_half_data(half_t* hx, half_t* hgamma, half_t* hbeta,\n",
        "                           int B, int D, float scale_x) {\n",
        "  unsigned int st = 123u;\n",
        "  for (int i = 0; i < B * D; ++i) {\n",
        "    float v = (frand(st) - 0.5f) * 2.0f * scale_x;\n",
        "    // TODO: convert float -> half_t\n",
        "    hx[i] = (half_t)0;\n",
        "  }\n",
        "  for (int i = 0; i < D; ++i) {\n",
        "    float g = 0.5f + frand(st);\n",
        "    float b = (frand(st) - 0.5f) * 0.1f;\n",
        "    // TODO: convert float -> half_t\n",
        "    hgamma[i] = (half_t)0;\n",
        "    hbeta[i]  = (half_t)0;\n",
        "  }\n",
        "}\n",
        "\n",
        "static void half_to_float_host(const half_t* in, float* out, int n) {\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    // TODO: convert half_t -> float (host-side)\n",
        "    out[i] = 0.0f;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Accuracy metrics\n",
        "// ============================================================\n",
        "struct ErrStats {\n",
        "  double max_abs = 0.0;\n",
        "  double mae = 0.0;\n",
        "  double max_rel = 0.0;\n",
        "};\n",
        "\n",
        "static ErrStats compute_error(const float* y, const float* yref, int n, float eps) {\n",
        "  ErrStats s;\n",
        "  double sum_abs = 0.0;\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    double a = (double)y[i];\n",
        "    double b = (double)yref[i];\n",
        "    double diff = std::fabs(a - b);\n",
        "    double rel = diff / (std::fabs(b) + eps);\n",
        "    sum_abs += diff;\n",
        "    if (diff > s.max_abs) s.max_abs = diff;\n",
        "    if (rel  > s.max_rel) s.max_rel = rel;\n",
        "  }\n",
        "  s.mae = sum_abs / (double)n;\n",
        "  return s;\n",
        "}\n",
        "\n",
        "// ============================================================\n",
        "// Benchmark helper\n",
        "// ============================================================\n",
        "template <typename Kernel, typename... Args>\n",
        "static float bench_kernel_ms(Kernel k, dim3 grid, dim3 block,\n",
        "                             int warmup, int iters, Args... args) {\n",
        "  for (int i = 0; i < warmup; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CUDA_CHECK(cudaEventCreate(&start));\n",
        "  CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  CUDA_CHECK(cudaEventRecord(start));\n",
        "  for (int i = 0; i < iters; ++i) {\n",
        "    k<<<grid, block>>>(args...);\n",
        "  }\n",
        "  CUDA_CHECK(cudaEventRecord(stop));\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "  float ms = 0.0f;\n",
        "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "  CUDA_CHECK(cudaEventDestroy(start));\n",
        "  CUDA_CHECK(cudaEventDestroy(stop));\n",
        "  return ms / iters;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // Args: ./ln_acc <B> <D> <iters> <scale_x>\n",
        "  int B = (argc > 1) ? std::atoi(argv[1]) : 4096;\n",
        "  int D = (argc > 2) ? std::atoi(argv[2]) : 1024;\n",
        "  int iters = (argc > 3) ? std::atoi(argv[3]) : 200;\n",
        "  float scale_x = (argc > 4) ? std::atof(argv[4]) : 1.0f;\n",
        "  float eps = 1e-5f;\n",
        "\n",
        "  printf(\"LN Accuracy Skeleton: B=%d D=%d iters=%d scale_x=%g\\n\", B, D, iters, scale_x);\n",
        "\n",
        "  size_t bytes_x = (size_t)B * D * sizeof(half_t);\n",
        "  size_t bytes_d = (size_t)D * sizeof(half_t);\n",
        "  size_t bytes_y = (size_t)B * D * sizeof(half_t);\n",
        "  size_t bytes_b = (size_t)B * sizeof(float);\n",
        "\n",
        "  // Host (half)\n",
        "  half_t* hx = (half_t*)std::malloc(bytes_x);\n",
        "  half_t* hgamma = (half_t*)std::malloc(bytes_d);\n",
        "  half_t* hbeta  = (half_t*)std::malloc(bytes_d);\n",
        "  half_t* hy_mixed = (half_t*)std::malloc(bytes_y);\n",
        "  half_t* hy_pure  = (half_t*)std::malloc(bytes_y);\n",
        "\n",
        "  // Host (float ref + float views)\n",
        "  float* hx_f32 = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hgamma_f32 = (float*)std::malloc((size_t)D * sizeof(float));\n",
        "  float* hbeta_f32  = (float*)std::malloc((size_t)D * sizeof(float));\n",
        "  float* hy_ref = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hmean_ref = (float*)std::malloc((size_t)B * sizeof(float));\n",
        "  float* hinv_ref  = (float*)std::malloc((size_t)B * sizeof(float));\n",
        "\n",
        "  float* hy_mixed_f32 = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "  float* hy_pure_f32  = (float*)std::malloc((size_t)B * D * sizeof(float));\n",
        "\n",
        "  if (!hx||!hgamma||!hbeta||!hy_mixed||!hy_pure||!hx_f32||!hgamma_f32||!hbeta_f32||\n",
        "      !hy_ref||!hmean_ref||!hinv_ref||!hy_mixed_f32||!hy_pure_f32) {\n",
        "    fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  // Init\n",
        "  init_half_data(hx, hgamma, hbeta, B, D, scale_x);\n",
        "\n",
        "  // Convert to float for CPU ref\n",
        "  half_to_float_host(hx, hx_f32, B * D);\n",
        "  half_to_float_host(hgamma, hgamma_f32, D);\n",
        "  half_to_float_host(hbeta,  hbeta_f32,  D);\n",
        "\n",
        "  // CPU reference FP32\n",
        "  ln_forward_cpu_f32(hx_f32, hgamma_f32, hbeta_f32, hy_ref, hmean_ref, hinv_ref, B, D, eps);\n",
        "\n",
        "  // Device alloc\n",
        "  half_t *dx=nullptr, *dg=nullptr, *db=nullptr; // unused, placeholder\n",
        "  half_t *x=nullptr, *gamma=nullptr, *beta=nullptr, *y_mixed=nullptr, *y_pure=nullptr;\n",
        "  float *mean_mixed=nullptr, *inv_mixed=nullptr, *mean_pure=nullptr, *inv_pure=nullptr;\n",
        "\n",
        "  CUDA_CHECK(cudaMalloc(&x, bytes_x));\n",
        "  CUDA_CHECK(cudaMalloc(&gamma, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&beta, bytes_d));\n",
        "  CUDA_CHECK(cudaMalloc(&y_mixed, bytes_y));\n",
        "  CUDA_CHECK(cudaMalloc(&y_pure,  bytes_y));\n",
        "\n",
        "  CUDA_CHECK(cudaMalloc(&mean_mixed, bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv_mixed,  bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&mean_pure,  bytes_b));\n",
        "  CUDA_CHECK(cudaMalloc(&inv_pure,   bytes_b));\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(x, hx, bytes_x, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(gamma, hgamma, bytes_d, cudaMemcpyHostToDevice));\n",
        "  CUDA_CHECK(cudaMemcpy(beta,  hbeta,  bytes_d, cudaMemcpyHostToDevice));\n",
        "\n",
        "  CUDA_CHECK(cudaMemset(y_mixed, 0, bytes_y));\n",
        "  CUDA_CHECK(cudaMemset(y_pure,  0, bytes_y));\n",
        "  CUDA_CHECK(cudaMemset(mean_mixed, 0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv_mixed,  0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(mean_pure,  0, bytes_b));\n",
        "  CUDA_CHECK(cudaMemset(inv_pure,   0, bytes_b));\n",
        "\n",
        "  // ------------------------------------------------------------\n",
        "  // TODO: choose launch config (same for both variants)\n",
        "  // ------------------------------------------------------------\n",
        "  dim3 block(0,0,1); // TODO\n",
        "  dim3 grid(0,0,1);  // TODO\n",
        "\n",
        "  // Run both variants once\n",
        "  ln_forward_mixed_fp32red<<<grid, block>>>(x, gamma, beta, y_mixed, mean_mixed, inv_mixed, B, D, eps);\n",
        "  ln_forward_pure_lowp<<<grid, block>>>(x, gamma, beta, y_pure, mean_pure, inv_pure, B, D, eps);\n",
        "  CUDA_CHECK(cudaGetLastError());\n",
        "  CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "  CUDA_CHECK(cudaMemcpy(hy_mixed, y_mixed, bytes_y, cudaMemcpyDeviceToHost));\n",
        "  CUDA_CHECK(cudaMemcpy(hy_pure,  y_pure,  bytes_y, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // Convert outputs to float for error metrics\n",
        "  half_to_float_host(hy_mixed, hy_mixed_f32, B * D);\n",
        "  half_to_float_host(hy_pure,  hy_pure_f32,  B * D);\n",
        "\n",
        "  ErrStats e_mixed = compute_error(hy_mixed_f32, hy_ref, B*D, 1e-8f);\n",
        "  ErrStats e_pure  = compute_error(hy_pure_f32,  hy_ref, B*D, 1e-8f);\n",
        "\n",
        "  printf(\"Accuracy vs FP32 ref:\\n\");\n",
        "  printf(\"  Mixed (FP32 reduction): max_abs=%.6g  mae=%.6g  max_rel=%.6g\\n\",\n",
        "         e_mixed.max_abs, e_mixed.mae, e_mixed.max_rel);\n",
        "  printf(\"  Pure  (lowp reduction): max_abs=%.6g  mae=%.6g  max_rel=%.6g\\n\",\n",
        "         e_pure.max_abs, e_pure.mae, e_pure.max_rel);\n",
        "\n",
        "  // Benchmark\n",
        "  int warmup = 20;\n",
        "  int benchIters = iters;\n",
        "\n",
        "  float ms_mixed = bench_kernel_ms(ln_forward_mixed_fp32red, grid, block, warmup, benchIters,\n",
        "                                   x, gamma, beta, y_mixed, mean_mixed, inv_mixed, B, D, eps);\n",
        "\n",
        "  float ms_pure  = bench_kernel_ms(ln_forward_pure_lowp, grid, block, warmup, benchIters,\n",
        "                                   x, gamma, beta, y_pure, mean_pure, inv_pure, B, D, eps);\n",
        "\n",
        "  printf(\"Benchmark:\\n\");\n",
        "  printf(\"  Mixed (FP32 reduction): %.4f ms\\n\", ms_mixed);\n",
        "  printf(\"  Pure  (lowp reduction): %.4f ms\\n\", ms_pure);\n",
        "\n",
        "  // Cleanup\n",
        "  CUDA_CHECK(cudaFree(x));\n",
        "  CUDA_CHECK(cudaFree(gamma));\n",
        "  CUDA_CHECK(cudaFree(beta));\n",
        "  CUDA_CHECK(cudaFree(y_mixed));\n",
        "  CUDA_CHECK(cudaFree(y_pure));\n",
        "  CUDA_CHECK(cudaFree(mean_mixed));\n",
        "  CUDA_CHECK(cudaFree(inv_mixed));\n",
        "  CUDA_CHECK(cudaFree(mean_pure));\n",
        "  CUDA_CHECK(cudaFree(inv_pure));\n",
        "\n",
        "  std::free(hx);\n",
        "  std::free(hgamma);\n",
        "  std::free(hbeta);\n",
        "  std::free(hy_mixed);\n",
        "  std::free(hy_pure);\n",
        "  std::free(hx_f32);\n",
        "  std::free(hgamma_f32);\n",
        "  std::free(hbeta_f32);\n",
        "  std::free(hy_ref);\n",
        "  std::free(hmean_ref);\n",
        "  std::free(hinv_ref);\n",
        "  std::free(hy_mixed_f32);\n",
        "  std::free(hy_pure_f32);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVzCT9d8iZBo"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 ln_mixed_precision_accuracy_skeleton.cu -o ln_mixed_precision_accuracy_skeleton\n",
        "!./ln_mixed_precision_accuracy_skeleton\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}