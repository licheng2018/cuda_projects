{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsQB7XZRREv5"
      },
      "source": [
        "# ðŸ“˜ Task Explanation: Memory Coalescing, Stride Experiments, and Nsight Systems Profiling\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "The objective of this task is to understand how **global memory access patterns** affect CUDA kernel performance, with a focus on **memory coalescing**.  \n",
        "You will experimentally measure how different **memory access strides** impact performance and use **Nsight Systems** to profile and interpret the results.\n",
        "\n",
        "This task builds intuition for why GPU kernels can be slow even when they are highly parallel.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Background: What Is Memory Coalescing?\n",
        "On NVIDIA GPUs, global memory accesses are serviced in **memory transactions** (typically 32/64/128 bytes).\n",
        "\n",
        "When threads in the **same warp (32 threads)**:\n",
        "- Access **contiguous memory addresses**\n",
        "- Properly aligned to cache-line boundaries\n",
        "\n",
        "their accesses are **coalesced** into fewer transactions, resulting in:\n",
        "- Higher effective bandwidth\n",
        "- Lower memory latency\n",
        "\n",
        "If threads access memory with a **stride** (gaps between addresses), the GPU must issue **more memory transactions**, which significantly degrades performance.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Part A â€” Study Memory Coalescing\n",
        "### Task\n",
        "Study how threads in a warp access global memory and how memory transactions are formed.\n",
        "\n",
        "### What to Learn\n",
        "- How `threadIdx.x` maps to memory addresses\n",
        "- How a warp of 32 threads loads data from global memory\n",
        "- Why `A[i]` is fast but `A[i * stride]` can be slow\n",
        "\n",
        "### Expected Outcome\n",
        "You should be able to explain:\n",
        "- Why contiguous access is optimal\n",
        "- How strided access increases memory traffic\n",
        "- Why memory coalescing is critical for ML kernels (e.g., LayerNorm, Softmax, Attention)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¬ Part B â€” Stride Performance Experiment (stride = 1 / 2 / 4)\n",
        "\n",
        "### Task\n",
        "Modify a CUDA kernel so that each thread accesses memory using a configurable stride:\n",
        "\n",
        "```cpp\n",
        "C[i] = A[i * stride] + B[i * stride];\n",
        "```\n",
        "\n",
        "### Run the Kernel With\n",
        "Execute the CUDA kernel multiple times using different memory access strides:\n",
        "\n",
        "- **stride = 1** â†’ fully coalesced memory access  \n",
        "- **stride = 2** â†’ partially coalesced memory access  \n",
        "- **stride = 4** â†’ poorly coalesced (highly fragmented) memory access  \n",
        "\n",
        "---\n",
        "\n",
        "### What to Measure\n",
        "For each stride configuration, measure:\n",
        "\n",
        "- **Kernel execution time**\n",
        "- **(Optional)** Effective memory bandwidth\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Observations\n",
        "\n",
        "| Stride | Memory Access Pattern | Performance |\n",
        "|------:|-----------------------|-------------|\n",
        "| 1 | Fully contiguous | Fastest |\n",
        "| 2 | Partially coalesced | Slower |\n",
        "| 4 | Highly fragmented | Much slower |\n",
        "\n",
        "---\n",
        "\n",
        "### Why This Matters\n",
        "Many CUDA kernels are slow **not because of computation**, but because of **inefficient memory access patterns**.  \n",
        "This experiment makes the performance cost of **uncoalesced global memory access** visible and measurable, helping you understand why memory behavior often dominates GPU performance.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Part C â€” Profiling With Nsight Systems\n",
        "\n",
        "### Task\n",
        "Use **Nsight Systems** to profile kernel execution for each stride configuration.\n",
        "\n",
        "---\n",
        "\n",
        "### What to Look For\n",
        "When analyzing the profiling results, focus on:\n",
        "\n",
        "- Kernel execution duration  \n",
        "- GPU utilization  \n",
        "- Differences in memory throughput across strides  \n",
        "- CPUâ€“GPU synchronization overhead  \n",
        "\n",
        "---\n",
        "\n",
        "### Key Questions to Answer\n",
        "- Does kernel execution time increase as the stride increases?  \n",
        "- Is the kernel **memory-bound** or **compute-bound**?  \n",
        "- Are there unnecessary synchronizations or idle GPU periods?\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Outcome\n",
        "You should be able to clearly correlate:\n",
        "\n",
        "> **Poor memory coalescing â†’ more memory transactions â†’ longer kernel runtime**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Deliverables\n",
        "You should produce the following:\n",
        "\n",
        "1. A **CUDA kernel** supporting configurable memory stride  \n",
        "2. **Benchmark results** showing runtime vs. stride  \n",
        "3. **Nsight Systems screenshots or logs**  \n",
        "4. A **short written analysis** explaining:\n",
        "   - Why stride affects performance  \n",
        "   - How memory coalescing explains the observed results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIKS7FYtxIwe",
        "outputId": "327c9455-79fe-43d1-a156-9e698a9ebc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Tue Dec 23 22:23:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVPrSr7t8lQG",
        "outputId": "fadaca06-8720-498c-b8fb-b6d031646783"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQGJrQkmRimn",
        "outputId": "d55c6084-9b9c-407d-c150-04e9a92c650f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting stride_coalescing_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile stride_coalescing_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// -------------------------------------------\n",
        "// TODO: Implement a kernel that uses STRIDED memory access.\n",
        "// Requirements:\n",
        "//  - Each thread computes a global index tid\n",
        "//  - Each thread should operate on indices that depend on `stride`\n",
        "//  - Make sure you do not read/write out-of-bounds\n",
        "//  - You may use either:\n",
        "//      (A) \"logical index\" i in [0, N) and access A[i*stride], or\n",
        "//      (B) a grid-stride loop over i, but with strided addressing\n",
        "//  - The goal is to change memory coalescing behavior as stride changes.\n",
        "// -------------------------------------------\n",
        "__global__ void vectorAddStrided(const float* A, const float* B, float* C, int N, int stride) {\n",
        "    // TODO:\n",
        "    // int tid = ...\n",
        "    // int gridStride = ...\n",
        "    // for (...) { ... }\n",
        "\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int gridStride = blockDim.x * gridDim.x;\n",
        "    if (tid < N){\n",
        "        for(int i = tid; i < N; i += gridStride)\n",
        "        {\n",
        "            int idx = i * stride;\n",
        "            C[idx] = A[idx] + B[idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU reference (for correctness)\n",
        "static void vectorAddStridedCPU(const float* A, const float* B, float* C, int N, int stride) {\n",
        "    // TODO:\n",
        "    // for (int i = 0; i < N; ++i) { ... }\n",
        "    //for(int i = 0; i < N; i++){\n",
        "    //    C[i] = A[i] + B[i];\n",
        "    //}\n",
        "    for(int i = 0; i < N; ++i)\n",
        "    {\n",
        "        int idx = i * stride;\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// -------------------------------------------\n",
        "// TODO: correctness check helper\n",
        "// Requirements:\n",
        "//  - Compare gpu[] vs cpu[] within tolerance\n",
        "//  - Print the first mismatch and return false\n",
        "// -------------------------------------------\n",
        "static bool checkClose(const float* gpu, const float* cpu, int count, float tol) {\n",
        "    // TODO\n",
        "    for (int i = 0; i < count; ++i)\n",
        "    {\n",
        "        float diff = fabsf(gpu[i] - cpu[i]);\n",
        "        if(diff > tol){\n",
        "          return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "// -------------------------------------------\n",
        "// Timing helper (CUDA events)\n",
        "// -------------------------------------------\n",
        "static float timeKernelMs(const float* dA, const float* dB, float* dC, int N, int stride,\n",
        "                          int gridSize, int blockSize, int warmupIters, int iters) {\n",
        "    // Warmup\n",
        "    for (int i = 0; i < warmupIters; ++i) {\n",
        "        vectorAddStrided<<<gridSize, blockSize>>>(dA, dB, dC, N, stride);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < iters; ++i) {\n",
        "        vectorAddStrided<<<gridSize, blockSize>>>(dA, dB, dC, N, stride);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    return ms / iters;\n",
        "}\n",
        "\n",
        "// -------------------------------------------\n",
        "// TODO: optional bandwidth calculation\n",
        "// Tip: estimate bytes moved per *kernel invocation* and convert to GB/s.\n",
        "// -------------------------------------------\n",
        "static double estimateBandwidthGBs(int N, int stride, float kernel_ms) {\n",
        "    // TODO:\n",
        "    // Decide what \"N\" means in your kernel (number of outputs vs logical outputs).\n",
        "    // Compute bytes_read + bytes_written per run.\n",
        "    // return (bytes_total / (kernel_ms/1e3)) / 1e9;\n",
        "    double bytes_total = (double)N * 3.0 * sizeof(float);\n",
        "    double sec = kernel_ms / 1e3;\n",
        "    return (bytes_total / sec) / 1e9;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // -------------------------------------------\n",
        "    // Experiment setup\n",
        "    // N controls how many output elements you compute (your design).\n",
        "    // If you access A[i*stride], make sure the allocated arrays are large enough.\n",
        "    // -------------------------------------------\n",
        "    const int N = 1 << 24;           // base size (tune if needed)\n",
        "    const float tol = 1e-6f;\n",
        "\n",
        "    // We'll test these strides:\n",
        "    const int strides[] = {1, 2, 4};\n",
        "    const int numStrides = sizeof(strides) / sizeof(strides[0]);\n",
        "\n",
        "    // -------------------------------------------\n",
        "    // TODO: Decide how big your arrays must be to support strided access.\n",
        "    // If you access A[i*stride] for i in [0, N), you may need:\n",
        "    //   allocCount = N * stride\n",
        "    // or something similar to avoid out-of-bounds.\n",
        "    // -------------------------------------------\n",
        "    int maxStride = strides[numStrides - 1];\n",
        "    int allocCount = N * maxStride; // TODO (must be >= max index accessed + 1)\n",
        "\n",
        "    size_t bytes = size_t(allocCount) * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytes);\n",
        "    float* hB = (float*)std::malloc(bytes);\n",
        "    float* hC_gpu = (float*)std::malloc(bytes);  // may only need N outputs; up to you\n",
        "    float* hC_cpu = (float*)std::malloc(bytes);\n",
        "\n",
        "    if (!hA || !hB || !hC_gpu || !hC_cpu) {\n",
        "        fprintf(stderr, \"Host allocation failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init\n",
        "    for (int i = 0; i < allocCount; ++i) {\n",
        "        hA[i] = 0.001f * i;\n",
        "        hB[i] = 0.002f * i;\n",
        "    }\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA = nullptr, *dB = nullptr, *dC = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, bytes));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytes, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // -------------------------------------------\n",
        "    // TODO: Choose launch config\n",
        "    // -------------------------------------------\n",
        "    int blockSize = 256; // TODO (e.g., 256)\n",
        "    int gridSize  = (N + blockSize - 1)/blockSize; // TODO (e.g., (N + blockSize - 1)/blockSize or a fixed value)\n",
        "\n",
        "    // Timing params\n",
        "    const int warmupIters = 5;\n",
        "    const int iters = 20;\n",
        "\n",
        "    printf(\"=== Stride Coalescing Experiment ===\\n\");\n",
        "    printf(\"N=%d, allocCount=%d, blockSize=%d, gridSize=%d\\n\", N, allocCount, blockSize, gridSize);\n",
        "\n",
        "    for (int s = 0; s < numStrides; ++s) {\n",
        "        int stride = strides[s];\n",
        "\n",
        "        // Optional: clear output\n",
        "        CUDA_CHECK(cudaMemset(dC, 0, bytes));\n",
        "\n",
        "        // Measure\n",
        "        float ms = timeKernelMs(dA, dB, dC, N, stride, gridSize, blockSize, warmupIters, iters);\n",
        "\n",
        "        // Copy back (you may only need first N outputs â€” your choice)\n",
        "        CUDA_CHECK(cudaMemcpy(hC_gpu, dC, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "        // CPU reference\n",
        "        vectorAddStridedCPU(hA, hB, hC_cpu, N, stride);\n",
        "\n",
        "        // TODO: correctness check (decide how many outputs are valid)\n",
        "        int checkCount = N; // TODO (e.g., N or allocCount depending on your design)\n",
        "        bool ok = checkClose(hC_gpu, hC_cpu, checkCount, tol);\n",
        "\n",
        "        // Optional bandwidth estimate\n",
        "        double gbs = estimateBandwidthGBs(N, stride, ms);\n",
        "\n",
        "        printf(\"[stride=%d] time=%.4f ms | bandwidth=%.2f GB/s | correctness=%s\\n\",\n",
        "               stride, ms, gbs, ok ? \"PASS\" : \"FAIL\");\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hC_gpu);\n",
        "    std::free(hC_cpu);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt50ev9vSOsg",
        "outputId": "e1d4da6e-64af-4146-919f-42c1b23d5c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Stride Coalescing Experiment ===\n",
            "N=16777216, allocCount=67108864, blockSize=256, gridSize=65536\n",
            "[stride=1] time=0.7965 ms | bandwidth=252.78 GB/s | correctness=PASS\n",
            "[stride=2] time=1.9959 ms | bandwidth=100.87 GB/s | correctness=PASS\n",
            "[stride=4] time=4.1621 ms | bandwidth=48.37 GB/s | correctness=PASS\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 stride_coalescing_skeleton.cu -o stride_coalescing_skeleton\n",
        "!./stride_coalescing_skeleton"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
