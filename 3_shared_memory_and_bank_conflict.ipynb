{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGCpFpA6dxz4"
      },
      "source": [
        "# üìò Task Explanation: Shared Memory and Bank Conflicts in CUDA\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to understand how **CUDA shared memory** works, why it is significantly faster than global memory, and how **bank conflicts** can degrade its performance.  \n",
        "You will then implement a **shared-memory tile copy kernel** to practice using shared memory correctly and efficiently.\n",
        "\n",
        "This task introduces one of the most important performance concepts in CUDA and GPU programming.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is Shared Memory?\n",
        "Shared memory is a **small, on-chip memory** that is:\n",
        "- Shared by all threads within the same thread block\n",
        "- Much faster than global memory\n",
        "- Manually managed by the programmer\n",
        "\n",
        "Typical characteristics:\n",
        "- Low latency (similar to registers)\n",
        "- Limited size (e.g., 48‚Äì64 KB per SM)\n",
        "- Lifetime = one thread block\n",
        "\n",
        "Shared memory is widely used in:\n",
        "- Matrix multiplication (tiling)\n",
        "- Reductions\n",
        "- Convolutions\n",
        "- Normalization layers (LayerNorm, BatchNorm)\n",
        "- Attention kernels\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Understand Bank Conflicts\n",
        "\n",
        "### What Are Memory Banks?\n",
        "Shared memory is divided into multiple **banks** (typically 32 banks).  \n",
        "Each bank can service **one access per cycle**.\n",
        "\n",
        "- If threads in a warp access **different banks** ‚Üí no conflict  \n",
        "- If multiple threads access the **same bank** ‚Üí **bank conflict**\n",
        "\n",
        "### Why Bank Conflicts Matter\n",
        "Bank conflicts cause:\n",
        "- Serialized memory accesses\n",
        "- Increased latency\n",
        "- Lower effective bandwidth\n",
        "\n",
        "Even though shared memory is fast, **poor access patterns can make it slow**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Example Concept (No Code Required)\n",
        "You should be able to reason about:\n",
        "- Why accessing `shared[threadIdx.x]` is conflict-free\n",
        "- Why accessing `shared[threadIdx.x * stride]` may cause conflicts\n",
        "- How padding shared memory can eliminate conflicts\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Implement a Shared-Memory Tile Copy Kernel\n",
        "\n",
        "### Task\n",
        "Write a CUDA kernel that:\n",
        "1. Loads a **tile** of data from global memory into shared memory  \n",
        "2. Synchronizes threads within the block  \n",
        "3. Writes the tile back to global memory  \n",
        "\n",
        "The kernel should:\n",
        "- Use shared memory as an intermediate buffer\n",
        "- Correctly synchronize threads using `__syncthreads()`\n",
        "- Avoid out-of-bounds memory access\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why a Tile Copy Kernel?\n",
        "A tile copy kernel is a **minimal example** that demonstrates:\n",
        "- Correct shared memory usage\n",
        "- Thread cooperation within a block\n",
        "- The impact of access patterns on shared memory performance\n",
        "\n",
        "This pattern is a building block for:\n",
        "- Matrix multiplication\n",
        "- Transpose kernels\n",
        "- Convolution kernels\n",
        "- ML operators using tiling\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Pitfalls to Watch For\n",
        "- Forgetting `__syncthreads()` after loading shared memory  \n",
        "- Accessing shared memory with patterns that cause bank conflicts  \n",
        "- Using incorrect shared memory indexing  \n",
        "- Reading/writing out of bounds  \n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A CUDA kernel that copies data using shared memory tiling  \n",
        "2. A correctness check comparing input and output  \n",
        "3. (Optional) A benchmark comparing:\n",
        "   - Direct global memory copy  \n",
        "   - Shared-memory tile copy  \n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How shared memory differs from global memory\n",
        "- How bank conflicts arise and why they hurt performance\n",
        "- How to design shared-memory access patterns\n",
        "- Why padding and layout matter in GPU kernels\n",
        "\n",
        "These skills are essential for:\n",
        "- CUDA optimization\n",
        "- ML kernel development\n",
        "- Writing high-performance GPU code\n",
        "- Understanding libraries like cuBLAS, cuDNN, and Triton\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Many ML kernels rely heavily on shared memory:\n",
        "- Matrix multiply uses tiled shared memory\n",
        "- LayerNorm uses shared memory for reductions\n",
        "- Attention kernels reuse data via shared memory\n",
        "\n",
        "Mastering shared memory and bank conflicts is a **core skill for ML Systems Engineers and GPU Kernel Engineers**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUUbnTxIOKIv",
        "outputId": "7b060fc6-5fe3-4fc0-989d-be8471558ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Fri Dec 26 21:49:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7skVvROMGD",
        "outputId": "9c0c1b72-f234-4f37-90d4-e5436f4a2705"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxRcNMC7ds_j",
        "outputId": "dd3097af-2875-4f6e-857a-88ffe3cf7056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting shared_mem_tile_copy_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile shared_mem_tile_copy_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (Task): Shared-memory tile copy kernel\n",
        "// Goal:\n",
        "//  - Copy input array A -> output array B using shared memory tiles.\n",
        "// Requirements:\n",
        "//  - Each block loads a tile from global memory into shared memory.\n",
        "//  - Synchronize threads (barrier) before writing back.\n",
        "//  - Write the tile from shared memory to global memory.\n",
        "//  - Avoid out-of-bounds accesses.\n",
        "// Notes:\n",
        "//  - Start with 1D tiling (simpler) or extend to 2D tiling (optional).\n",
        "// ------------------------------------------------------------\n",
        "__global__ void tileCopyShared(const float* __restrict__ A,\n",
        "                               float* __restrict__ B,\n",
        "                               int N) {\n",
        "    // TODO: declare shared memory tile\n",
        "    // Example (static): __shared__ float tile[TILE_SIZE];\n",
        "    // Or (dynamic): extern __shared__ float tile[];\n",
        "    extern __shared__ float tile[];\n",
        "\n",
        "    // TODO: compute global index\n",
        "    // int tid = ...\n",
        "    // int gid = ...\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // TODO: load from global -> shared (guarded)\n",
        "    // if (...) tile[...] = A[...];\n",
        "    if(tid < N){\n",
        "      tile[tid] = A[gid];\n",
        "    }\n",
        "\n",
        "    // TODO: synchronize\n",
        "    // __syncthreads();\n",
        "    __syncthreads();\n",
        "\n",
        "    // TODO: store from shared -> global (guarded)\n",
        "    // if (...) B[...] = tile[...];\n",
        "    if (gid < N){\n",
        "        B[gid] = tile[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// CPU reference copy\n",
        "// ------------------------------------------------------------\n",
        "static void copyCPU(const float* A, float* B, int N) {\n",
        "    for (int i = 0; i < N; ++i) B[i] = A[i];\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: correctness checker\n",
        "// Requirements:\n",
        "//  - Compare arrays elementwise within tolerance\n",
        "//  - Print first mismatch and return false\n",
        "// ------------------------------------------------------------\n",
        "static bool checkClose(const float* out, const float* ref, int N, float tol) {\n",
        "    // TODO\n",
        "    for(int i = 0; i < N; ++i){\n",
        "        float diff = fabsf(out[i] - ref[i]);\n",
        "        if (diff > tol){\n",
        "          return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 24; // ~16M floats\n",
        "    const float tol = 1e-6f;\n",
        "    const size_t bytes = size_t(N) * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytes);\n",
        "    float* hB = (float*)std::malloc(bytes);\n",
        "    float* hRef = (float*)std::malloc(bytes);\n",
        "\n",
        "    if (!hA || !hB || !hRef) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init input\n",
        "    for (int i = 0; i < N; ++i) hA[i] = 0.001f * i;\n",
        "\n",
        "    // CPU reference\n",
        "    copyCPU(hA, hRef, N);\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA = nullptr, *dB = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytes));\n",
        "\n",
        "    // H2D\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Choose launch configuration\n",
        "    // - Decide TILE_SIZE (or blockSize)\n",
        "    // - Decide gridSize\n",
        "    // - If using dynamic shared memory, pass sharedMemBytes\n",
        "    // ------------------------------------------------------------\n",
        "    int blockSize = 256;     // TODO\n",
        "    int gridSize  = (N + blockSize - 1) / blockSize;     // TODO\n",
        "    size_t sharedMemBytes = blockSize * sizeof(float); // TODO (0 if using static shared mem)\n",
        "\n",
        "    // Launch\n",
        "    tileCopyShared<<<gridSize, blockSize, sharedMemBytes>>>(dA, dB, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // D2H\n",
        "    CUDA_CHECK(cudaMemcpy(hB, dB, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Correctness check\n",
        "    // ------------------------------------------------------------\n",
        "    bool ok = checkClose(hB, hRef, N, tol);\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hRef);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnXqPl5beOP9",
        "outputId": "49b6db60-a9b5-43e7-cf52-5ebdba5b33e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correctness: PASS\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 shared_mem_tile_copy_skeleton.cu -o tile_copy\n",
        "!./tile_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7x5FLZnenky"
      },
      "source": [
        "# üìò Task Explanation: Shared Memory Bank-Conflict Experiment\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to **understand, observe, and quantify shared-memory bank conflicts** in CUDA.  \n",
        "You will design controlled experiments that access shared memory with different patterns and measure how **bank conflicts impact kernel performance**.\n",
        "\n",
        "This task builds intuition about why shared memory can be either **extremely fast or unexpectedly slow**, depending on access patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is a Bank Conflict?\n",
        "Shared memory on NVIDIA GPUs is divided into **multiple memory banks** (typically 32 banks).\n",
        "\n",
        "- Each bank can service **one access per cycle**\n",
        "- Threads in a **warp (32 threads)** access shared memory simultaneously\n",
        "\n",
        "### Access Scenarios\n",
        "- **Conflict-free**: each thread accesses a different bank  \n",
        "- **Bank conflict**: two or more threads access the same bank  \n",
        "- **Result**: accesses are serialized ‚Üí increased latency\n",
        "\n",
        "Even though shared memory is on-chip, **bank conflicts can significantly degrade performance**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Design Conflict-Free Shared Memory Access\n",
        "\n",
        "### Task\n",
        "Create a kernel where each thread accesses shared memory in a **conflict-free pattern**, for example:\n",
        "\n",
        "- Thread `t` accesses `shared[t]`\n",
        "- Consecutive threads map to consecutive banks\n",
        "\n",
        "### Goal\n",
        "Establish a **performance baseline** where shared memory is used optimally.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Introduce Bank Conflicts via Strided Access\n",
        "\n",
        "### Task\n",
        "Modify the kernel so that threads access shared memory with a **stride**, such as:\n",
        "\n",
        "- Thread `t` accesses `shared[t * stride]`\n",
        "\n",
        "Test multiple stride values (e.g., 1, 2, 4, 8).\n",
        "\n",
        "### Expected Effect\n",
        "As the stride increases:\n",
        "- More threads map to the same bank\n",
        "- The number of bank conflicts increases\n",
        "- Kernel execution time increases\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Eliminate Bank Conflicts Using Padding\n",
        "\n",
        "### Task\n",
        "Redesign the shared memory layout by adding **padding**, such as:\n",
        "\n",
        "- Declaring shared memory with extra elements\n",
        "- Changing indexing so threads map to different banks\n",
        "\n",
        "### Goal\n",
        "Demonstrate that **bank conflicts are a layout problem**, not an inherent limitation of shared memory.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Measure\n",
        "For each configuration (conflict-free, conflicted, padded):\n",
        "\n",
        "- Kernel execution time\n",
        "- (Optional) Effective shared memory bandwidth\n",
        "- Relative slowdown compared to conflict-free case\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Observations\n",
        "\n",
        "| Access Pattern | Bank Conflicts | Performance |\n",
        "|---------------|---------------|-------------|\n",
        "| Contiguous (`shared[t]`) | None | Fastest |\n",
        "| Strided (`shared[t*2]`) | Moderate | Slower |\n",
        "| Highly strided (`shared[t*4]`) | Severe | Much slower |\n",
        "| Padded layout | Eliminated | Near baseline |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- How does stride affect bank mapping?\n",
        "- Why does padding eliminate conflicts?\n",
        "- Why do bank conflicts serialize memory access?\n",
        "- How do these effects scale with block size?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "\n",
        "1. A CUDA kernel demonstrating **conflict-free shared memory access**\n",
        "2. A CUDA kernel demonstrating **bank-conflicted access**\n",
        "3. A CUDA kernel demonstrating **conflict elimination via padding**\n",
        "4. Benchmark results comparing all three cases\n",
        "5. A short written analysis explaining:\n",
        "   - How bank conflicts arise\n",
        "   - Why padding works\n",
        "   - How performance changes across configurations\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "\n",
        "- How shared memory banks work\n",
        "- Why access patterns matter more than raw computation\n",
        "- How to diagnose shared memory performance issues\n",
        "- How to design bank-conflict-free shared memory layouts\n",
        "\n",
        "These skills are critical for:\n",
        "- CUDA optimization\n",
        "- Matrix multiplication and transpose kernels\n",
        "- Reductions and prefix sums\n",
        "- ML kernels using shared memory (LayerNorm, Attention)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Many high-performance ML kernels rely heavily on shared memory:\n",
        "\n",
        "- GEMM tiling in cuBLAS\n",
        "- LayerNorm and BatchNorm reductions\n",
        "- Attention and FlashAttention kernels\n",
        "\n",
        "Understanding and eliminating bank conflicts is a **core competency for ML Systems Engineers and GPU Kernel Engineers**.\n",
        "\n",
        "This task trains the same reasoning used in real-world CUDA and Triton kernel optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH_nTlqZewg3",
        "outputId": "ba6a580f-bd64-4021-d4da-e75586833aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting bank_conflict_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile bank_conflict_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "__global__ void bankConflictKernel(float* out, int N, int mode, int stride, int iters) {\n",
        "    extern __shared__ float smem[];\n",
        "\n",
        "    int gid  = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int lane = threadIdx.x;\n",
        "\n",
        "    // init: one element per thread for mode 0/1\n",
        "    if (lane < blockDim.x) smem[lane] = (float)lane;\n",
        "    __syncthreads();\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    for (int k = 0; k < iters; ++k) {\n",
        "        if (mode == 0) {\n",
        "            // baseline\n",
        "            int idx = lane;\n",
        "            acc += smem[idx];\n",
        "\n",
        "        } else if (mode == 1) {\n",
        "            // strided (wrap to avoid OOB)\n",
        "            int idx = (lane * stride) % blockDim.x;\n",
        "            acc += smem[idx];\n",
        "\n",
        "        } else {\n",
        "            // padded tile experiment using only first 32 threads\n",
        "            // to keep shared memory small and demonstrate bank behavior clearly\n",
        "            const int TILE = 32;\n",
        "            const int pitch = TILE + 1; // padding\n",
        "\n",
        "            if (lane < TILE) {\n",
        "                // layout uses first TILE*pitch floats in smem\n",
        "                // treat as tile[row][col]\n",
        "                int row = lane;\n",
        "                int col = (k * stride) % TILE;\n",
        "\n",
        "                int idx = row * pitch + col;  // within TILE*(TILE+1)\n",
        "                acc += smem[idx];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (gid < N) out[gid] = acc;\n",
        "}\n",
        "\n",
        "static float timeKernelMs(float* dOut, int N, int mode, int stride, int iters,\n",
        "                          int gridSize, int blockSize, size_t sharedBytes,\n",
        "                          int warmup, int runs) {\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        bankConflictKernel<<<gridSize, blockSize, sharedBytes>>>(dOut, N, mode, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        bankConflictKernel<<<gridSize, blockSize, sharedBytes>>>(dOut, N, mode, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "    return ms / runs;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 20;\n",
        "    const int blockSize = 256;\n",
        "    const int gridSize  = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    const int strides[] = {1, 2, 4, 8, 16, 32};\n",
        "    const int numStrides = (int)(sizeof(strides) / sizeof(strides[0]));\n",
        "\n",
        "    const int iters  = 2048;\n",
        "    const int warmup = 5;\n",
        "    const int runs   = 20;\n",
        "\n",
        "    // shared memory bytes:\n",
        "    // need at least blockSize floats for mode 0/1\n",
        "    // and at least TILE*(TILE+1) floats for mode 2\n",
        "    const int TILE = 32;\n",
        "    size_t needFloats = (size_t)blockSize;\n",
        "    size_t paddedFloats = (size_t)TILE * (size_t)(TILE + 1);\n",
        "    if (paddedFloats > needFloats) needFloats = paddedFloats;\n",
        "    size_t sharedBytes = needFloats * sizeof(float);\n",
        "\n",
        "    float* dOut = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, (size_t)N * sizeof(float)));\n",
        "\n",
        "    printf(\"=== Bank Conflict Experiment (fixed) ===\\n\");\n",
        "    printf(\"N=%d, grid=%d, block=%d, iters=%d, sharedBytes=%.2f KB\\n\",\n",
        "           N, gridSize, blockSize, iters, sharedBytes / 1024.0);\n",
        "\n",
        "    for (int s = 0; s < numStrides; ++s) {\n",
        "        int stride = strides[s];\n",
        "\n",
        "        for (int mode = 0; mode <= 2; ++mode) {\n",
        "            float ms = timeKernelMs(dOut, N, mode, stride, iters,\n",
        "                                    gridSize, blockSize, sharedBytes,\n",
        "                                    warmup, runs);\n",
        "\n",
        "            const char* name =\n",
        "                (mode == 0) ? \"baseline\" :\n",
        "                (mode == 1) ? \"strided\"  :\n",
        "                              \"padded32x33\";\n",
        "\n",
        "            printf(\"[stride=%2d][mode=%d:%s] time=%.4f ms\\n\", stride, mode, name, ms);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5FImTMmIyyn",
        "outputId": "cf34764a-24a3-4c25-ea26-04a72ab0c572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Bank Conflict Experiment (fixed) ===\n",
            "N=1048576, grid=4096, block=256, iters=2048, sharedBytes=4.12 KB\n",
            "[stride= 1][mode=0:baseline] time=1.4614 ms\n",
            "[stride= 1][mode=1:strided] time=1.5553 ms\n",
            "[stride= 1][mode=2:padded32x33] time=5.3495 ms\n",
            "\n",
            "[stride= 2][mode=0:baseline] time=0.9506 ms\n",
            "[stride= 2][mode=1:strided] time=1.0115 ms\n",
            "[stride= 2][mode=2:padded32x33] time=3.8302 ms\n",
            "\n",
            "[stride= 4][mode=0:baseline] time=0.5552 ms\n",
            "[stride= 4][mode=1:strided] time=0.6016 ms\n",
            "[stride= 4][mode=2:padded32x33] time=3.0060 ms\n",
            "\n",
            "[stride= 8][mode=0:baseline] time=0.5602 ms\n",
            "[stride= 8][mode=1:strided] time=0.6188 ms\n",
            "[stride= 8][mode=2:padded32x33] time=2.9954 ms\n",
            "\n",
            "[stride=16][mode=0:baseline] time=0.5604 ms\n",
            "[stride=16][mode=1:strided] time=0.6213 ms\n",
            "[stride=16][mode=2:padded32x33] time=2.9825 ms\n",
            "\n",
            "[stride=32][mode=0:baseline] time=0.5602 ms\n",
            "[stride=32][mode=1:strided] time=0.6209 ms\n",
            "[stride=32][mode=2:padded32x33] time=3.0003 ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 bank_conflict_skeleton.cu -o bank_conflict_skeleton\n",
        "!./bank_conflict_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiDeEOp-osqv",
        "outputId": "00af337a-1854-48a3-a741-5afe35c57d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing bank_conflict_clean.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile bank_conflict_clean.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// blockDim should be (32, 8) or (32, 16)\n",
        "__global__ void bankConflictClean(float* out, int iters, int mode) {\n",
        "    // We'll use a 32x32 tile. For padding mode, pitch=33.\n",
        "    extern __shared__ float smem[];\n",
        "\n",
        "    int tx = threadIdx.x; // 0..31\n",
        "    int ty = threadIdx.y; // 0..(BLOCK_ROWS-1)\n",
        "\n",
        "    const int TILE = 32;\n",
        "    const int pitch = (mode == 2) ? (TILE + 1) : TILE; // padding only in mode=2\n",
        "\n",
        "    // Initialize: each thread writes multiple rows (like transpose)\n",
        "    for (int j = 0; j < TILE; j += blockDim.y) {\n",
        "        int row = ty + j;\n",
        "        smem[row * pitch + tx] = (float)(row * 0.1f + tx);\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    for (int k = 0; k < iters; ++k) {\n",
        "        // choose a column that changes over time (avoid cache weirdness)\n",
        "        int col = (k & 31);\n",
        "\n",
        "        int idx;\n",
        "        if (mode == 0) {\n",
        "            // Row access (conflict-free-ish): smem[ty][tx]\n",
        "            idx = ty * pitch + tx;\n",
        "        } else {\n",
        "            // Column access (classic conflict when pitch=32):\n",
        "            // smem[tx][col] -> threads in warp differ in row (tx), same col\n",
        "            idx = tx * pitch + col;\n",
        "        }\n",
        "        acc += smem[idx];\n",
        "    }\n",
        "\n",
        "    // write something\n",
        "    int gid = (blockIdx.x * blockDim.x * blockDim.y) + (ty * blockDim.x + tx);\n",
        "    out[gid] = acc;\n",
        "}\n",
        "\n",
        "static float timeMs(float* dOut, int iters, int mode,\n",
        "                    dim3 grid, dim3 block, size_t shBytes, int warmup, int runs) {\n",
        "    for (int i=0;i<warmup;i++) bankConflictClean<<<grid, block, shBytes>>>(dOut, iters, mode);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t s,e;\n",
        "    CUDA_CHECK(cudaEventCreate(&s));\n",
        "    CUDA_CHECK(cudaEventCreate(&e));\n",
        "    CUDA_CHECK(cudaEventRecord(s));\n",
        "    for (int i=0;i<runs;i++) bankConflictClean<<<grid, block, shBytes>>>(dOut, iters, mode);\n",
        "    CUDA_CHECK(cudaEventRecord(e));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(e));\n",
        "\n",
        "    float ms=0;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms,s,e));\n",
        "    CUDA_CHECK(cudaEventDestroy(s));\n",
        "    CUDA_CHECK(cudaEventDestroy(e));\n",
        "    return ms/runs;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int iters = 4096;\n",
        "    const int warmup=5, runs=20;\n",
        "\n",
        "    dim3 block(32, 8);\n",
        "    dim3 grid(256);\n",
        "\n",
        "    // Allocate enough for the padded case: 32*(33) floats\n",
        "    size_t shBytes = 32 * 33 * sizeof(float);\n",
        "\n",
        "    float* dOut=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, grid.x * block.x * block.y * sizeof(float)));\n",
        "\n",
        "    printf(\"=== Clean Bank Conflict Demo ===\\n\");\n",
        "    printf(\"block=(%d,%d) grid=%d iters=%d shmem=%.2fKB\\n\",\n",
        "           block.x, block.y, (int)grid.x, iters, shBytes/1024.0);\n",
        "\n",
        "    // mode 0: row access\n",
        "    float t0 = timeMs(dOut, iters, 0, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    // mode 1: column access with pitch=32 -> conflicts\n",
        "    float t1 = timeMs(dOut, iters, 1, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    // mode 2: column access with pitch=33 -> padding removes conflicts\n",
        "    float t2 = timeMs(dOut, iters, 2, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    printf(\"mode0 row(no conflict) : %.4f ms\\n\", t0);\n",
        "    printf(\"mode1 col(pitch=32)    : %.4f ms (conflict)\\n\", t1);\n",
        "    printf(\"mode2 col(pitch=33)    : %.4f ms (padded)\\n\", t2);\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7w2NbMxqIEY",
        "outputId": "9fa8f67b-c858-4bd0-f2ee-1707c966dbb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Clean Bank Conflict Demo ===\n",
            "block=(32,8) grid=256 iters=4096 shmem=4.12KB\n",
            "mode0 row(no conflict) : 0.2050 ms\n",
            "mode1 col(pitch=32)    : 12.6755 ms (conflict)\n",
            "mode2 col(pitch=33)    : 0.2935 ms (padded)\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 bank_conflict_clean.cu -o bank_conflict_clean\n",
        "!./bank_conflict_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtH6dJ49fdzo"
      },
      "source": [
        "# üìò Follow-Up Task Explanation: Shared-Memory Matrix Multiplication (Tiled GEMM)\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to implement **matrix multiplication (GEMM)** using **shared memory tiling** in CUDA and understand **why shared memory dramatically improves performance** over a naive global-memory implementation.\n",
        "\n",
        "This task builds directly on:\n",
        "- Shared memory fundamentals\n",
        "- Thread cooperation within a block\n",
        "- Bank-conflict awareness\n",
        "- Memory coalescing concepts\n",
        "\n",
        "It is one of the **most important CUDA optimization patterns** and a foundation for ML workloads.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: Why Matrix Multiplication Matters\n",
        "Matrix multiplication is the computational core of:\n",
        "- Neural network layers (Linear, Attention, MLP)\n",
        "- Convolutions (after im2col)\n",
        "- Scientific computing and simulations\n",
        "\n",
        "A naive CUDA implementation is usually **memory-bound** because:\n",
        "- Each matrix element is loaded repeatedly from global memory\n",
        "- Global memory latency dominates execution time\n",
        "\n",
        "Shared-memory tiling solves this problem by **reusing data**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Naive Matrix Multiplication (Baseline)\n",
        "\n",
        "### Task\n",
        "Implement a naive CUDA matrix multiplication kernel:\n",
        "\n",
        "\\[\n",
        "C_{ij} = \\sum_k A_{ik} \\cdot B_{kj}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- Each thread computes **one element** of matrix `C`\n",
        "- All reads come directly from **global memory**\n",
        "\n",
        "### Goal\n",
        "Establish a **performance baseline** to compare against the optimized version.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Shared-Memory Tiled Matrix Multiplication\n",
        "\n",
        "### Task\n",
        "Rewrite the kernel using **shared memory tiling**:\n",
        "\n",
        "1. Divide matrices into square **tiles** (e.g., 16√ó16 or 32√ó32)\n",
        "2. Each thread block:\n",
        "   - Loads one tile of `A` and one tile of `B` into shared memory\n",
        "3. Synchronize threads\n",
        "4. Compute partial dot products using shared memory\n",
        "5. Repeat for all tiles along the K dimension\n",
        "\n",
        "### Key Requirements\n",
        "- Use `__shared__` memory for tiles\n",
        "- Use `__syncthreads()` correctly\n",
        "- Ensure global memory loads are coalesced\n",
        "- Avoid shared-memory bank conflicts\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Tiling Works\n",
        "Shared-memory tiling:\n",
        "- Reduces global memory loads by **reuse**\n",
        "- Converts many slow global reads into fast shared reads\n",
        "- Increases arithmetic intensity (FLOPs per byte)\n",
        "\n",
        "This is the core idea behind **cuBLAS**, **Tensor Cores**, and most ML kernels.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Tile Size and Performance Tuning\n",
        "\n",
        "### Task\n",
        "Experiment with different tile sizes:\n",
        "- 16√ó16\n",
        "- 32√ó32\n",
        "\n",
        "Observe how tile size affects:\n",
        "- Performance\n",
        "- Occupancy\n",
        "- Shared memory usage\n",
        "\n",
        "### Key Trade-offs\n",
        "- Larger tiles ‚Üí more reuse, but higher shared memory usage\n",
        "- Smaller tiles ‚Üí lower shared memory, but less reuse\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Measure\n",
        "For both naive and tiled kernels, measure:\n",
        "- Kernel execution time\n",
        "- Speedup of tiled version over naive\n",
        "- (Optional) Effective FLOPs or bandwidth\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Observations\n",
        "\n",
        "| Kernel Type | Memory Behavior | Performance |\n",
        "|------------|-----------------|-------------|\n",
        "| Naive | Repeated global loads | Slow |\n",
        "| Tiled (shared memory) | Data reuse in shared memory | Much faster |\n",
        "| Tuned tile size | Balanced reuse & occupancy | Best |\n",
        "\n",
        "A correct tiled implementation should achieve **multiple-X speedup** over the naive version.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Pitfalls\n",
        "- Forgetting `__syncthreads()` after loading tiles\n",
        "- Incorrect indexing into shared memory\n",
        "- Out-of-bounds accesses at matrix edges\n",
        "- Bank conflicts in shared memory tiles\n",
        "- Choosing tile sizes that exceed shared memory limits\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A **naive CUDA matrix multiplication kernel**\n",
        "2. A **shared-memory tiled matrix multiplication kernel**\n",
        "3. Benchmark results comparing both implementations\n",
        "4. A short analysis explaining:\n",
        "   - Why the tiled version is faster\n",
        "   - How shared memory reuse improves performance\n",
        "   - How tile size affects efficiency\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to design cooperative thread-block algorithms\n",
        "- How shared memory enables data reuse\n",
        "- How memory hierarchy affects performance\n",
        "- How real ML kernels are structured internally\n",
        "\n",
        "This task is a **cornerstone skill** for:\n",
        "- CUDA optimization\n",
        "- ML kernel engineering\n",
        "- Understanding cuBLAS and Triton matmul\n",
        "- LLM training and inference acceleration\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Shared-memory matrix multiplication is the conceptual basis for:\n",
        "- cuBLAS GEMM\n",
        "- Tensor Core operations\n",
        "- Attention QK·µÄ and QKV kernels\n",
        "- Triton and compiler-generated GPU kernels\n",
        "\n",
        "Mastering this task means you are thinking like a **GPU kernel engineer**, not just a CUDA user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3-JyMOIfedi"
      },
      "outputs": [],
      "source": [
        "%%writefile tiled_gemm_shared_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (Task): Shared-memory tiled matrix multiplication (GEMM)\n",
        "// Compute C = A * B\n",
        "//\n",
        "// Shapes (row-major):\n",
        "//   A: M x K\n",
        "//   B: K x N\n",
        "//   C: M x N\n",
        "//\n",
        "// Requirements:\n",
        "//  - Each thread computes one (row, col) element of C\n",
        "//  - Use shared memory tiling for A and B\n",
        "//  - Use __syncthreads() correctly\n",
        "//  - Avoid out-of-bounds at edges (M,N,K may not be multiples of TILE)\n",
        "//  - Coalesced global loads for tiles (best effort)\n",
        "// ------------------------------------------------------------\n",
        "\n",
        "// TODO: choose a tile size (e.g., 16 or 32)\n",
        "#ifndef TILE\n",
        "#define TILE 0  // TODO\n",
        "#endif\n",
        "\n",
        "__global__ void matmulTiledShared(const float* __restrict__ A,\n",
        "                                  const float* __restrict__ B,\n",
        "                                  float* __restrict__ C,\n",
        "                                  int M, int N, int K) {\n",
        "    // TODO: declare shared memory tiles\n",
        "    // __shared__ float As[TILE][TILE];\n",
        "    // __shared__ float Bs[TILE][TILE];\n",
        "\n",
        "    // TODO: compute global row/col for this thread\n",
        "    // int row = ...\n",
        "    // int col = ...\n",
        "\n",
        "    // TODO: accumulator\n",
        "    // float acc = 0.0f;\n",
        "\n",
        "    // TODO: loop over tiles along K dimension\n",
        "    // for (int t = 0; t < ...; ++t) {\n",
        "    //   - load A tile element into As (guarded)\n",
        "    //   - load B tile element into Bs (guarded)\n",
        "    //   - __syncthreads()\n",
        "    //   - compute partial dot product over k in [0, TILE)\n",
        "    //   - __syncthreads()\n",
        "    // }\n",
        "\n",
        "    // TODO: write C[row, col] (guarded)\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// CPU reference GEMM (for correctness)\n",
        "// ------------------------------------------------------------\n",
        "static void matmulCPU(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            float acc = 0.0f;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                acc += A[i * K + k] * B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = acc;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: correctness checker\n",
        "// Requirements:\n",
        "//  - Compare C_gpu vs C_ref within tolerance\n",
        "//  - Print first mismatch (i, value_gpu, value_ref)\n",
        "// ------------------------------------------------------------\n",
        "static bool checkClose(const float* gpu, const float* ref, int count, float tol) {\n",
        "    // TODO\n",
        "    return false;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Timing helper (CUDA events) - implemented\n",
        "// ------------------------------------------------------------\n",
        "static float timeGemmMs(const float* dA, const float* dB, float* dC,\n",
        "                        int M, int N, int K,\n",
        "                        dim3 grid, dim3 block,\n",
        "                        int warmup, int iters) {\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < iters; ++i) {\n",
        "        matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    return ms / iters;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (optional): estimate GFLOPS\n",
        "// GEMM does ~2*M*N*K FLOPs\n",
        "// GFLOPS = FLOPs / time_seconds / 1e9\n",
        "// ------------------------------------------------------------\n",
        "static double estimateGFLOPS(int M, int N, int K, float kernel_ms) {\n",
        "    // TODO\n",
        "    return 0.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ------------------------------------------------------------\n",
        "    // Sizes (you can tune these)\n",
        "    // NOTE: try sizes that are NOT multiples of TILE to test edges.\n",
        "    // ------------------------------------------------------------\n",
        "    const int M = 1024;\n",
        "    const int N = 1024;\n",
        "    const int K = 1024;\n",
        "\n",
        "    const float tol = 1e-3f; // TODO: choose tolerance (float accumulation)\n",
        "    const size_t bytesA = size_t(M) * K * sizeof(float);\n",
        "    const size_t bytesB = size_t(K) * N * sizeof(float);\n",
        "    const size_t bytesC = size_t(M) * N * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytesA);\n",
        "    float* hB = (float*)std::malloc(bytesB);\n",
        "    float* hC_gpu = (float*)std::malloc(bytesC);\n",
        "    float* hC_ref = (float*)std::malloc(bytesC);\n",
        "\n",
        "    if (!hA || !hB || !hC_gpu || !hC_ref) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init A, B\n",
        "    for (int i = 0; i < M * K; ++i) hA[i] = 0.001f * (i % 1000);\n",
        "    for (int i = 0; i < K * N; ++i) hB[i] = 0.002f * (i % 1000);\n",
        "\n",
        "    // CPU reference (can be slow for large sizes; reduce sizes if needed)\n",
        "    matmulCPU(hA, hB, hC_ref, M, N, K);\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA=nullptr, *dB=nullptr, *dC=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytesA));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytesB));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, bytesC));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytesA, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB, bytesB, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Choose launch config based on TILE\n",
        "    // Typical:\n",
        "    //   block = (TILE, TILE)\n",
        "    //   grid  = (ceil(N/TILE), ceil(M/TILE))\n",
        "    // ------------------------------------------------------------\n",
        "    dim3 block(0, 0, 1); // TODO\n",
        "    dim3 grid(0, 0, 1);  // TODO\n",
        "\n",
        "    // Launch once (correctness)\n",
        "    matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(hC_gpu, dC, bytesC, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: correctness check\n",
        "    // ------------------------------------------------------------\n",
        "    bool ok = checkClose(hC_gpu, hC_ref, M * N, tol);\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // Timing\n",
        "    const int warmup = 5;\n",
        "    const int iters = 20;\n",
        "    float ms = timeGemmMs(dA, dB, dC, M, N, K, grid, block, warmup, iters);\n",
        "\n",
        "    // Optional: GFLOPS\n",
        "    double gflops = estimateGFLOPS(M, N, K, ms);\n",
        "\n",
        "    printf(\"GEMM: M=%d N=%d K=%d | time=%.4f ms | GFLOPS=%.2f\\n\", M, N, K, ms, gflops);\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hC_gpu);\n",
        "    std::free(hC_ref);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
