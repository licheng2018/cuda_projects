{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Task Explanation: Shared Memory and Bank Conflicts in CUDA\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to understand how **CUDA shared memory** works, why it is significantly faster than global memory, and how **bank conflicts** can degrade its performance.  \n",
        "You will then implement a **shared-memory tile copy kernel** to practice using shared memory correctly and efficiently.\n",
        "\n",
        "This task introduces one of the most important performance concepts in CUDA and GPU programming.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is Shared Memory?\n",
        "Shared memory is a **small, on-chip memory** that is:\n",
        "- Shared by all threads within the same thread block\n",
        "- Much faster than global memory\n",
        "- Manually managed by the programmer\n",
        "\n",
        "Typical characteristics:\n",
        "- Low latency (similar to registers)\n",
        "- Limited size (e.g., 48‚Äì64 KB per SM)\n",
        "- Lifetime = one thread block\n",
        "\n",
        "Shared memory is widely used in:\n",
        "- Matrix multiplication (tiling)\n",
        "- Reductions\n",
        "- Convolutions\n",
        "- Normalization layers (LayerNorm, BatchNorm)\n",
        "- Attention kernels\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Understand Bank Conflicts\n",
        "\n",
        "### What Are Memory Banks?\n",
        "Shared memory is divided into multiple **banks** (typically 32 banks).  \n",
        "Each bank can service **one access per cycle**.\n",
        "\n",
        "- If threads in a warp access **different banks** ‚Üí no conflict  \n",
        "- If multiple threads access the **same bank** ‚Üí **bank conflict**\n",
        "\n",
        "### Why Bank Conflicts Matter\n",
        "Bank conflicts cause:\n",
        "- Serialized memory accesses\n",
        "- Increased latency\n",
        "- Lower effective bandwidth\n",
        "\n",
        "Even though shared memory is fast, **poor access patterns can make it slow**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Example Concept (No Code Required)\n",
        "You should be able to reason about:\n",
        "- Why accessing `shared[threadIdx.x]` is conflict-free\n",
        "- Why accessing `shared[threadIdx.x * stride]` may cause conflicts\n",
        "- How padding shared memory can eliminate conflicts\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Implement a Shared-Memory Tile Copy Kernel\n",
        "\n",
        "### Task\n",
        "Write a CUDA kernel that:\n",
        "1. Loads a **tile** of data from global memory into shared memory  \n",
        "2. Synchronizes threads within the block  \n",
        "3. Writes the tile back to global memory  \n",
        "\n",
        "The kernel should:\n",
        "- Use shared memory as an intermediate buffer\n",
        "- Correctly synchronize threads using `__syncthreads()`\n",
        "- Avoid out-of-bounds memory access\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why a Tile Copy Kernel?\n",
        "A tile copy kernel is a **minimal example** that demonstrates:\n",
        "- Correct shared memory usage\n",
        "- Thread cooperation within a block\n",
        "- The impact of access patterns on shared memory performance\n",
        "\n",
        "This pattern is a building block for:\n",
        "- Matrix multiplication\n",
        "- Transpose kernels\n",
        "- Convolution kernels\n",
        "- ML operators using tiling\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Pitfalls to Watch For\n",
        "- Forgetting `__syncthreads()` after loading shared memory  \n",
        "- Accessing shared memory with patterns that cause bank conflicts  \n",
        "- Using incorrect shared memory indexing  \n",
        "- Reading/writing out of bounds  \n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A CUDA kernel that copies data using shared memory tiling  \n",
        "2. A correctness check comparing input and output  \n",
        "3. (Optional) A benchmark comparing:\n",
        "   - Direct global memory copy  \n",
        "   - Shared-memory tile copy  \n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How shared memory differs from global memory\n",
        "- How bank conflicts arise and why they hurt performance\n",
        "- How to design shared-memory access patterns\n",
        "- Why padding and layout matter in GPU kernels\n",
        "\n",
        "These skills are essential for:\n",
        "- CUDA optimization\n",
        "- ML kernel development\n",
        "- Writing high-performance GPU code\n",
        "- Understanding libraries like cuBLAS, cuDNN, and Triton\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Many ML kernels rely heavily on shared memory:\n",
        "- Matrix multiply uses tiled shared memory\n",
        "- LayerNorm uses shared memory for reductions\n",
        "- Attention kernels reuse data via shared memory\n",
        "\n",
        "Mastering shared memory and bank conflicts is a **core skill for ML Systems Engineers and GPU Kernel Engineers**."
      ],
      "metadata": {
        "id": "hGCpFpA6dxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "iUUbnTxIOKIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b060fc6-5fe3-4fc0-989d-be8471558ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Fri Dec 26 21:49:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda-toolkit-12-4"
      ],
      "metadata": {
        "id": "XH7skVvROMGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0c1b72-f234-4f37-90d4-e5436f4a2705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.22)] [Connecting to security.ub\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,225 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,860 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,572 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,966 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Fetched 38.3 MB in 4s (8,924 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4-config-common cuda-tools-12-4\n",
            "  cuda-visual-tools-12-4 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gds-tools-12-4 gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libcublas-12-4 libcublas-dev-12-4 libcufft-12-4\n",
            "  libcufft-dev-12-4 libcufile-12-4 libcufile-dev-12-4 libcurand-12-4\n",
            "  libcurand-dev-12-4 libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4\n",
            "  libcusparse-dev-12-4 libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4\n",
            "  libnvfatbin-dev-12-4 libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4\n",
            "  libnvjpeg-dev-12-4 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxcomposite1 libxkbcommon-x11-0 libxtst6 libxxf86dga1\n",
            "  nsight-compute-2024.1.1 nsight-systems-2023.4.4 openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core cuda-cccl-12-4 cuda-command-line-tools-12-4 cuda-compiler-12-4\n",
            "  cuda-crt-12-4 cuda-cudart-12-4 cuda-cudart-dev-12-4 cuda-cuobjdump-12-4\n",
            "  cuda-cupti-12-4 cuda-cupti-dev-12-4 cuda-cuxxfilt-12-4\n",
            "  cuda-documentation-12-4 cuda-driver-dev-12-4 cuda-gdb-12-4\n",
            "  cuda-libraries-12-4 cuda-libraries-dev-12-4 cuda-nsight-12-4\n",
            "  cuda-nsight-compute-12-4 cuda-nsight-systems-12-4 cuda-nvcc-12-4\n",
            "  cuda-nvdisasm-12-4 cuda-nvml-dev-12-4 cuda-nvprof-12-4 cuda-nvprune-12-4\n",
            "  cuda-nvrtc-12-4 cuda-nvrtc-dev-12-4 cuda-nvtx-12-4 cuda-nvvm-12-4\n",
            "  cuda-nvvp-12-4 cuda-opencl-12-4 cuda-opencl-dev-12-4 cuda-profiler-api-12-4\n",
            "  cuda-sanitizer-12-4 cuda-toolkit-12-4 cuda-toolkit-12-4-config-common\n",
            "  cuda-tools-12-4 cuda-visual-tools-12-4 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gds-tools-12-4\n",
            "  gsettings-desktop-schemas libatk-bridge2.0-0 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libatk1.0-0 libatk1.0-data libatspi2.0-0\n",
            "  libcublas-12-4 libcublas-dev-12-4 libcufft-12-4 libcufft-dev-12-4\n",
            "  libcufile-12-4 libcufile-dev-12-4 libcurand-12-4 libcurand-dev-12-4\n",
            "  libcusolver-12-4 libcusolver-dev-12-4 libcusparse-12-4 libcusparse-dev-12-4\n",
            "  libnpp-12-4 libnpp-dev-12-4 libnvfatbin-12-4 libnvfatbin-dev-12-4\n",
            "  libnvjitlink-12-4 libnvjitlink-dev-12-4 libnvjpeg-12-4 libnvjpeg-dev-12-4\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 libxxf86dga1 nsight-compute-2024.1.1\n",
            "  nsight-systems-2023.4.4 openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "0 upgraded, 88 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 3,068 MB of archives.\n",
            "After this operation, 6,782 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-12-4 12.4.127-1 [1,200 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-12-4 12.4.127-1 [16.8 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.29+7-1ubuntu1~22.04 [42.6 MB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-12-4 12.4.127-1 [3,830 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-12-4 12.4.127-1 [49.9 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-12-4 12.4.127-1 [225 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.29+7-1ubuntu1~22.04 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-12-4 12.4.127-1 [4,920 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-12-4 12.4.127-1 [2,431 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-12-4 12.4.127-1 [51.5 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-12-4 12.4.127-1 [9,148 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-12-4 12.4.1-1 [2,538 B]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-12-4 12.4.127-1 [191 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4-config-common 12.4.127-1 [16.4 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-12-4 12.4.127-1 [165 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-12-4 12.4.127-1 [28.6 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-12-4 12.4.127-1 [1,000 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvm-12-4 12.4.131-1 [19.5 MB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-crt-12-4 12.4.131-1 [78.0 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-12-4 12.4.131-1 [32.0 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-12-4 12.4.127-1 [58.8 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-12-4 12.4.1-1 [2,510 B]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-12-4 12.4.127-1 [50.0 kB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-12-4 12.4.127-1 [17.6 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-12-4 12.4.127-1 [24.0 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-12-4 12.4.5.8-1 [231 MB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-12-4 11.2.1.3-1 [171 MB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-12-4 1.9.1.3-1 [850 kB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-12-4 10.3.5.147-1 [41.4 MB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-12-4 11.6.1.9-1 [78.9 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-12-4 12.3.1.170-1 [115 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-12-4 12.2.5.30-1 [95.5 MB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-12-4 12.4.127-1 [15.5 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-12-4 12.4.127-1 [721 kB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-12-4 12.3.1.117-1 [2,327 kB]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-12-4 12.4.1-1 [2,606 B]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-12-4 12.4.127-1 [18.7 kB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-12-4 12.4.127-1 [16.9 MB]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-dev-12-4 12.4.127-1 [87.0 kB]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-12-4 12.4.5.8-1 [249 MB]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-12-4 11.2.1.3-1 [342 MB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-12-4 1.9.1.3-1 [2,435 kB]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-12-4 10.3.5.147-1 [41.6 MB]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-12-4 11.6.1.9-1 [51.4 MB]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-12-4 12.3.1.170-1 [116 MB]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-12-4 12.2.5.30-1 [92.0 MB]\n",
            "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-dev-12-4 12.4.127-1 [13.9 MB]\n",
            "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvfatbin-dev-12-4 12.4.127-1 [591 kB]\n",
            "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-12-4 12.3.1.117-1 [2,025 kB]\n",
            "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-12-4 12.4.1-1 [2,646 B]\n",
            "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-12-4 12.4.127-1 [119 MB]\n",
            "Get:79 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2024.1.1 2024.1.1.4-1 [594 MB]\n",
            "Get:80 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-12-4 12.4.1-1 [4,060 B]\n",
            "Get:81 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.4.4 2023.4.4.54-234433681190v0 [316 MB]\n",
            "Get:82 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-12-4 12.4.1-1 [3,348 B]\n",
            "Get:83 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-12-4 12.4.127-1 [145 kB]\n",
            "Get:84 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-12-4 12.4.127-1 [115 MB]\n",
            "Get:85 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-12-4 12.4.1-1 [2,942 B]\n",
            "Get:86 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-12-4 1.9.1.3-1 [39.0 MB]\n",
            "Get:87 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-12-4 12.4.1-1 [2,462 B]\n",
            "Get:88 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-4 12.4.1-1 [3,336 B]\n",
            "Fetched 3,068 MB in 56s (54.8 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package cuda-cccl-12-4.\n",
            "Preparing to unpack .../05-cuda-cccl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-12-4.\n",
            "Preparing to unpack .../06-cuda-cupti-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-12-4.\n",
            "Preparing to unpack .../07-cuda-cupti-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-12-4.\n",
            "Preparing to unpack .../08-cuda-nvdisasm-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-12-4.\n",
            "Preparing to unpack .../09-cuda-cuobjdump-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-gdb-12-4.\n",
            "Preparing to unpack .../10-cuda-gdb-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-12-4.\n",
            "Preparing to unpack .../11-cuda-nvprof-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-12-4.\n",
            "Preparing to unpack .../12-cuda-nvtx-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-12-4.\n",
            "Preparing to unpack .../13-cuda-sanitizer-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-12-4.\n",
            "Preparing to unpack .../14-cuda-command-line-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-12-4.\n",
            "Preparing to unpack .../15-cuda-cuxxfilt-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4-config-common.\n",
            "Preparing to unpack .../16-cuda-toolkit-12-4-config-common_12.4.127-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-12-4.\n",
            "Preparing to unpack .../17-cuda-cudart-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-12-4.\n",
            "Preparing to unpack .../18-cuda-driver-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-12-4.\n",
            "Preparing to unpack .../19-cuda-cudart-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvm-12-4.\n",
            "Preparing to unpack .../20-cuda-nvvm-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-crt-12-4.\n",
            "Preparing to unpack .../21-cuda-crt-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-crt-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-12-4.\n",
            "Preparing to unpack .../22-cuda-nvcc-12-4_12.4.131-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-12-4.\n",
            "Preparing to unpack .../23-cuda-nvprune-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-compiler-12-4.\n",
            "Preparing to unpack .../24-cuda-compiler-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-documentation-12-4.\n",
            "Preparing to unpack .../25-cuda-documentation-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-12-4.\n",
            "Preparing to unpack .../26-cuda-nvrtc-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-12-4.\n",
            "Preparing to unpack .../27-cuda-opencl-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-12-4.\n",
            "Preparing to unpack .../28-libcublas-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-12-4.\n",
            "Preparing to unpack .../29-libcufft-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-12-4.\n",
            "Preparing to unpack .../30-libcufile-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-12-4.\n",
            "Preparing to unpack .../31-libcurand-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-12-4.\n",
            "Preparing to unpack .../32-libcusolver-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-12-4.\n",
            "Preparing to unpack .../33-libcusparse-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-12-4.\n",
            "Preparing to unpack .../34-libnpp-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-12-4.\n",
            "Preparing to unpack .../35-libnvjitlink-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-12-4.\n",
            "Preparing to unpack .../36-libnvfatbin-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-12-4.\n",
            "Preparing to unpack .../37-libnvjpeg-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-12-4.\n",
            "Preparing to unpack .../38-cuda-libraries-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-12-4.\n",
            "Preparing to unpack .../39-cuda-profiler-api-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-12-4.\n",
            "Preparing to unpack .../40-cuda-nvrtc-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-opencl-dev-12-4.\n",
            "Preparing to unpack .../41-cuda-opencl-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libcublas-dev-12-4.\n",
            "Preparing to unpack .../42-libcublas-dev-12-4_12.4.5.8-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Selecting previously unselected package libcufft-dev-12-4.\n",
            "Preparing to unpack .../43-libcufft-dev-12-4_11.2.1.3-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Selecting previously unselected package libcufile-dev-12-4.\n",
            "Preparing to unpack .../44-libcufile-dev-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package libcurand-dev-12-4.\n",
            "Preparing to unpack .../45-libcurand-dev-12-4_10.3.5.147-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-12-4.\n",
            "Preparing to unpack .../46-libcusolver-dev-12-4_11.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-12-4.\n",
            "Preparing to unpack .../47-libcusparse-dev-12-4_12.3.1.170-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Selecting previously unselected package libnpp-dev-12-4.\n",
            "Preparing to unpack .../48-libnpp-dev-12-4_12.2.5.30-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Selecting previously unselected package libnvjitlink-dev-12-4.\n",
            "Preparing to unpack .../49-libnvjitlink-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvfatbin-dev-12-4.\n",
            "Preparing to unpack .../50-libnvfatbin-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-12-4.\n",
            "Preparing to unpack .../51-libnvjpeg-dev-12-4_12.3.1.117-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-12-4.\n",
            "Preparing to unpack .../52-cuda-libraries-dev-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../53-openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../54-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../55-openjdk-11-jre_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../56-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-12-4.\n",
            "Preparing to unpack .../57-cuda-nsight-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package nsight-compute-2024.1.1.\n",
            "Preparing to unpack .../58-nsight-compute-2024.1.1_2024.1.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-12-4.\n",
            "Preparing to unpack .../59-cuda-nsight-compute-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../60-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../61-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../62-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../63-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../64-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../65-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../66-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../67-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../68-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../69-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../70-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package nsight-systems-2023.4.4.\n",
            "Preparing to unpack .../71-nsight-systems-2023.4.4_2023.4.4.54-234433681190v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-12-4.\n",
            "Preparing to unpack .../72-cuda-nsight-systems-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-12-4.\n",
            "Preparing to unpack .../73-cuda-nvml-dev-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-12-4.\n",
            "Preparing to unpack .../74-cuda-nvvp-12-4_12.4.127-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-12-4.\n",
            "Preparing to unpack .../75-cuda-visual-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package gds-tools-12-4.\n",
            "Preparing to unpack .../76-gds-tools-12-4_1.9.1.3-1_amd64.deb ...\n",
            "Unpacking gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Selecting previously unselected package cuda-tools-12-4.\n",
            "Preparing to unpack .../77-cuda-tools-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-tools-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-4.\n",
            "Preparing to unpack .../78-cuda-toolkit-12-4_12.4.1-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../79-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../80-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../81-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../82-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../83-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../84-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../85-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../86-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../87-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-cupti-12-4 (12.4.127-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up cuda-nvml-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cccl-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-cupti-dev-12-4 (12.4.127-1) ...\n",
            "Setting up gds-tools-12-4 (1.9.1.3-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-documentation-12-4 (12.4.127-1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up cuda-profiler-api-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvdisasm-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up cuda-nvvm-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvprune-12-4 (12.4.127-1) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-nvtx-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cuxxfilt-12-4 (12.4.127-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2024.1.1 (2024.1.1.4-1) ...\n",
            "Setting up cuda-driver-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvprof-12-4 (12.4.127-1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-cuobjdump-12-4 (12.4.127-1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up cuda-toolkit-12-4-config-common (12.4.127-1) ...\n",
            "Setting alternatives\n",
            "Setting up libnvjitlink-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nvrtc-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-sanitizer-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-12-4 (10.3.5.147-1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libnpp-12-4 (12.2.5.30-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcufft-12-4 (11.2.1.3-1) ...\n",
            "Setting up libnvjitlink-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-12-4 (12.4.5.8-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libcusparse-12-4 (12.3.1.170-1) ...\n",
            "Setting up libnvfatbin-12-4 (12.4.127-1) ...\n",
            "Setting up libcurand-dev-12-4 (10.3.5.147-1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libnvjpeg-12-4 (12.3.1.117-1) ...\n",
            "Setting up cuda-nsight-compute-12-4 (12.4.1-1) ...\n",
            "Setting up libcufile-12-4 (1.9.1.3-1) ...\n",
            "Setting alternatives\n",
            "Setting up nsight-systems-2023.4.4 (2023.4.4.54-234433681190v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.4.4/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libnpp-dev-12-4 (12.2.5.30-1) ...\n",
            "Setting up cuda-opencl-12-4 (12.4.127-1) ...\n",
            "Setting up libcusparse-dev-12-4 (12.3.1.170-1) ...\n",
            "Setting up cuda-nvrtc-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libcublas-dev-12-4 (12.4.5.8-1) ...\n",
            "Setting up cuda-cudart-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-gdb-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-nsight-12-4 (12.4.127-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libcusolver-12-4 (11.6.1.9-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up cuda-libraries-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-nsight-systems-12-4 (12.4.1-1) ...\n",
            "Setting up libcusolver-dev-12-4 (11.6.1.9-1) ...\n",
            "Setting up libcufft-dev-12-4 (11.2.1.3-1) ...\n",
            "Setting up cuda-command-line-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-opencl-dev-12-4 (12.4.127-1) ...\n",
            "Setting up libnvfatbin-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-cudart-dev-12-4 (12.4.127-1) ...\n",
            "Setting up cuda-crt-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-nvvp-12-4 (12.4.127-1) ...\n",
            "Setting up libnvjpeg-dev-12-4 (12.3.1.117-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libcufile-dev-12-4 (1.9.1.3-1) ...\n",
            "Setting up cuda-nvcc-12-4 (12.4.131-1) ...\n",
            "Setting up cuda-compiler-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-libraries-dev-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-visual-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-tools-12-4 (12.4.1-1) ...\n",
            "Setting up cuda-toolkit-12-4 (12.4.1-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxRcNMC7ds_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3097af-2875-4f6e-857a-88ffe3cf7056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting shared_mem_tile_copy_skeleton.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile shared_mem_tile_copy_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (Task): Shared-memory tile copy kernel\n",
        "// Goal:\n",
        "//  - Copy input array A -> output array B using shared memory tiles.\n",
        "// Requirements:\n",
        "//  - Each block loads a tile from global memory into shared memory.\n",
        "//  - Synchronize threads (barrier) before writing back.\n",
        "//  - Write the tile from shared memory to global memory.\n",
        "//  - Avoid out-of-bounds accesses.\n",
        "// Notes:\n",
        "//  - Start with 1D tiling (simpler) or extend to 2D tiling (optional).\n",
        "// ------------------------------------------------------------\n",
        "__global__ void tileCopyShared(const float* __restrict__ A,\n",
        "                               float* __restrict__ B,\n",
        "                               int N) {\n",
        "    // TODO: declare shared memory tile\n",
        "    // Example (static): __shared__ float tile[TILE_SIZE];\n",
        "    // Or (dynamic): extern __shared__ float tile[];\n",
        "    extern __shared__ float tile[];\n",
        "\n",
        "    // TODO: compute global index\n",
        "    // int tid = ...\n",
        "    // int gid = ...\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // TODO: load from global -> shared (guarded)\n",
        "    // if (...) tile[...] = A[...];\n",
        "    if(tid < N){\n",
        "      tile[tid] = A[gid];\n",
        "    }\n",
        "\n",
        "    // TODO: synchronize\n",
        "    // __syncthreads();\n",
        "    __syncthreads();\n",
        "\n",
        "    // TODO: store from shared -> global (guarded)\n",
        "    // if (...) B[...] = tile[...];\n",
        "    if (gid < N){\n",
        "        B[gid] = tile[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// CPU reference copy\n",
        "// ------------------------------------------------------------\n",
        "static void copyCPU(const float* A, float* B, int N) {\n",
        "    for (int i = 0; i < N; ++i) B[i] = A[i];\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: correctness checker\n",
        "// Requirements:\n",
        "//  - Compare arrays elementwise within tolerance\n",
        "//  - Print first mismatch and return false\n",
        "// ------------------------------------------------------------\n",
        "static bool checkClose(const float* out, const float* ref, int N, float tol) {\n",
        "    // TODO\n",
        "    for(int i = 0; i < N; ++i){\n",
        "        float diff = fabsf(out[i] - ref[i]);\n",
        "        if (diff > tol){\n",
        "          return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 24; // ~16M floats\n",
        "    const float tol = 1e-6f;\n",
        "    const size_t bytes = size_t(N) * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytes);\n",
        "    float* hB = (float*)std::malloc(bytes);\n",
        "    float* hRef = (float*)std::malloc(bytes);\n",
        "\n",
        "    if (!hA || !hB || !hRef) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init input\n",
        "    for (int i = 0; i < N; ++i) hA[i] = 0.001f * i;\n",
        "\n",
        "    // CPU reference\n",
        "    copyCPU(hA, hRef, N);\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA = nullptr, *dB = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytes));\n",
        "\n",
        "    // H2D\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Choose launch configuration\n",
        "    // - Decide TILE_SIZE (or blockSize)\n",
        "    // - Decide gridSize\n",
        "    // - If using dynamic shared memory, pass sharedMemBytes\n",
        "    // ------------------------------------------------------------\n",
        "    int blockSize = 256;     // TODO\n",
        "    int gridSize  = (N + blockSize - 1) / blockSize;     // TODO\n",
        "    size_t sharedMemBytes = blockSize * sizeof(float); // TODO (0 if using static shared mem)\n",
        "\n",
        "    // Launch\n",
        "    tileCopyShared<<<gridSize, blockSize, sharedMemBytes>>>(dA, dB, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // D2H\n",
        "    CUDA_CHECK(cudaMemcpy(hB, dB, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Correctness check\n",
        "    // ------------------------------------------------------------\n",
        "    bool ok = checkClose(hB, hRef, N, tol);\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hRef);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 shared_mem_tile_copy_skeleton.cu -o tile_copy\n",
        "!./tile_copy"
      ],
      "metadata": {
        "id": "vnXqPl5beOP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b6db60-a9b5-43e7-cf52-5ebdba5b33e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctness: PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Task Explanation: Shared Memory Bank-Conflict Experiment\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to **understand, observe, and quantify shared-memory bank conflicts** in CUDA.  \n",
        "You will design controlled experiments that access shared memory with different patterns and measure how **bank conflicts impact kernel performance**.\n",
        "\n",
        "This task builds intuition about why shared memory can be either **extremely fast or unexpectedly slow**, depending on access patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: What Is a Bank Conflict?\n",
        "Shared memory on NVIDIA GPUs is divided into **multiple memory banks** (typically 32 banks).\n",
        "\n",
        "- Each bank can service **one access per cycle**\n",
        "- Threads in a **warp (32 threads)** access shared memory simultaneously\n",
        "\n",
        "### Access Scenarios\n",
        "- **Conflict-free**: each thread accesses a different bank  \n",
        "- **Bank conflict**: two or more threads access the same bank  \n",
        "- **Result**: accesses are serialized ‚Üí increased latency\n",
        "\n",
        "Even though shared memory is on-chip, **bank conflicts can significantly degrade performance**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Design Conflict-Free Shared Memory Access\n",
        "\n",
        "### Task\n",
        "Create a kernel where each thread accesses shared memory in a **conflict-free pattern**, for example:\n",
        "\n",
        "- Thread `t` accesses `shared[t]`\n",
        "- Consecutive threads map to consecutive banks\n",
        "\n",
        "### Goal\n",
        "Establish a **performance baseline** where shared memory is used optimally.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Introduce Bank Conflicts via Strided Access\n",
        "\n",
        "### Task\n",
        "Modify the kernel so that threads access shared memory with a **stride**, such as:\n",
        "\n",
        "- Thread `t` accesses `shared[t * stride]`\n",
        "\n",
        "Test multiple stride values (e.g., 1, 2, 4, 8).\n",
        "\n",
        "### Expected Effect\n",
        "As the stride increases:\n",
        "- More threads map to the same bank\n",
        "- The number of bank conflicts increases\n",
        "- Kernel execution time increases\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Eliminate Bank Conflicts Using Padding\n",
        "\n",
        "### Task\n",
        "Redesign the shared memory layout by adding **padding**, such as:\n",
        "\n",
        "- Declaring shared memory with extra elements\n",
        "- Changing indexing so threads map to different banks\n",
        "\n",
        "### Goal\n",
        "Demonstrate that **bank conflicts are a layout problem**, not an inherent limitation of shared memory.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Measure\n",
        "For each configuration (conflict-free, conflicted, padded):\n",
        "\n",
        "- Kernel execution time\n",
        "- (Optional) Effective shared memory bandwidth\n",
        "- Relative slowdown compared to conflict-free case\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Observations\n",
        "\n",
        "| Access Pattern | Bank Conflicts | Performance |\n",
        "|---------------|---------------|-------------|\n",
        "| Contiguous (`shared[t]`) | None | Fastest |\n",
        "| Strided (`shared[t*2]`) | Moderate | Slower |\n",
        "| Highly strided (`shared[t*4]`) | Severe | Much slower |\n",
        "| Padded layout | Eliminated | Near baseline |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Questions to Answer\n",
        "- How does stride affect bank mapping?\n",
        "- Why does padding eliminate conflicts?\n",
        "- Why do bank conflicts serialize memory access?\n",
        "- How do these effects scale with block size?\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "\n",
        "1. A CUDA kernel demonstrating **conflict-free shared memory access**\n",
        "2. A CUDA kernel demonstrating **bank-conflicted access**\n",
        "3. A CUDA kernel demonstrating **conflict elimination via padding**\n",
        "4. Benchmark results comparing all three cases\n",
        "5. A short written analysis explaining:\n",
        "   - How bank conflicts arise\n",
        "   - Why padding works\n",
        "   - How performance changes across configurations\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "\n",
        "- How shared memory banks work\n",
        "- Why access patterns matter more than raw computation\n",
        "- How to diagnose shared memory performance issues\n",
        "- How to design bank-conflict-free shared memory layouts\n",
        "\n",
        "These skills are critical for:\n",
        "- CUDA optimization\n",
        "- Matrix multiplication and transpose kernels\n",
        "- Reductions and prefix sums\n",
        "- ML kernels using shared memory (LayerNorm, Attention)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Many high-performance ML kernels rely heavily on shared memory:\n",
        "\n",
        "- GEMM tiling in cuBLAS\n",
        "- LayerNorm and BatchNorm reductions\n",
        "- Attention and FlashAttention kernels\n",
        "\n",
        "Understanding and eliminating bank conflicts is a **core competency for ML Systems Engineers and GPU Kernel Engineers**.\n",
        "\n",
        "This task trains the same reasoning used in real-world CUDA and Triton kernel optimization."
      ],
      "metadata": {
        "id": "K7x5FLZnenky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bank_conflict_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "__global__ void bankConflictKernel(float* out, int N, int mode, int stride, int iters) {\n",
        "    extern __shared__ float smem[];\n",
        "\n",
        "    int gid  = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int lane = threadIdx.x;\n",
        "\n",
        "    // init: one element per thread for mode 0/1\n",
        "    if (lane < blockDim.x) smem[lane] = (float)lane;\n",
        "    __syncthreads();\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    for (int k = 0; k < iters; ++k) {\n",
        "        if (mode == 0) {\n",
        "            // baseline\n",
        "            int idx = lane;\n",
        "            acc += smem[idx];\n",
        "\n",
        "        } else if (mode == 1) {\n",
        "            // strided (wrap to avoid OOB)\n",
        "            int idx = (lane * stride) % blockDim.x;\n",
        "            acc += smem[idx];\n",
        "\n",
        "        } else {\n",
        "            // padded tile experiment using only first 32 threads\n",
        "            // to keep shared memory small and demonstrate bank behavior clearly\n",
        "            const int TILE = 32;\n",
        "            const int pitch = TILE + 1; // padding\n",
        "\n",
        "            if (lane < TILE) {\n",
        "                // layout uses first TILE*pitch floats in smem\n",
        "                // treat as tile[row][col]\n",
        "                int row = lane;\n",
        "                int col = (k * stride) % TILE;\n",
        "\n",
        "                int idx = row * pitch + col;  // within TILE*(TILE+1)\n",
        "                acc += smem[idx];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (gid < N) out[gid] = acc;\n",
        "}\n",
        "\n",
        "static float timeKernelMs(float* dOut, int N, int mode, int stride, int iters,\n",
        "                          int gridSize, int blockSize, size_t sharedBytes,\n",
        "                          int warmup, int runs) {\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        bankConflictKernel<<<gridSize, blockSize, sharedBytes>>>(dOut, N, mode, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        bankConflictKernel<<<gridSize, blockSize, sharedBytes>>>(dOut, N, mode, stride, iters);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "    return ms / runs;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 20;\n",
        "    const int blockSize = 256;\n",
        "    const int gridSize  = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    const int strides[] = {1, 2, 4, 8, 16, 32};\n",
        "    const int numStrides = (int)(sizeof(strides) / sizeof(strides[0]));\n",
        "\n",
        "    const int iters  = 2048;\n",
        "    const int warmup = 5;\n",
        "    const int runs   = 20;\n",
        "\n",
        "    // shared memory bytes:\n",
        "    // need at least blockSize floats for mode 0/1\n",
        "    // and at least TILE*(TILE+1) floats for mode 2\n",
        "    const int TILE = 32;\n",
        "    size_t needFloats = (size_t)blockSize;\n",
        "    size_t paddedFloats = (size_t)TILE * (size_t)(TILE + 1);\n",
        "    if (paddedFloats > needFloats) needFloats = paddedFloats;\n",
        "    size_t sharedBytes = needFloats * sizeof(float);\n",
        "\n",
        "    float* dOut = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, (size_t)N * sizeof(float)));\n",
        "\n",
        "    printf(\"=== Bank Conflict Experiment (fixed) ===\\n\");\n",
        "    printf(\"N=%d, grid=%d, block=%d, iters=%d, sharedBytes=%.2f KB\\n\",\n",
        "           N, gridSize, blockSize, iters, sharedBytes / 1024.0);\n",
        "\n",
        "    for (int s = 0; s < numStrides; ++s) {\n",
        "        int stride = strides[s];\n",
        "\n",
        "        for (int mode = 0; mode <= 2; ++mode) {\n",
        "            float ms = timeKernelMs(dOut, N, mode, stride, iters,\n",
        "                                    gridSize, blockSize, sharedBytes,\n",
        "                                    warmup, runs);\n",
        "\n",
        "            const char* name =\n",
        "                (mode == 0) ? \"baseline\" :\n",
        "                (mode == 1) ? \"strided\"  :\n",
        "                              \"padded32x33\";\n",
        "\n",
        "            printf(\"[stride=%2d][mode=%d:%s] time=%.4f ms\\n\", stride, mode, name, ms);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "GH_nTlqZewg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6a580f-bd64-4021-d4da-e75586833aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bank_conflict_skeleton.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 bank_conflict_skeleton.cu -o bank_conflict_skeleton\n",
        "!./bank_conflict_skeleton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5FImTMmIyyn",
        "outputId": "cf34764a-24a3-4c25-ea26-04a72ab0c572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bank Conflict Experiment (fixed) ===\n",
            "N=1048576, grid=4096, block=256, iters=2048, sharedBytes=4.12 KB\n",
            "[stride= 1][mode=0:baseline] time=1.4614 ms\n",
            "[stride= 1][mode=1:strided] time=1.5553 ms\n",
            "[stride= 1][mode=2:padded32x33] time=5.3495 ms\n",
            "\n",
            "[stride= 2][mode=0:baseline] time=0.9506 ms\n",
            "[stride= 2][mode=1:strided] time=1.0115 ms\n",
            "[stride= 2][mode=2:padded32x33] time=3.8302 ms\n",
            "\n",
            "[stride= 4][mode=0:baseline] time=0.5552 ms\n",
            "[stride= 4][mode=1:strided] time=0.6016 ms\n",
            "[stride= 4][mode=2:padded32x33] time=3.0060 ms\n",
            "\n",
            "[stride= 8][mode=0:baseline] time=0.5602 ms\n",
            "[stride= 8][mode=1:strided] time=0.6188 ms\n",
            "[stride= 8][mode=2:padded32x33] time=2.9954 ms\n",
            "\n",
            "[stride=16][mode=0:baseline] time=0.5604 ms\n",
            "[stride=16][mode=1:strided] time=0.6213 ms\n",
            "[stride=16][mode=2:padded32x33] time=2.9825 ms\n",
            "\n",
            "[stride=32][mode=0:baseline] time=0.5602 ms\n",
            "[stride=32][mode=1:strided] time=0.6209 ms\n",
            "[stride=32][mode=2:padded32x33] time=3.0003 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bank_conflict_clean.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// blockDim should be (32, 8) or (32, 16)\n",
        "__global__ void bankConflictClean(float* out, int iters, int mode) {\n",
        "    // We'll use a 32x32 tile. For padding mode, pitch=33.\n",
        "    extern __shared__ float smem[];\n",
        "\n",
        "    int tx = threadIdx.x; // 0..31\n",
        "    int ty = threadIdx.y; // 0..(BLOCK_ROWS-1)\n",
        "\n",
        "    const int TILE = 32;\n",
        "    const int pitch = (mode == 2) ? (TILE + 1) : TILE; // padding only in mode=2\n",
        "\n",
        "    // Initialize: each thread writes multiple rows (like transpose)\n",
        "    for (int j = 0; j < TILE; j += blockDim.y) {\n",
        "        int row = ty + j;\n",
        "        smem[row * pitch + tx] = (float)(row * 0.1f + tx);\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    for (int k = 0; k < iters; ++k) {\n",
        "        // choose a column that changes over time (avoid cache weirdness)\n",
        "        int col = (k & 31);\n",
        "\n",
        "        int idx;\n",
        "        if (mode == 0) {\n",
        "            // Row access (conflict-free-ish): smem[ty][tx]\n",
        "            idx = ty * pitch + tx;\n",
        "        } else {\n",
        "            // Column access (classic conflict when pitch=32):\n",
        "            // smem[tx][col] -> threads in warp differ in row (tx), same col\n",
        "            idx = tx * pitch + col;\n",
        "        }\n",
        "        acc += smem[idx];\n",
        "    }\n",
        "\n",
        "    // write something\n",
        "    int gid = (blockIdx.x * blockDim.x * blockDim.y) + (ty * blockDim.x + tx);\n",
        "    out[gid] = acc;\n",
        "}\n",
        "\n",
        "static float timeMs(float* dOut, int iters, int mode,\n",
        "                    dim3 grid, dim3 block, size_t shBytes, int warmup, int runs) {\n",
        "    for (int i=0;i<warmup;i++) bankConflictClean<<<grid, block, shBytes>>>(dOut, iters, mode);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t s,e;\n",
        "    CUDA_CHECK(cudaEventCreate(&s));\n",
        "    CUDA_CHECK(cudaEventCreate(&e));\n",
        "    CUDA_CHECK(cudaEventRecord(s));\n",
        "    for (int i=0;i<runs;i++) bankConflictClean<<<grid, block, shBytes>>>(dOut, iters, mode);\n",
        "    CUDA_CHECK(cudaEventRecord(e));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(e));\n",
        "\n",
        "    float ms=0;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms,s,e));\n",
        "    CUDA_CHECK(cudaEventDestroy(s));\n",
        "    CUDA_CHECK(cudaEventDestroy(e));\n",
        "    return ms/runs;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int iters = 4096;\n",
        "    const int warmup=5, runs=20;\n",
        "\n",
        "    dim3 block(32, 8);\n",
        "    dim3 grid(256);\n",
        "\n",
        "    // Allocate enough for the padded case: 32*(33) floats\n",
        "    size_t shBytes = 32 * 33 * sizeof(float);\n",
        "\n",
        "    float* dOut=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, grid.x * block.x * block.y * sizeof(float)));\n",
        "\n",
        "    printf(\"=== Clean Bank Conflict Demo ===\\n\");\n",
        "    printf(\"block=(%d,%d) grid=%d iters=%d shmem=%.2fKB\\n\",\n",
        "           block.x, block.y, (int)grid.x, iters, shBytes/1024.0);\n",
        "\n",
        "    // mode 0: row access\n",
        "    float t0 = timeMs(dOut, iters, 0, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    // mode 1: column access with pitch=32 -> conflicts\n",
        "    float t1 = timeMs(dOut, iters, 1, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    // mode 2: column access with pitch=33 -> padding removes conflicts\n",
        "    float t2 = timeMs(dOut, iters, 2, grid, block, shBytes, warmup, runs);\n",
        "\n",
        "    printf(\"mode0 row(no conflict) : %.4f ms\\n\", t0);\n",
        "    printf(\"mode1 col(pitch=32)    : %.4f ms (conflict)\\n\", t1);\n",
        "    printf(\"mode2 col(pitch=33)    : %.4f ms (padded)\\n\", t2);\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiDeEOp-osqv",
        "outputId": "00af337a-1854-48a3-a741-5afe35c57d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bank_conflict_clean.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 bank_conflict_clean.cu -o bank_conflict_clean\n",
        "!./bank_conflict_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7w2NbMxqIEY",
        "outputId": "9fa8f67b-c858-4bd0-f2ee-1707c966dbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Clean Bank Conflict Demo ===\n",
            "block=(32,8) grid=256 iters=4096 shmem=4.12KB\n",
            "mode0 row(no conflict) : 0.2050 ms\n",
            "mode1 col(pitch=32)    : 12.6755 ms (conflict)\n",
            "mode2 col(pitch=33)    : 0.2935 ms (padded)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Follow-Up Task Explanation: Shared-Memory Matrix Multiplication (Tiled GEMM)\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this task is to implement **matrix multiplication (GEMM)** using **shared memory tiling** in CUDA and understand **why shared memory dramatically improves performance** over a naive global-memory implementation.\n",
        "\n",
        "This task builds directly on:\n",
        "- Shared memory fundamentals\n",
        "- Thread cooperation within a block\n",
        "- Bank-conflict awareness\n",
        "- Memory coalescing concepts\n",
        "\n",
        "It is one of the **most important CUDA optimization patterns** and a foundation for ML workloads.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Background: Why Matrix Multiplication Matters\n",
        "Matrix multiplication is the computational core of:\n",
        "- Neural network layers (Linear, Attention, MLP)\n",
        "- Convolutions (after im2col)\n",
        "- Scientific computing and simulations\n",
        "\n",
        "A naive CUDA implementation is usually **memory-bound** because:\n",
        "- Each matrix element is loaded repeatedly from global memory\n",
        "- Global memory latency dominates execution time\n",
        "\n",
        "Shared-memory tiling solves this problem by **reusing data**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part A ‚Äî Naive Matrix Multiplication (Baseline)\n",
        "\n",
        "### Task\n",
        "Implement a naive CUDA matrix multiplication kernel:\n",
        "\n",
        "\\[\n",
        "C_{ij} = \\sum_k A_{ik} \\cdot B_{kj}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- Each thread computes **one element** of matrix `C`\n",
        "- All reads come directly from **global memory**\n",
        "\n",
        "### Goal\n",
        "Establish a **performance baseline** to compare against the optimized version.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part B ‚Äî Shared-Memory Tiled Matrix Multiplication\n",
        "\n",
        "### Task\n",
        "Rewrite the kernel using **shared memory tiling**:\n",
        "\n",
        "1. Divide matrices into square **tiles** (e.g., 16√ó16 or 32√ó32)\n",
        "2. Each thread block:\n",
        "   - Loads one tile of `A` and one tile of `B` into shared memory\n",
        "3. Synchronize threads\n",
        "4. Compute partial dot products using shared memory\n",
        "5. Repeat for all tiles along the K dimension\n",
        "\n",
        "### Key Requirements\n",
        "- Use `__shared__` memory for tiles\n",
        "- Use `__syncthreads()` correctly\n",
        "- Ensure global memory loads are coalesced\n",
        "- Avoid shared-memory bank conflicts\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Tiling Works\n",
        "Shared-memory tiling:\n",
        "- Reduces global memory loads by **reuse**\n",
        "- Converts many slow global reads into fast shared reads\n",
        "- Increases arithmetic intensity (FLOPs per byte)\n",
        "\n",
        "This is the core idea behind **cuBLAS**, **Tensor Cores**, and most ML kernels.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Part C ‚Äî Tile Size and Performance Tuning\n",
        "\n",
        "### Task\n",
        "Experiment with different tile sizes:\n",
        "- 16√ó16\n",
        "- 32√ó32\n",
        "\n",
        "Observe how tile size affects:\n",
        "- Performance\n",
        "- Occupancy\n",
        "- Shared memory usage\n",
        "\n",
        "### Key Trade-offs\n",
        "- Larger tiles ‚Üí more reuse, but higher shared memory usage\n",
        "- Smaller tiles ‚Üí lower shared memory, but less reuse\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What to Measure\n",
        "For both naive and tiled kernels, measure:\n",
        "- Kernel execution time\n",
        "- Speedup of tiled version over naive\n",
        "- (Optional) Effective FLOPs or bandwidth\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Observations\n",
        "\n",
        "| Kernel Type | Memory Behavior | Performance |\n",
        "|------------|-----------------|-------------|\n",
        "| Naive | Repeated global loads | Slow |\n",
        "| Tiled (shared memory) | Data reuse in shared memory | Much faster |\n",
        "| Tuned tile size | Balanced reuse & occupancy | Best |\n",
        "\n",
        "A correct tiled implementation should achieve **multiple-X speedup** over the naive version.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Pitfalls\n",
        "- Forgetting `__syncthreads()` after loading tiles\n",
        "- Incorrect indexing into shared memory\n",
        "- Out-of-bounds accesses at matrix edges\n",
        "- Bank conflicts in shared memory tiles\n",
        "- Choosing tile sizes that exceed shared memory limits\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Deliverables\n",
        "You should produce:\n",
        "1. A **naive CUDA matrix multiplication kernel**\n",
        "2. A **shared-memory tiled matrix multiplication kernel**\n",
        "3. Benchmark results comparing both implementations\n",
        "4. A short analysis explaining:\n",
        "   - Why the tiled version is faster\n",
        "   - How shared memory reuse improves performance\n",
        "   - How tile size affects efficiency\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You Learn from This Task\n",
        "By completing this task, you will understand:\n",
        "- How to design cooperative thread-block algorithms\n",
        "- How shared memory enables data reuse\n",
        "- How memory hierarchy affects performance\n",
        "- How real ML kernels are structured internally\n",
        "\n",
        "This task is a **cornerstone skill** for:\n",
        "- CUDA optimization\n",
        "- ML kernel engineering\n",
        "- Understanding cuBLAS and Triton matmul\n",
        "- LLM training and inference acceleration\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Relevance to ML Systems\n",
        "Shared-memory matrix multiplication is the conceptual basis for:\n",
        "- cuBLAS GEMM\n",
        "- Tensor Core operations\n",
        "- Attention QK·µÄ and QKV kernels\n",
        "- Triton and compiler-generated GPU kernels\n",
        "\n",
        "Mastering this task means you are thinking like a **GPU kernel engineer**, not just a CUDA user."
      ],
      "metadata": {
        "id": "RtH6dJ49fdzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tiled_gemm_shared_skeleton.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                   \\\n",
        "  cudaError_t err = (call);                                     \\\n",
        "  if (err != cudaSuccess) {                                     \\\n",
        "    fprintf(stderr, \"CUDA error %s:%d: %s\\n\",                   \\\n",
        "            __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "    std::exit(EXIT_FAILURE);                                    \\\n",
        "  }                                                             \\\n",
        "} while(0)\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (Task): Shared-memory tiled matrix multiplication (GEMM)\n",
        "// Compute C = A * B\n",
        "//\n",
        "// Shapes (row-major):\n",
        "//   A: M x K\n",
        "//   B: K x N\n",
        "//   C: M x N\n",
        "//\n",
        "// Requirements:\n",
        "//  - Each thread computes one (row, col) element of C\n",
        "//  - Use shared memory tiling for A and B\n",
        "//  - Use __syncthreads() correctly\n",
        "//  - Avoid out-of-bounds at edges (M,N,K may not be multiples of TILE)\n",
        "//  - Coalesced global loads for tiles (best effort)\n",
        "// ------------------------------------------------------------\n",
        "\n",
        "// TODO: choose a tile size (e.g., 16 or 32)\n",
        "#ifndef TILE\n",
        "#define TILE 0  // TODO\n",
        "#endif\n",
        "\n",
        "__global__ void matmulTiledShared(const float* __restrict__ A,\n",
        "                                  const float* __restrict__ B,\n",
        "                                  float* __restrict__ C,\n",
        "                                  int M, int N, int K) {\n",
        "    // TODO: declare shared memory tiles\n",
        "    // __shared__ float As[TILE][TILE];\n",
        "    // __shared__ float Bs[TILE][TILE];\n",
        "\n",
        "    // TODO: compute global row/col for this thread\n",
        "    // int row = ...\n",
        "    // int col = ...\n",
        "\n",
        "    // TODO: accumulator\n",
        "    // float acc = 0.0f;\n",
        "\n",
        "    // TODO: loop over tiles along K dimension\n",
        "    // for (int t = 0; t < ...; ++t) {\n",
        "    //   - load A tile element into As (guarded)\n",
        "    //   - load B tile element into Bs (guarded)\n",
        "    //   - __syncthreads()\n",
        "    //   - compute partial dot product over k in [0, TILE)\n",
        "    //   - __syncthreads()\n",
        "    // }\n",
        "\n",
        "    // TODO: write C[row, col] (guarded)\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// CPU reference GEMM (for correctness)\n",
        "// ------------------------------------------------------------\n",
        "static void matmulCPU(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            float acc = 0.0f;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                acc += A[i * K + k] * B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = acc;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO: correctness checker\n",
        "// Requirements:\n",
        "//  - Compare C_gpu vs C_ref within tolerance\n",
        "//  - Print first mismatch (i, value_gpu, value_ref)\n",
        "// ------------------------------------------------------------\n",
        "static bool checkClose(const float* gpu, const float* ref, int count, float tol) {\n",
        "    // TODO\n",
        "    return false;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// Timing helper (CUDA events) - implemented\n",
        "// ------------------------------------------------------------\n",
        "static float timeGemmMs(const float* dA, const float* dB, float* dC,\n",
        "                        int M, int N, int K,\n",
        "                        dim3 grid, dim3 block,\n",
        "                        int warmup, int iters) {\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    for (int i = 0; i < iters; ++i) {\n",
        "        matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    return ms / iters;\n",
        "}\n",
        "\n",
        "// ------------------------------------------------------------\n",
        "// TODO (optional): estimate GFLOPS\n",
        "// GEMM does ~2*M*N*K FLOPs\n",
        "// GFLOPS = FLOPs / time_seconds / 1e9\n",
        "// ------------------------------------------------------------\n",
        "static double estimateGFLOPS(int M, int N, int K, float kernel_ms) {\n",
        "    // TODO\n",
        "    return 0.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ------------------------------------------------------------\n",
        "    // Sizes (you can tune these)\n",
        "    // NOTE: try sizes that are NOT multiples of TILE to test edges.\n",
        "    // ------------------------------------------------------------\n",
        "    const int M = 1024;\n",
        "    const int N = 1024;\n",
        "    const int K = 1024;\n",
        "\n",
        "    const float tol = 1e-3f; // TODO: choose tolerance (float accumulation)\n",
        "    const size_t bytesA = size_t(M) * K * sizeof(float);\n",
        "    const size_t bytesB = size_t(K) * N * sizeof(float);\n",
        "    const size_t bytesC = size_t(M) * N * sizeof(float);\n",
        "\n",
        "    // Host alloc\n",
        "    float* hA = (float*)std::malloc(bytesA);\n",
        "    float* hB = (float*)std::malloc(bytesB);\n",
        "    float* hC_gpu = (float*)std::malloc(bytesC);\n",
        "    float* hC_ref = (float*)std::malloc(bytesC);\n",
        "\n",
        "    if (!hA || !hB || !hC_gpu || !hC_ref) {\n",
        "        fprintf(stderr, \"Host malloc failed.\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    // Init A, B\n",
        "    for (int i = 0; i < M * K; ++i) hA[i] = 0.001f * (i % 1000);\n",
        "    for (int i = 0; i < K * N; ++i) hB[i] = 0.002f * (i % 1000);\n",
        "\n",
        "    // CPU reference (can be slow for large sizes; reduce sizes if needed)\n",
        "    matmulCPU(hA, hB, hC_ref, M, N, K);\n",
        "\n",
        "    // Device alloc\n",
        "    float *dA=nullptr, *dB=nullptr, *dC=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, bytesA));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, bytesB));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, bytesC));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA, bytesA, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB, bytesB, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: Choose launch config based on TILE\n",
        "    // Typical:\n",
        "    //   block = (TILE, TILE)\n",
        "    //   grid  = (ceil(N/TILE), ceil(M/TILE))\n",
        "    // ------------------------------------------------------------\n",
        "    dim3 block(0, 0, 1); // TODO\n",
        "    dim3 grid(0, 0, 1);  // TODO\n",
        "\n",
        "    // Launch once (correctness)\n",
        "    matmulTiledShared<<<grid, block>>>(dA, dB, dC, M, N, K);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(hC_gpu, dC, bytesC, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ------------------------------------------------------------\n",
        "    // TODO: correctness check\n",
        "    // ------------------------------------------------------------\n",
        "    bool ok = checkClose(hC_gpu, hC_ref, M * N, tol);\n",
        "    printf(\"Correctness: %s\\n\", ok ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "    // Timing\n",
        "    const int warmup = 5;\n",
        "    const int iters = 20;\n",
        "    float ms = timeGemmMs(dA, dB, dC, M, N, K, grid, block, warmup, iters);\n",
        "\n",
        "    // Optional: GFLOPS\n",
        "    double gflops = estimateGFLOPS(M, N, K, ms);\n",
        "\n",
        "    printf(\"GEMM: M=%d N=%d K=%d | time=%.4f ms | GFLOPS=%.2f\\n\", M, N, K, ms, gflops);\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "    std::free(hA);\n",
        "    std::free(hB);\n",
        "    std::free(hC_gpu);\n",
        "    std::free(hC_ref);\n",
        "\n",
        "    return ok ? EXIT_SUCCESS : EXIT_FAILURE;\n",
        "}"
      ],
      "metadata": {
        "id": "S3-JyMOIfedi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}